---
layout: post
title: "ğŸ–¥ï¸ FG-Clip Practice!! : FG-Clip ì‹¤ìŠµ!! with python"
author: [DrFirst]
date: 2025-06-10 09:00:00 +0900
categories: [AI, Experiment]
tags: [FG-Clip, Clip, fine-grained understanding, Python,  ICML, ICML 2025]
sitemap :
  changefreq : weekly
  priority : 0.9
---

---

## ğŸ¦– FG-CLIP Practice!!  
> **FG-CLIP** : Fine-Grained Visual and Textual Alignment  

Today, weâ€™ll walk through a hands-on session with the hot new model from ICML 2025:  
[FG-CLIP](https://arxiv.org/pdf/2505.05071)!  

---

### ğŸ§± 1. Clone the FG-CLIP Git Repository  

- Clone the repo from the [official GitHub page](https://github.com/360CVGroup/FG-CLIP)!

```bash
git@github.com:360CVGroup/FG-CLIP.git
```

---

### ğŸ“¦ 2. Install Required Packages in Virtual Environment  

I used a `conda` virtual environment to install the required packages.  
I followed the instructions from the official GitHub repo!

```bash
conda create -n FGCLIP python=3.10 -y  
conda activate FGCLIP  
cd FG-CLIP && pip install -e .  
```

In addition, I installed the following packages separately  
(because I ran into some errors...):

```bash
pip install Pillow  
pip install matplotlib  
pip install torchvision --extra-index-url https://download.pytorch.org/whl/{insert-your-cu-version-here}  
```

---

### ğŸ§Š 3. Test the FG-CLIP Model!  

I tested it using the notebook environment provided by FG-CLIP.  
The code looks like this:

```python
import torch
from PIL import Image
from transformers import (
    AutoImageProcessor,
    AutoTokenizer,
    AutoModelForCausalLM,
)

model_root = "qihoo360/fg-clip-base"
image_size=224
model = AutoModelForCausalLM.from_pretrained(model_root,trust_remote_code=True).cuda()

device = model.device

tokenizer = AutoTokenizer.from_pretrained(model_root)
image_processor = AutoImageProcessor.from_pretrained(model_root)
```

Now the model downloads and loads successfully!

Letâ€™s try with a sample image provided in the repo: `cat_dfclor.jpg`

![catcam](https://github.com/user-attachments/assets/e72f0746-0082-4696-a097-9197bbc88f93) 

```python
img_root = "cat_dfclor.jpg"
image = Image.open(img_root).convert("RGB")
image = image.resize((image_size,image_size))

image_input = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].to(device)

walk_short_pos = True
captions=["a photo of a cat", "a photo of a dog", "a photo of a animal"]
caption_input = torch.tensor(tokenizer(captions, max_length=77, padding="max_length", truncation=True).input_ids, dtype=torch.long, device=device)

with torch.no_grad():
  image_feature = model.get_image_features(image_input)
  text_feature = model.get_text_features(caption_input,walk_short_pos=walk_short_pos)
  image_feature = image_feature / image_feature.norm(p=2, dim=-1, keepdim=True)
  text_feature = text_feature / text_feature.norm(p=2, dim=-1, keepdim=True)

logits_per_image = image_feature @ text_feature.T 
logits_per_image = model.logit_scale.exp() * logits_per_image
probs = logits_per_image.softmax(dim=1) 
print(probs)
```

It outputs the similarity between the image and the three captions:  
["a photo of a cat", "a photo of a dog", "a photo of a animal"]

```
tensor([[9.6813e-01, 3.2603e-05, 3.1839e-02]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
```

As expected, â€œcatâ€ scored the highest, followed by â€œanimalâ€, and â€œdogâ€ the lowest.  

But just seeing numbers isnâ€™t enoughâ€”we want to **visualize** the similarity!  

```python
import math
import matplotlib
matplotlib.use('Agg') 
import matplotlib.pyplot as plt

img_root = "cat_dfclor.jpg"
image = Image.open(img_root).convert("RGB")
image = image.resize((image_size,image_size))

image_input = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].to(device)

with torch.no_grad():
    dense_image_feature = model.get_image_dense_features(image_input)
    cap = "cat"
    captions = [cap]
    caption_input = torch.tensor(tokenizer(captions, max_length=77, padding="max_length", truncation=True).input_ids, dtype=torch.long, device=device)
    text_feature = model.get_text_features(caption_input,walk_short_pos=True)
    text_feature = text_feature / text_feature.norm(p=2, dim=-1, keepdim=True)
    dense_image_feature = dense_image_feature / dense_image_feature.norm(p=2, dim=-1, keepdim=True)

similarity = dense_image_feature.squeeze() @ text_feature.squeeze().T
similarity = similarity.cpu().numpy()
patch_size = int(math.sqrt(similarity.shape[0]))

original_shape = (patch_size, patch_size)
show_image = similarity.reshape(original_shape) 

plt.figure(figsize=(6, 6))
plt.imshow(show_image)
plt.title('similarity Visualization')
plt.axis('off')  
plt.savefig(f"{cap}_{img_root}.png")
```

This highlights the region associated with "cat"!  
Youâ€™ll see something like this:

> **black cat** â†’ It really highlights the black cat only!  
![blackcat](https://github.com/user-attachments/assets/b9e926f0-1ace-44d0-935f-1c1265fe0e14)

> **keyboard** â†’ Nice!  
![keyboard](https://github.com/user-attachments/assets/8e44fba3-4796-4e09-a5c6-ee09f478cde7)

> **blanket** and **chair** â†’ These work to some extent too!  
![etc](https://github.com/user-attachments/assets/cc1a05b3-baad-4d71-a18d-381a28693ae7)


Then I tested a baseball stadium image:

> **hold a bat** â†’ Works well!  
![holdbat](https://github.com/user-attachments/assets/5bfdc081-f25a-43f8-b04b-18dd4d497998)

> **player** â†’ Seems pretty accurate!?  
![player](https://github.com/user-attachments/assets/3dd81666-9f69-42e0-b71f-b5e2725f6830)

> **catch** â†’ Is it right...?
![catch](https://github.com/user-attachments/assets/d69491f5-12e7-43d5-b00c-6d765be88d62)

---

### ğŸ”² Letâ€™s Try Bounding Boxes!  

Numbers and heatmaps are nice, but how about **bounding boxes**?  
Here's the code I used for BBox generation:

```python
import torch
from PIL import Image
from transformers import (
    AutoImageProcessor,
    AutoTokenizer,
    AutoModelForCausalLM,
)
import math
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from scipy.ndimage import label  # Used to find adjacent regions

# --- 1. Load model and tokenizer ---
model_root = "qihoo360/fg-clip-base"
image_size = 224
model = AutoModelForCausalLM.from_pretrained(model_root, trust_remote_code=True).cuda()
device = model.device
tokenizer = AutoTokenizer.from_pretrained(model_root)
image_processor = AutoImageProcessor.from_pretrained(model_root)

# --- 2. Load image and set caption ---
img_root = "baseball_bat_000106.jpg"
cap = "handle"

try:
    image = Image.open(img_root).convert("RGB")
except FileNotFoundError:
    print(f"'{img_root}' not found. Generating a black image for fallback.")
    image = Image.new('RGB', (image_size, image_size), color = 'black')

image = image.resize((image_size, image_size))

# --- 3. Feature extraction and similarity calculation ---
image_input = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].to(device)
with torch.no_grad():
    dense_image_feature = model.get_image_dense_features(image_input)
    captions = [cap]
    caption_input = torch.tensor(tokenizer(captions, max_length=77, padding="max_length", truncation=True).input_ids, dtype=torch.long, device=device)
    text_feature = model.get_text_features(caption_input, walk_short_pos=True)
    text_feature = text_feature / text_feature.norm(p=2, dim=-1, keepdim=True)
    dense_image_feature = dense_image_feature / dense_image_feature.norm(p=2, dim=-1, keepdim=True)

similarity = dense_image_feature.squeeze() @ text_feature.squeeze().T
similarity = similarity.cpu().numpy()

# --- 4. Group regions above average and calculate BBox coordinates ---
patch_size_in_grid = int(math.sqrt(similarity.shape[0]))
pixel_per_patch = image_size // patch_size_in_grid

# 1) Reshape similarity into a 2D grid
similarity_map = similarity.reshape((patch_size_in_grid, patch_size_in_grid))

# 2) Set a threshold using the average value
threshold = 0.22  # You can also try: np.mean(similarity) * 1.4

# 3) Create a binary mask where values above the threshold are True
print(f"threshold : {threshold}")
print(f"similarity_map : {similarity_map}")

mask = similarity_map > threshold
print("Mask shape:", mask.shape)

# 4) Label adjacent True regions (clusters) using scipy.ndimage.label
labeled_array, num_features = label(mask)

# --- 5. Draw BBox for each grouped region ---
fig, ax = plt.subplots(1, figsize=(8, 8))
ax.imshow(image)

# Initialize overall bounding box coordinates
all_x1, all_y1 = float('inf'), float('inf')
all_x2, all_y2 = float('-inf'), float('-inf')

for i in range(1, num_features + 1):
    
    # Find all (row, col) indices for current label
    rows, cols = np.where(labeled_array == i)
    print(rows, cols)
    
    # Get min/max for current cluster
    min_row, max_row = np.min(rows), np.max(rows)
    min_col, max_col = np.min(cols), np.max(cols)
    
    # Compute top-left and size of BBox
    bbox_x = min_col * pixel_per_patch
    bbox_y = min_row * pixel_per_patch
    bbox_w = (max_col - min_col + 1) * pixel_per_patch
    bbox_h = (max_row - min_row + 1) * pixel_per_patch
    print(bbox_x, bbox_y, bbox_w, bbox_h)

    # Create rectangle patch
    rect = patches.Rectangle(
        (bbox_x, bbox_y), bbox_w, bbox_h,
        linewidth=2, edgecolor='cyan', facecolor='none'
    )
    ax.add_patch(rect)

    cluster_sim_values = similarity_map[rows, cols]
    mean_similarity = np.mean(cluster_sim_values)

    # ğŸ¯ Display label at the center
    center_col = bbox_x + bbox_w * 0.5
    center_row = bbox_y + bbox_h * 0.5
    ax.text(center_col, center_row,  f"{mean_similarity:.3f}", color='black', ha='center', va='center', fontsize=10, weight='bold')
    
print(f"num_features : {num_features}")
for i in range(1, num_features + 1):
    rows, cols = np.where(labeled_array == i)
    if len(rows) == 0:
        continue

    min_row, max_row = np.min(rows), np.max(rows)
    min_col, max_col = np.min(cols), np.max(cols)

    bbox_x1 = min_col * pixel_per_patch
    bbox_y1 = min_row * pixel_per_patch
    bbox_x2 = (max_col + 1) * pixel_per_patch
    bbox_y2 = (max_row + 1) * pixel_per_patch

    # Update overall BBox range
    all_x1 = min(all_x1, bbox_x1)
    all_y1 = min(all_y1, bbox_y1)
    all_x2 = max(all_x2, bbox_x2)
    all_y2 = max(all_y2, bbox_y2)

# Draw final merged BBox
final_w = all_x2 - all_x1
final_h = all_y2 - all_y1

rect = patches.Rectangle(
    (all_x1, all_y1), final_w, final_h,
    linewidth=3, edgecolor='red', facecolor='none'
)
ax.add_patch(rect)

ax.set_title(f"Regions with Similarity > Average for: '{cap}' // threshold :{threshold}", fontsize=14)
ax.axis('off')
plt.savefig(f"bbox/bbox_multi_{cap}_{img_root}.png")
print(f"bbox/bbox_multi_{cap}_{img_root}.png")

```

This code lets you highlight the region corresponding to your captionâ€”dynamically!

For example:  
> **handle** â†’ Beautiful result!  
![handle](https://github.com/user-attachments/assets/d281fb4b-1d83-47f2-93e1-c3aa25097b12)

> **hit the ball** â†’ Somewhat localized to the bat area!  
![hittheball](https://github.com/user-attachments/assets/acbe5bef-d947-4ff1-a9b8-0a24b837e015)

> **frisbee** â†’ Kind of... interesting!  
![frisbee](https://github.com/user-attachments/assets/0174bfe1-8ef3-431e-9093-14f21f671745)

---

### ğŸ‰ Wrapping Up

Today, we explored and tested the FG-CLIP model!  
In the next post, Iâ€™ll dive into how it actually works under the hood. Stay tuned!


---

## ğŸ¦–(í•œêµ­ì–´) FG-Clip ì‹¤ìŠµ!!
> **FG-CLIP** : Fine-Grained Visual and Textual Alignment


ì˜¤ëŠ˜ì€ 2025ë…„ ICMLì—ì„œ ê³µê°œëœ ë”°ëˆë”°ëˆí•œ ì‹ ëª¨ë¸,  
[FG-CLIP](https://arxiv.org/pdf/2505.05071) ì— ëŒ€í•˜ì—¬ ì‹¤ìŠµì„ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤!!  

---

### ğŸ§± 1. FG-CLIP Git Clone 

- [ê³µì‹ Git ì‚¬ì´íŠ¸](https://github.com/360CVGroup/FG-CLIP)ì—ì„œ Repoë¥¼ Clone í•©ë‹ˆë‹¤!!

```bash
git@github.com:360CVGroup/FG-CLIP.git
```

---

### ğŸ“¦ 2. ê°€ìƒí™˜ê²½ì—ì„œì˜ í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜!!

ì €ëŠ” conda ê°€ìƒí™˜ê²½ì—ì„œ í•„ìš” íŒ¨í‚¤ì§€ë“¤ì„ ì„¤ì¹˜í–ˆìŠµë‹ˆë‹¤!!  
ê³µì‹ git ì—ì„œ ì„¤ëª…í•œëŒ€ë¡œ ë”°ë¼ê°”ì§€ìš”~!  

```bash
conda create -n FGCLIP python=3.10 -y
conda activate FGCLIP
cd FG-CLIP && pip install -e .
```

ê·¸ ì™¸ì—ë„ ì•„ë˜ëŠ” ì œê°€ ë³„ë„ë¡œ ì„¤ì¹˜í•´ì£¼ì—ˆìŠµë‹ˆë‹¤!!  
(ì—ëŸ¬ê°€ ë‚˜ë”ë¼êµ¬ìš”!,,)  

```bash
pip install Pillow
pip install matplotlib
pip install torchvision --extra-index-url https://download.pytorch.org/whl/{ì—¬ê¸°ëŠ” ê°ì ì•Œë§ì€ cu ë²„ì ¼ìœ¼ë¡œ!}
```
---

### ğŸ§Š 3. FG-CLIP ëª¨ë¸ í…ŒìŠ¤íŠ¸!!

ì €ëŠ” FGClipì˜ ë…¸íŠ¸ë¶ í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í–ˆìŠµë‹ˆë‹¤!!  
dì„  ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¡œ 

```python
import torch
from PIL import Image
from transformers import (
    AutoImageProcessor,
    AutoTokenizer,
    AutoModelForCausalLM,
)

model_root = "qihoo360/fg-clip-base"
image_size=224
model = AutoModelForCausalLM.from_pretrained(model_root,trust_remote_code=True).cuda()

device = model.device

tokenizer = AutoTokenizer.from_pretrained(model_root)
image_processor = AutoImageProcessor.from_pretrained(model_root)
```

ê·¸ëŸ¼!! ëª¨ë¸ì„ ë‹¤ìš´ë°›ì•„ load ê°€ ë©ë‹ˆë‹¤!!  

ì´ì œ! ê¸°ë³¸ì ìœ¼ë¡œ ì œê³µëœ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸í•´ë³¼ê¹Œìš”!?

![cat_dfclor.jpg](https://github.com/user-attachments/assets/49723476-d402-4a22-94c7-2d1b801b4aa4)

`cat_dfclor.jpg` ë¼ëŠ”, repoì— ìˆë˜ ì´ë¯¸ì§€ë¡œ ì•„ë˜ì™€ ê°™ì´ ì§„í–‰í•´ë³´ë©´!!


```python

img_root = "cat_dfclor.jpg"
image = Image.open(img_root).convert("RGB")
image = image.resize((image_size,image_size))

image_input = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].to(device)

# NOTE Short captions: max_length=77 && walk_short_pos=True
walk_short_pos = True
captions=["a photo of a cat", "a photo of a dog", "a photo of a animal"]
caption_input = torch.tensor(tokenizer(captions, max_length=77, padding="max_length", truncation=True).input_ids, dtype=torch.long, device=device)

# NOTE Long captions: max_length=248 && walk_short_pos=False
# ......

with torch.no_grad():
  image_feature = model.get_image_features(image_input)
  text_feature = model.get_text_features(caption_input,walk_short_pos=walk_short_pos)
  image_feature = image_feature / image_feature.norm(p=2, dim=-1, keepdim=True)
  text_feature = text_feature / text_feature.norm(p=2, dim=-1, keepdim=True)

logits_per_image = image_feature @ text_feature.T 
logits_per_image = model.logit_scale.exp() * logits_per_image
probs = logits_per_image.softmax(dim=1) 
print(probs)
```

ì„¸ê°œì˜ í”„ë¡¬í¬íŠ¸ "["a photo of a cat", "a photo of a dog", "a photo of a animal"]" ì— ëŒ€í•˜ì—¬  
ì•„ë˜ì™€ ê°™ì´ ê°ê°ì˜ ì—°ê´€ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤!

```
tensor([[9.6813e-01, 3.2603e-05, 3.1839e-02]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
```

catì´ë¼ëŠ”ê²Œ ê°€ì¥ ë†’ê³ , ê·¸ë‹¤ìŒì€ ë™ë¬¼, ê°•ì•„ì§€ëŠ” ê°€ì¥ ìˆ˜ì¹˜ê°€ ì‘ê²Œ ë‚´ì™”ìŠµë‹ˆë‹¤!

ê·¸ëŸ°ë°! ì´ë ‡ê²Œ ìˆ«ìë¡œ ë³´ëŠ”ê²ƒ ë§ê³ , ì´ë¯¸ì§€ë¡œ ë°”ë°”ì•¼ê² ì§€ìš”!?  

```python

import math
import matplotlib
matplotlib.use('Agg') 
import matplotlib.pyplot as plt


img_root = "cat_dfclor.jpg"
image = Image.open(img_root).convert("RGB")
image = image.resize((image_size,image_size))

image_input = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].to(device)

with torch.no_grad():
    dense_image_feature = model.get_image_dense_features(image_input)
    cap = "cat"
    captions = [cap]
    caption_input = torch.tensor(tokenizer(captions, max_length=77, padding="max_length", truncation=True).input_ids, dtype=torch.long, device=device)
    text_feature = model.get_text_features(caption_input,walk_short_pos=True)
    text_feature = text_feature / text_feature.norm(p=2, dim=-1, keepdim=True)
    dense_image_feature = dense_image_feature / dense_image_feature.norm(p=2, dim=-1, keepdim=True)

similarity = dense_image_feature.squeeze() @ text_feature.squeeze().T
similarity = similarity.cpu().numpy()
patch_size = int(math.sqrt(similarity.shape[0]))


original_shape = (patch_size, patch_size)
show_image = similarity.reshape(original_shape) 


plt.figure(figsize=(6, 6))
plt.imshow(show_image)
plt.title('similarity Visualization')
plt.axis('off')  
plt.savefig(f"{cap}_{img_root}.png")

```
ê·¸ëŸ¼!! 

![catcam](https://github.com/user-attachments/assets/e72f0746-0082-4696-a097-9197bbc88f93) 

ìœ„ì™€ ê°™ì´ ê³ ì–‘ì´ ë¶€ë¶„ì´ í™œì„±í™”ë©ë‹ˆë‹¤!!

ê°™ì€ ê·¸ë¦¼ì—ì„œ ì•„ë˜ì™€ ê°™ì´ ì¶”ê°€í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³´ì•˜ì–´ìš”!

> black cat : ì •ë§ ê²€ì •ê³ ì–‘ì´ ë¶€ë¶„ë§Œ í™œì„±í™”ë©ë‹ˆë‹¤!!  
![blackcat](https://github.com/user-attachments/assets/b9e926f0-1ace-44d0-935f-1c1265fe0e14)

>  keyboard : êµ³!!  
![keyboard](https://github.com/user-attachments/assets/8e44fba3-4796-4e09-a5c6-ee09f478cde7)

> ê·¸ ì™¸ì— ë°°ê²½ì—ìˆë˜ blanketê³¼ chair. ì–´ëŠì •ë„ í•˜ëŠ”ê²ƒ ê°™ë„¤ìš”!!  
![etc](https://github.com/user-attachments/assets/cc1a05b3-baad-4d71-a18d-381a28693ae7)

ì´ë²ˆì—” ë§ì´ ë´¤ë˜ ì•¼êµ¬ì¥ ì‚¬ì§„ìœ¼ë¡œ!  
> `hold a bat` ! ì˜ í•˜ë„¤ìš”!!  
![holdbat](https://github.com/user-attachments/assets/5bfdc081-f25a-43f8-b04b-18dd4d497998)

> player ì–´ëŠì •ë„ ë§ëŠ”ê²ƒ ê°™ì€!?  
![player](https://github.com/user-attachments/assets/3dd81666-9f69-42e0-b71f-b5e2725f6830)

> catch  ë§ëŠ”ê±¸ê¹Œìš”!?..   
![catch](https://github.com/user-attachments/assets/d69491f5-12e7-43d5-b00c-6d765be88d62)


ê·¸ëŸ°ë°!! ì´ë ‡ê²Œë§Œ ë³´ëŠ”ê²ƒì€ ë„ˆë¬´ ë‹µë‹µí–ˆìŠµë‹ˆë‹¤~   
í•œë²ˆ bboxë¥¼ í•´ë³´ëŠ”ê²ƒì€ ì–´ë–¨ê¹Œìš”?  
ì´ë¥¼ìœ„í•´ ì €ëŠ” ë³„ë„ì˜ ì•„ë˜ ì½”ë“œë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤!  

```python
import torch
from PIL import Image
from transformers import (
    AutoImageProcessor,
    AutoTokenizer,
    AutoModelForCausalLM,
)
import math
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from scipy.ndimage import label # ì¸ì ‘í•œ ì˜ì—­ì„ ì°¾ëŠ” ë° ì‚¬ìš©

# --- 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---
model_root = "qihoo360/fg-clip-base"
image_size = 224
model = AutoModelForCausalLM.from_pretrained(model_root, trust_remote_code=True).cuda()
device = model.device
tokenizer = AutoTokenizer.from_pretrained(model_root)
image_processor = AutoImageProcessor.from_pretrained(model_root)

# --- 2. ì´ë¯¸ì§€ ë¡œë“œ ë° ìº¡ì…˜ ì„¤ì • ---
img_root = "baseball_bat_000106.jpg"
cap = "handle"

try:
    image = Image.open(img_root).convert("RGB")
except FileNotFoundError:
    print(f"'{img_root}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹¤í–‰ì„ ìœ„í•´ ì„ì˜ì˜ ê²€ì€ìƒ‰ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
    image = Image.new('RGB', (image_size, image_size), color = 'black')

image = image.resize((image_size, image_size))

# --- 3. í”¼ì²˜ ì¶”ì¶œ ë° ìœ ì‚¬ë„ ê³„ì‚° ---
image_input = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].to(device)
with torch.no_grad():
    dense_image_feature = model.get_image_dense_features(image_input)
    captions = [cap]
    caption_input = torch.tensor(tokenizer(captions, max_length=77, padding="max_length", truncation=True).input_ids, dtype=torch.long, device=device)
    text_feature = model.get_text_features(caption_input, walk_short_pos=True)
    text_feature = text_feature / text_feature.norm(p=2, dim=-1, keepdim=True)
    dense_image_feature = dense_image_feature / dense_image_feature.norm(p=2, dim=-1, keepdim=True)

similarity = dense_image_feature.squeeze() @ text_feature.squeeze().T
similarity = similarity.cpu().numpy()

# --- 4. í‰ê·  ì´ìƒ ì˜ì—­ì„ ê·¸ë£¹í™”í•˜ê³  BBox ì¢Œí‘œ ê³„ì‚° ---
patch_size_in_grid = int(math.sqrt(similarity.shape[0]))
pixel_per_patch = image_size // patch_size_in_grid

# 1) ìœ ì‚¬ë„ë¥¼ 2D ê·¸ë¦¬ë“œ í˜•íƒœë¡œ ë³€í™˜
similarity_map = similarity.reshape((patch_size_in_grid, patch_size_in_grid))

# 2) í‰ê· ê°’ì„ ì„ê³„ê°’(threshold)ìœ¼ë¡œ ì„¤ì •
threshold = 0.22#np.mean(similarity) * 1.4

# 3) ì„ê³„ê°’ë³´ë‹¤ ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ íŒ¨ì¹˜ë§Œ Trueë¡œ í‘œì‹œí•˜ëŠ” ë§ˆìŠ¤í¬ ìƒì„±
print(f"threshold : {threshold}")
print(f"similarity_map : {similarity_map}")

mask = similarity_map > threshold
print("Mask shape:", mask.shape)
# 4) scipy.ndimage.labelì„ ì‚¬ìš©í•´ ì¸ì ‘í•œ True ì˜ì—­(í´ëŸ¬ìŠ¤í„°)ì— ê³ ìœ  ë²ˆí˜¸(ë ˆì´ë¸”)ë¥¼ ë¶™ì„
labeled_array, num_features = label(mask)

# --- 5. ê·¸ë£¹í™”ëœ ê° ì˜ì—­ì— BBox ê·¸ë ¤ì„œ ì‹œê°í™” ---
fig, ax = plt.subplots(1, figsize=(8, 8))
ax.imshow(image)


# ëˆ„ì ìš© ì „ì²´ bbox ì¢Œí‘œ ì´ˆê¸°í™”
all_x1, all_y1 = float('inf'), float('inf')
all_x2, all_y2 = float('-inf'), float('-inf')


for i in range(1, num_features + 1):
    
    # í˜„ì¬ ë ˆì´ë¸”ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  í”½ì…€ì˜ (í–‰, ì—´) ì¢Œí‘œ ì°¾ê¸°
    rows, cols = np.where(labeled_array == i)
    print(rows, cols)
    
    # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ë¥¼ ê°ì‹¸ëŠ” ìµœì†Œ/ìµœëŒ€ ì¢Œí‘œ ì°¾ê¸°
    min_row, max_row = np.min(rows), np.max(rows)
    min_col, max_col = np.min(cols), np.max(cols)
    
    # BBoxì˜ ì¢Œì¸¡ ìƒë‹¨ í”½ì…€ ì¢Œí‘œì™€ ë„ˆë¹„/ë†’ì´ ê³„ì‚°
    bbox_x = min_col * pixel_per_patch
    bbox_y = min_row * pixel_per_patch
    bbox_w = (max_col - min_col + 1) * pixel_per_patch
    bbox_h = (max_row - min_row + 1) * pixel_per_patch
    print(bbox_x, bbox_y, bbox_w, bbox_h)
    # BBox ì‚¬ê°í˜• ìƒì„±
    rect = patches.Rectangle(
        (bbox_x, bbox_y), bbox_w, bbox_h,
        linewidth=2, edgecolor='cyan', facecolor='none'
    )
    ax.add_patch(rect)
    cluster_sim_values = similarity_map[rows, cols]
    mean_similarity = np.mean(cluster_sim_values)
    # ğŸ¯ ì¤‘ì‹¬ì ì— ë¼ë²¨ í…ìŠ¤íŠ¸ í‘œì‹œ
    center_col = bbox_x + bbox_w*0.5
    center_row = bbox_y+ bbox_h*0.5
    ax.text(center_col, center_row,  f"{mean_similarity:.3f}", color='black', ha='center', va='center', fontsize=10, weight='bold')
    
print(f"num_features : {num_features}")
for i in range(1, num_features + 1):
    rows, cols = np.where(labeled_array == i)
    if len(rows) == 0:
        continue

    min_row, max_row = np.min(rows), np.max(rows)
    min_col, max_col = np.min(cols), np.max(cols)

    bbox_x1 = min_col * pixel_per_patch
    bbox_y1 = min_row * pixel_per_patch
    bbox_x2 = (max_col + 1) * pixel_per_patch
    bbox_y2 = (max_row + 1) * pixel_per_patch

    # ì „ì²´ ë²”ìœ„ í™•ì¥
    all_x1 = min(all_x1, bbox_x1)
    all_y1 = min(all_y1, bbox_y1)
    all_x2 = max(all_x2, bbox_x2)
    all_y2 = max(all_y2, bbox_y2)

# ìµœì¢… í•©ì³ì§„ BBox ê·¸ë¦¬ê¸°
final_w = all_x2 - all_x1
final_h = all_y2 - all_y1

rect = patches.Rectangle(
    (all_x1, all_y1), final_w, final_h,
    linewidth=3, edgecolor='red', facecolor='none'
)
ax.add_patch(rect)

ax.set_title(f"Regions with Similarity > Average for: '{cap}' // threshold :{threshold}", fontsize=14)
ax.axis('off')
plt.savefig(f"bbox/bbox_multi_{cap}_{img_root}.png")
print(f"bbox/bbox_multi_{cap}_{img_root}.png")
```

ìœ„ì˜ ì½”ë“œì—ì„œ bboxë¥¼ ìœ„í•œ thresholdëŠ” ì œê°€,, ì˜ ë§ê²Œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤!!

ê·¸ ê²°ê³¼!!

> handle : ì•„ì£¼ ì¢‹ì£ !?
![handle](https://github.com/user-attachments/assets/d281fb4b-1d83-47f2-93e1-c3aa25097b12)

> hit the ball : ì–´ëŠì •ë„ ë°©ë§ì´ ë¶€ë¶„ì¸ë“¯!?
![hittheball](https://github.com/user-attachments/assets/acbe5bef-d947-4ff1-a9b8-0a24b837e015)

> frisbee 
![frisbee](https://github.com/user-attachments/assets/0174bfe1-8ef3-431e-9093-14f21f671745)

---

### ğŸ‰ ë§ˆë¬´ë¦¬

ì˜¤ëŠ˜ì€ ì´ë ‡ê²Œ FG-CLIPì— ëŒ€í•˜ì—¬ í…ŒìŠ¤íŠ¸í•´ë³´ì•˜ìŠµë‹ˆë‹¤!  
ë‹¤ìŒ í¬ìŠ¤íŒ…ì—ì„œ ì›ë¦¬ì— ëŒ€í•˜ì—¬ ê³µë¶€í•´ë³´ê² ìŠµë‹ˆë‹¤!  