---
layout: post
title: "🧠 OSrCIR: Reason-before-Retrieve for Composed Image Retrieval"
author: [DrFirst]
date: 2025-07-30 09:00:00 +0900
categories: [AI, Research]
tags: [MLLM, CIR, Image Retrieval, CVPR 2025, Reasoning, One-Stage, CVPR, CVPR 2025 ]
sitemap :
  changefreq : monthly
  priority : 0.8

---

---


### 🧠 (한국어) OSrCIR: Reason-before-Retrieve 블라블라블라

- **제목**: [Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/pdf/2412.11077)  
- **학회**: CVPR 2025 (Highlight paperYuanmin Tang et al.)  
- **코드**: [osrcir (GitHub)](https://github.com/Pter61/osrcir)  
- **핵심 키워드**: `Composed Image Retrieval`, `Chain-of-Thought`, `One-Stage`, `MLLM`, `Zero-Shot`

---

## 📌 3줄 요약

1. 기존의 **이미지+텍스트 조합 검색(CIR)** 은 대부분 **2-Stage 구조** (이미지 캡션 → 텍스트 추론) 사용  
2. OSrCIR은 MLLM이 Reference 이미지를 **직접 reasoning하여**, 텍스트 없이 **Target 이미지의 특성 자체를 추론**  
3. 결과적으로 정확도/속도 향상, **사전 학습 없이 zero-shot inference만으로** 작동 가능

---

## 🔍 기존 CIR 구조의 한계

| 방식 | 구조 | 문제점 |
|------|------|--------|
| 2-Stage CIR | (1) 이미지 → 캡션 생성 (2) 텍스트 → 추론 → 검색 | 이미지 정보 손실, reasoning 오류 발생 |
| Text-Only Reasoning | Reference 이미지 정보를 간접적으로 전달 | 시각적 속성 반영 어려움 |
| MLLM 활용 방식 | 질문 응답으로 간접 reasoning | 시간 소요, 일관성 부족 |

→ 즉, **텍스트를 중간 매개로 삼는 방식 자체가 본질적인 정보 손실을 유발**함.

---


##

# 📊 Comparison of CIRCO and CIRR Test Data

## 🧾 Overview

| 항목                 | CIRCO (Composable Image Retrieval) | CIRR (Composable Image Retrieval on Real life) |
|----------------------|------------------------------------|------------------------------------------------|
| 목적                | 구성 요소 기반 이미지 검색 (조합적) | 일상 장면 기반의 구성 이미지 검색              |
| 데이터 유형         | Synthetic-style + Multi-object     | Real-life 사진 (도시, 일상 등)                 |
| 주요 Task           | Compositional Retrieval            | Reference + Text-based Target Retrieval        |
| 샘플 구성           | Query Image + Target Attribute     | Reference Image + Caption                      |
| 정답 수             | Top-1 또는 Top-k (단일 정답)       | Top-1 또는 Top-k (단일 정답)                   |
| Negative 구조       | Disentangled Hard Negatives        | Semantically Similar Distractors               |
| 난이도 특징         | Attribute 수준 조합의 다양성 높음  | 장면 유사도 기반 Distractor 포함               |
| 주요 사용 목적      | 모델의 조합 일반화 능력 평가       | 실제 상황에서의 Text-Image 조합 검색 평가     |

---

## 📁 CIRCO Dataset

- **출처**: [CIRCO: Compositional Image Retrieval with Complex Object Descriptions](https://arxiv.org/abs/2306.05459)
- **구성**: 다양한 물체 속성과 배경 조합으로 생성된 쿼리-타깃 쌍
- **입력 쿼리 예시**:  
  - 이미지: '개가 나무 옆에 있음'  
  - 속성 변경: '나무 → 벤치'
- **특징**: 시각 개체 구성 요소 단위의 조합 능력 평가 가능

---

## 📁 CIRR Dataset

- **출처**: [Compositional Image Retrieval on Real-life images](https://arxiv.org/abs/2104.00613)
- **구성**: 실제 이미지 기반, 문장 설명과 함께 참조 이미지 제공
- **입력 쿼리 예시**:  
  - 참조 이미지 + 텍스트 설명: "같은 여자가 있는데 옷 색이 다르고 뒤에 있는 자동차는 없음"
- **특징**: 실제 장면에서의 의미 기반 조합 검색 능력 평가

---

## 🧠 요약 정리

- **CIRCO**: 구조적, 조합 일반화 능력을 테스트하는 데 중점.
- **CIRR**: 현실 기반의 직관적인 텍스트-이미지 검색 테스트에 중점.

> 두 데이터셋 모두 V+L 모델의 "조합적 이해와 검색 능력"을 평가하는 데 사용되지만, **CIRCO는 더 복잡한 조합 패턴**, **CIRR은 실제 사진과 설명 기반의 직관적 평가**에 초점을 둡니다.



## 🌱 OSrCIR의 핵심 아이디어

> “**Reason first. Then retrieve.**”

- 기존 CIR은 “Retrieve-and-Reason” 방식  
- **OSrCIR은 반대로 ‘Reason-before-Retrieve’**  
- **MLLM을 사용해 이미지에서 직접 target 특성 추론**  
- 이 reasoning 결과(텍스트)를 기반으로 Target 이미지 검색 수행

---

## 🔧 OSrCIR 아키텍처 요약

![arch](https://github.com/OSoMeLab/osrcir/assets/osrcir_arch.png)

- **입력**: (Reference Image, Text Query)
- **Stage 1**: MLLM을 활용해 Reference 이미지에 대해 chain-of-thought 스타일 추론 수행  
- **Stage 2**: 추론 결과를 텍스트 쿼리로 정제
- **Stage 3**: 검색 후보 이미지들과 CLIP 기반 텍스트-이미지 매칭 수행 (zero-shot)

→ 전체 과정이 end-to-end로 **단일 단계(one-stage)** 에서 처리됨

---

## 🧪 실험 결과 요약

> 주요 벤치마크에서 **기존 2-Stage 방법들보다 정확도 + 효율 모두 우수**한 성과 달성!

| Dataset | Recall@1 (기존 SOTA) | OSrCIR | 향상폭 |
|---------|-----------------------|--------|--------|
| CIRR    | 52.1 (FashionIQ-CLIP) | **57.4** | +5.3   |
| CIRCO   | 33.8                 | **37.9** | +4.1   |
| FashionIQ | 48.7               | **54.2** | +5.5   |

- **Zero-shot 설정**에서 실현됨 (학습 없이 inference만으로)
- Ablation 결과, reasoning을 생략하면 성능 급락

---

## ✅ 결론 및 의의

- OSrCIR은 MLLM의 고차 reasoning 능력을 CIR에 **최적화된 방식**으로 끌어낸 대표적 사례  
- 별도 학습 없이 inference만으로 동작 → **Training-free + Generalizable**
- **Chain-of-Thought reasoning이 단일 스테이지 retrieval에 직접 적용된 최초 사례 중 하나**
- 향후 VLM 기반 **튜터링, 검색, AGI planning 등**에서의 응용 가능성 매우 큼

> “Retrieval is not just about matching. It’s about **reasoning what to match**.”