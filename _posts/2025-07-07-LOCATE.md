---
layout: post
title: "ğŸ“Œ LOCATE: Weakly Supervised Affordance Groundingì„ ìœ„í•œ Object Part Localization & Transfer"
author: [DrFirst]
date: 2025-07-07 09:00:00 +0900
categories: [AI, Research]
tags: [Computer Vision, Affordance, Weakly-Supervised, LOCATE, CVPR 2023, CVPR]
sitemap:
  changefreq: monthly
  priority: 0.8
---

### ğŸ“Œ (í•œêµ­ì–´) LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding  

![Image](https://github.com/user-attachments/assets/3eeaf4b4-a0dd-4712-8feb-ed7b9384c4a7)

* **ì œëª©**: [LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_LOCATE_Localize_and_Transfer_Object_Parts_for_Weakly_Supervised_Affordance_CVPR_2023_paper.pdf)
* **í•™íšŒ**: CVPR 2023  
* **ì½”ë“œ/ì²´í¬í¬ì¸íŠ¸**: [GitHub â€“ LOCATE](https://github.com/Reagan1311/LOCATE)    
* **ì €ì**: Gen Li (University of Edinburgh), Varun Jampani (Google Research), Deqing Sun (Google Research), Laura Sevilla-Lara (University of Edinburgh)  
* **í•µì‹¬ í‚¤ì›Œë“œ**: `Affordance`, `Weakly-Supervised`, `Knowledge Transfer`, `Object Parts`, `DINO-ViT`   
* **ìš”ì•½**: LOCATEëŠ” **ì´ë¯¸ì§€ ìˆ˜ì¤€ ë¼ë²¨ë§Œ ì´ìš©**í•˜ëŠ” WSAG(Weakly Supervised Affordance Grounding) ë¬¸ì œì—ì„œ, **Object-Part Prototype ì¶”ì¶œ ë° ì „ì´** ë°©ì‹ì„ í†µí•´ affordance ë¶€ìœ„ë¥¼ ë” ì •í™•íˆ ì°¾ì•„ë‚´ëŠ” í”„ë ˆì„ì›Œí¬. ê¸°ì¡´ SOTA ëŒ€ë¹„ **seen/unseen ê°ì²´ ëª¨ë‘ì—ì„œ ì„±ëŠ¥ í–¥ìƒ** ğŸš€  

---

### ğŸš€ ì—°êµ¬ í•µì‹¬ ìš”ì•½

> í•œ ì¤„ ìš”ì•½: **â€œLOCATE = Object-Part Localization + Prototype Selection + Part-Level Knowledge Transferâ€**

1) **ì—°êµ¬ ë°°ê²½ (WSAG)**  
- Affordance Grounding: ì‚¬ë¬¼ì˜ íŠ¹ì • ë¶€ìœ„ê°€ ì–´ë–¤ í–‰ë™ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ”ì§€ í•™ìŠµ (ì˜ˆ: ì»µ ì†ì¡ì´=ì¡ê¸°, ì¹¼ë‚ =ìë¥´ê¸°)  
- ê¸°ì¡´ CAM ê¸°ë°˜ ì ‘ê·¼ì€ **ë¼ë²¨ë§ ë¹„ìš©ì´ í¬ê³ **, í•™ìŠµëœ activationì´ **ì‚¬ëŒÂ·ë°°ê²½ ë…¸ì´ì¦ˆ**ë¥¼ í¬í•¨í•˜ëŠ” ë¬¸ì œ ì¡´ì¬  

2) **LOCATE ë°©ë²•ë¡ **  
- **Step 1. Interaction Localization (CAM)**: exocentric ì´ë¯¸ì§€ì—ì„œ ìƒí˜¸ì‘ìš© ì˜ì—­ì„ CAMìœ¼ë¡œ ê²€ì¶œ  
- **Step 2. PartSelect ëª¨ë“ˆ**: ROI ì„ë² ë”©ì„ í´ëŸ¬ìŠ¤í„°ë§ â†’ `ì‚¬ëŒ / ë°°ê²½ / ê°ì²´ ë¶€ìœ„`ë¡œ ë¶„ë¦¬, DINO-ViT featureë¡œ **object part prototype** ì„ íƒ  
- **Step 3. Part-Level Knowledge Transfer**: ì„ íƒëœ object-part prototypeì„ egocentric ì´ë¯¸ì§€ í•™ìŠµì— supervisionìœ¼ë¡œ ì‚¬ìš©  

3) **Loss ì„¤ê³„**  
- **L_cls**: affordance ë¶„ë¥˜ë¥¼ ìœ„í•œ cross-entropy loss  
- **L_cos**: object-part prototypeê³¼ egocentric feature ì •ë ¬ì„ ìœ„í•œ cosine embedding loss  
- **L_c**: affordance ì˜ì—­ ì§‘ì¤‘í™”ë¥¼ ìœ„í•œ concentration loss  
- ìµœì¢… Loss: `L = L_cls + Î»cos * L_cos + Î»c * L_c`  

---

### ğŸ” ê¸°ì¡´ì˜ ê´€ë ¨ ì—°êµ¬ë“¤

1. **Fully Supervised**: AffordanceNet (Do et al., 2018), Myers et al. (2015) â€“ í”½ì…€ ë‹¨ìœ„ ë¼ë²¨ í•„ìš” â†’ ë¹„ìš© í¼  

2. **Weakly Supervised**: Hotspots (Nagarajan et al., 2019), Cross-View-AG (Luo et al., 2022) â€“ CAM ê¸°ë°˜ affordance grounding 
  - Cross-View-AG ê°€ ìµœì‹ ì¸ë°, ë‹¤ë§Œ GKT(Global Knowledge Transferì—¬ì„œì„œ) ì¡ìŒì´ ë„ˆë¬´ ë§ì´ë“¤ì–´ê°„ë‹¤!   
  ![Image](https://github.com/user-attachments/assets/5ba5502f-ac09-456f-8d74-21da6061380e)

3. **LOCATEì˜ ì°¨ë³„ì **: Global feature transfer ëŒ€ì‹  **part-level prototype transfer**ë¡œ ë” ì •ë°€í•˜ê³  ì¼ë°˜í™” ê°€ëŠ¥  

---

### ğŸ” ë³¸ ì—°êµ¬ì˜ ë°©ë²•ë¡ !!!  

#### 3.1 . Locating Interaction Regions(based on CAM)

![Image](https://github.com/user-attachments/assets/4c168707-a6c8-46fd-ab18-c4c7fc138289)

- Input: Image(exo and ego) + label(action)  
- Model : projection + MLP  
- Output : Classification score(z) + CAM  
- Loss : Cross-entropy loss(L_cls)  


#### 3.2 Object-Part Embedding Selection

![Image](https://github.com/user-attachments/assets/49ecc049-edee-4613-8243-18cf0fac12a4)

1. 3.1ì˜ projection + MLP ì— exo ì´ë¯¸ì§€ê²°ê³¼ ì¤‘, zê°€ Ï„(ì¼ì • ì„ê³„ê°’) ì´ìƒì¸ ê²ƒë§Œ ì¶”ì¶œí•¨(L)  
2. ì´ Lë“¤ì„ N ê°œì˜ í´ëŸ¬í„°ë¡œ ë§Œë“¦!  
  -  ì‹¤í—˜ì—ì„œëŠ” Nì„ 3ìœ¼ë¡œ í–ˆê³ , ì‚¬ëŒ/ë°°ê²½/objectë¡œ ê°œë…í™”ë¨!  

![Image](https://github.com/user-attachments/assets/6b70be17-740a-45b0-8955-1e9aedae2e4d)

3. Nê°œì˜ í´ëŸ¬ìŠ¤í„°ë‘ egoì´ë¯¸ì§€ìœ¼ ì¸ì½”ë”© F_egoë‘ Similarityë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¹„êµí•¨  
  - egoì´ë¯¸ì§€ì˜ DINO-VITì„ë² ë”©ì˜ íŒ¨ì¹˜ë³„ í´ëŸ¬ìŠ¤í„°ì™€ì˜ ìœ ì‚¬ë„ë¥¼ ë¹„êµ!  
4. egoë¥¼ DINO-ViTë¥¼ í†µí•´ ì„ë² ë”© ì‚°ì¶œ
5. 3,4ì˜ ê²°ê³¼ë¬¼ì„ PartIOUë¥¼ ê³„ì‚°í•´ì„œ ìµœì¢… saliency ì„ ì •!!


#### 3.3. Part-Level Knowledge Transfer  

![Image](https://github.com/user-attachments/assets/fdf27e74-feef-4192-b54d-8deed047705c)

1. 3.1ì—ì„œì˜ ê²°ê³¼ë¬¼ CAMê³¼ 3.2ì—ì„œì˜ ê²°ê³¼ë¬¼ Saliencyë¥¼ ë™ì¼í•˜ê²Œ ë§Œë“¬ : L_cos (Cosine Similarity loss)  
2. ê²°ê³¼ë¥¼ ë” ì¤‘ì•™ì— ì§‘ì¤‘ë˜ê²Œ ë§Œë“¬ : L_c

#### ê·¸ë˜ì„œ ìµœì¢… LOSSëŠ”!?

L = L_cls + Î»_cos * L_cos + Î»_c * L_c


---

### ğŸ§ª ì‹¤í—˜ ê²°ê³¼ ë° ì„±ëŠ¥  

- **ë°ì´í„°ì…‹**: AGD20K (20k exocentric, 3.7k egocentric, 36 affordance)  

![Image](https://github.com/user-attachments/assets/c78bf980-f489-4623-a388-a7e3d5e134a4)

- Seen/Unseen ëª¨ë‘ì—ì„œ SOTA ë‹¬ì„±  
- ê¸°ì¡´ Cross-View-AG+ ëŒ€ë¹„ **KLD 20.4% â†“, SIM 33.3% â†‘, NSS 31.2% â†‘**  
- íŒŒë¼ë¯¸í„° ìˆ˜ 6.5M, ì¶”ë¡  ì†ë„ 0.011s â†’ íš¨ìœ¨ì   

- Ablation Test!?  

![Image](https://github.com/user-attachments/assets/39e153f7-57b0-47cd-932f-001d6624ea2b)

- **GKT â†’ RKT**: ê¸°ì¡´ GKT ë³´ë‹¤ RKTê°€ íš¨ê³¼ì ì„!!  
- **Lc**: ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë” ì§‘ì¤‘ì‹œí‚¤ëŠ” ë³´ì¡°ì  ì—­í• ë„ ì†Œí­ ìƒìŠ¹ìœ¼ë¡œ ì˜ë¯¸ê°€ ìˆì—ˆê³ ,    
- **PartSelect (S)**: ì„±ëŠ¥ í–¥ìƒì˜ í•µì‹¬ ëª¨ë“ˆë¡œì„œ Part-Level Knowledge Transferê¸°ëŠ¥ í™•ì¸     
- **ìµœì¢… ì¡°í•© (S+Lc)**: Seen/Unseen ëª¨ë‘ì—ì„œ SOTA ì„±ëŠ¥ ë‹¬ì„±  

---

## âœ… ê²°ë¡   

- LOCATEëŠ” **Part-Level Object Affordance Transfer**ë¥¼ í†µí•´ WSAG ë¬¸ì œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°  
- ì£¼ìš” ê¸°ì—¬:  
  1. Exocentric â†’ Egocentric **ë¶€ìœ„ ë‹¨ìœ„ ì§€ì‹ ì „ì´**  
  2. **PartSelect ëª¨ë“ˆ**ë¡œ object part prototype ìë™ ì„ íƒ  
  3. SOTA ì„±ëŠ¥ + íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„± ì…ì¦  
- â†’ ë¡œë´‡ ì§€ê°, ì¸ê°„-ë¡œë´‡ ìƒí˜¸ì‘ìš©, AR/VR ì‘ìš©ì—ì„œ í™œìš© ê°€ëŠ¥ ğŸ¯  

---
