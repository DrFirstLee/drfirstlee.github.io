---
layout: post
title: "Segment Anything, You are amazing! - ëˆ„ë¼ì˜ ê´´ë¬¼, SAMì˜ ë“±ì¥!! (ICCV, 2023)"
author: [DrFirst]
date: 2025-05-04 09:00:00 +0900
categories: [AI, Research]
tags: [SAM, Segment Anything, Vision AI, Meta AI, ICCV, ICCV 2023, Segmentation, FAIR, SA-1B]
lastmod : 2025-05-04 09:00:00
sitemap :
  changefreq : weekly
  priority : 0.9
---


## ğŸ§  What is SAM?

_Studying ã€Segment Anythingã€ (ICCV, 2023)_

![SAM_paper](https://github.com/user-attachments/assets/f322fa22-9511-4c0a-aa91-095e61839712)

ğŸ“– **Paper Title**: Segment Anything  
âœï¸ **Authors**: Meta AI Research (Kirillov, Alexey et al.)  
ğŸŒŸ **One-line Summary**: A general-purpose segmentation model that can segment **anything**, in **any image**, from **any prompt**!

---

### ğŸ“š Key Idea

![manwha](https://github.com/user-attachments/assets/d0b88bb5-e8b7-4172-a34d-5553550b7652)

- **SAM** stands for **Segment Anything Model**
- Unlike traditional segmentation models,
- SAM is a **universal segmentation AI** that can extract any object using a **single pre-trained model**
- Without predefined classes, SAM can **segment targets from user prompts**
- Itâ€™s often called the **â€œGPT for Segmentationâ€** due to its generalization ability

---

### ğŸ” Background of the SAM Research

- The era of Foundation Models:
  - Language models work well with large-scale data
  - In vision, CLIP, ALIGN, and image encoders have emerged
  - But vision segmentation lacks data diversity
- Research Goal: **Build a foundation model for image segmentation**
  - Three key challenges:
    a. Task: What segmentation task to define?
    b. Model: What architecture to use?
    c. Data: How to collect it?

### ğŸ¯ The Task Definition in SAM

- Limitations of existing segmentation models:
  - Rely on **predefined classes**
  - Require **labelled data**
  - Need **fine-tuning** for new objects

- Need for **Prompt-based, Open-Vocabulary segmentation**:
  - With multimodal models like CLIP, now we want:
  - Models that can segment **user-defined targets using text, point, box prompts**

ğŸ‘‰ So SAM was defined as a **"segment-anything" universal segmentation system**

---

### âš™ï¸ SAM Model Architecture

![architecture](https://github.com/user-attachments/assets/c26a899b-8f94-449f-a1f4-499be174c044)

| Component         | Description |
|------------------|-------------|
| **Image Encoder** | Encodes entire image into a fixed embedding (done once) |
| **Prompt Encoder**| Encodes prompts like points, boxes, masks |
| **Mask Decoder**  | Combines image & prompt embeddings to predict segmentation mask |

#### Components in Detail

1. **Image Encoder** (ViT-H, MAE pre-trained)
   - Uses ViT with **Masked Autoencoder (MAE)** training
   - Produces rich visual representation
   - Image embeddings are reused for multiple prompts

2. **Prompt Encoder**
   - Handles two types of inputs:

   | Type     | Example | Encoding | Notes |
   |----------|---------|----------|-------|
   | Sparse   | Point, Box, Text | Position + learned embeddings / CLIP text encoder | Text uses CLIP text encoder |
   | Dense    | Mask    | Convolution + element-wise sum with image embedding | Used for dense prompts like masks |

3. **Mask Decoder**

  ![mask_encoder](https://github.com/user-attachments/assets/48fb43bb-1972-4f70-94d5-210ab9984432)


   - Core logic that fuses prompt and image to output the final mask

   | Step | Description |
   |------|-------------|
   | 1. Input | Image Embedding + Prompt Embedding + Output Token |
   | 2. Decoder Blocks Ã—2 | Transformer decoder variant + self & cross attention |
   | 3. Upsampling | Upsamples decoder output using image embedding |
   | 4. Dynamic Prediction | MLP â†’ Linear classifier to produce per-pixel FG probabilities |
   | 5. Output | Generates 3 mask candidates with confidence scores (to resolve ambiguity) |

---

## ğŸ—ï¸ SA-1B Dataset and the Data Engine

![datasets](https://github.com/user-attachments/assets/9c67fe6a-7497-415c-a512-e34e232a1595)

- **SA-1B**: The largest segmentation dataset ever, built by Meta for SAM
- Contains **11M images** and over **1.1B masks**
- 400Ã— more masks than prior datasets
- âœ… Fully automatic annotation, âœ… High diversity and quality

### ğŸ› ï¸ 3-Stage Data Engine

| Stage | Name | Who | Method | Key Features |
|-------|------|-----|--------|--------------|
| 1ï¸âƒ£ | Assisted-manual | Human + SAM | Human segments, SAM assists | Interactive tool, semantic-free |
| 2ï¸âƒ£ | Semi-automatic | SAM + Human | SAM segments, human fills the rest | Efficient + diverse |
| 3ï¸âƒ£ | Fully-automatic | SAM only | Grid prompts, full automation | ~100 masks/image, 99.1% of SA-1B |

#### Assisted-manual Stage
- Professional annotators use browser tool with SAM
- Click foreground/background points
- Refinement via brush & eraser
- Focused on recognizable objects (but no label stored)
- Moved to next image if took >30 seconds

| Metric | Result |
|--------|--------|
| Avg. annotation time | 34 â†’ 14 sec (6.5Ã— faster than COCO) |
| Masks/image | 20 â†’ 44 |
| Total | 120K images, 4.3M masks |
| Retraining | 6 times total |

#### Semi-automatic Stage

| Metric | Result |
|--------|--------|
| Additional masks | +5.9M (total 10.2M) |
| Images | 180K |
| Retraining | 5 more times |
| Time/image | 34 sec (excluding auto-masks) |
| Masks/image | 44 â†’ 72 |

#### Fully Automatic Stage

- Grid of 32Ã—32 point prompts
- Predicts multiple masks per point (sub-part, part, whole)
- IoU prediction module filters reliable masks
- Stability check with probability thresholding
- Non-Max Suppression (NMS) removes duplicates
- Cropped regions help improve small object coverage

---

### ğŸ“¦ Final SA-1B Dataset Summary

| Aspect | Description |
|--------|-------------|
| Image count | 11M |
| Resolution | Avg. 3300Ã—4950 px |
| Licensing | Licensed from photographers |
| Privacy | Faces/plates blurred |
| Released images | Resized (short side 1500 px) |
| Comparison | Higher-res than COCO (480Ã—640) |

| Masks | Details |
|-------|---------|
| Total | 1.1B masks |
| Auto-generated | 99.1% |
| Human-level quality | 94% of masks have IoU > 90% w/ expert |
| Fair & diverse | Balanced across gender, regions |

---

## ğŸ”¬ Zero-Shot Transfer Experiments

SAM proves it's **not just a segmentation tool**, but a **universal model**.  
Evaluated on 5 tasks without fine-tuning:

| Task | Outcome |
|------|---------|
| 1. Single-Point Mask | Outperforms RITM (auto & human eval) |
| 2. Edge Detection | Strong edges from prompts (even w/o training) |
| 3. Object Proposal | Excellent for mid/rare objects (beats ViTDet) |
| 4. Instance Segmentation | Better visual quality than ViTDet, even if AP is lower |
| 5. Text-to-Mask | Uses CLIP text embeddings for free-text segmentation |

---

### 1ï¸âƒ£ Single-Point Valid Mask

- Only one foreground point â†’ segment object
- Evaluation: mIoU + human rating (1â€“10)
- SAM beats RITM on 16/23 datasets (mIoU), and all datasets (oracle mode)
- Human ratings: 7â€“9 (higher than RITM)

---

### 2ï¸âƒ£ Edge Detection

- Dataset: BSDS500
- Prompted via 16Ã—16 grid
- Sobel edge detection applied to mask probabilities
- Matches early DL models like HED
- Recallâ†‘, Precisionâ†“ due to over-segmentation (expected)

---

### 3ï¸âƒ£ Object Proposal (LVIS)

- Method: Mask output used as object proposals
- Compared to ViTDet-H + Mask R-CNN
- SAM outperforms in:
  - Medium/large objects
  - Common/rare categories
- Falls behind on small/frequent ones

---

### 4ï¸âƒ£ Instance Segmentation

- ViTDet boxes â†’ fed as prompt to SAM
- COCO/LVIS: SAM slightly behind in AP, but
- **Visual quality better** (confirmed via human study)
- Less biased by noisy ground truth (unlike ViTDet)

---

### 5ï¸âƒ£ Text-to-Mask

- Uses CLIP text encoder as prompt
- Training with CLIP image embedding â†’ inference with text embedding
- Example prompts: â€œa wheelâ€, â€œwipersâ€
- Additional point improves ambiguous cases

---

## âœ¨ Final Thoughts

Meta didnâ€™t just build a model â€” they released the **model + high-quality data** with **strong fairness**,  
making a true contribution to the open AI community.

Letâ€™s hope we can do the same in the future â€” building & sharing great models and datasets!

---


## (í•œêµ­ì–´) ğŸ§  SAMì´ë€ ë¬´ì—‡ì¸ê°€?
_ã€Segment Anythingã€(ICCV, 2023) ê³µë¶€_

![SAM_paper](https://github.com/user-attachments/assets/f322fa22-9511-4c0a-aa91-095e61839712)

ğŸ“– **ë…¼ë¬¸ ì œëª©**: Segment Anything  
âœï¸ **ì €ì**: Meta AI Research (Kirillov, Mintun et al.)  
ğŸŒŸ **í•œì¤„ ìš”ì•½**: ì–´ë–¤ ê°ì²´ë“ , ì–´ë–¤ ì´ë¯¸ì§€ë“ , ì–´ë–¤ ì…ë ¥ì´ë“  "ë¬´ì—‡ì´ë“ " ì˜ë¼ë‚´ëŠ” ë²”ìš© ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ì˜ ë“±ì¥!!

---

### ğŸ“š í•µì‹¬ ì•„ì´ë””ì–´

![manwha](https://github.com/user-attachments/assets/d0b88bb5-e8b7-4172-a34d-5553550b7652)

- SAMì€ **Segment Anything Model**ì˜ ì•½ìë¡œ,  
- ê¸°ì¡´ì˜ ì˜ì—­ ë¶„í• (Segmentation) ëª¨ë¸ë“¤ê³¼ëŠ” ë‹¬ë¦¬,  
- **ì–´ë–¤ ê°ì²´ë“  ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ í•˜ë‚˜ë¡œ ì˜ë¼ë‚¼ ìˆ˜ ìˆëŠ” ë²”ìš© Segmentation ì¸ê³µì§€ëŠ¥**ì…ë‹ˆë‹¤!  
- ì¦‰, ë¯¸ë¦¬ ì •ì˜ëœ í´ë˜ìŠ¤ê°€ ì—†ì–´ë„, **"ì‚¬ìš©ì ì…ë ¥(Prompt)"ë§Œìœ¼ë¡œ ì›í•˜ëŠ” ëŒ€ìƒì„ ë¶„ë¦¬**í•  ìˆ˜ ìˆì–´ìš”.  
- SAMì€ "Segmentationì„ ìœ„í•œ GPT"ë¼ê³  ë¶ˆë¦´ ì •ë„ë¡œ ë²”ìš©ì„±ì´ ê°•ë ¥í•©ë‹ˆë‹¤.

---

### ğŸ” SAM ì—°êµ¬ì˜ ë°°ê²½

- ë°”ì•¼íë¡œ Foundation Modelì˜ ì‹œëŒ€!!  
  - ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¡œ Language Modelë“¤ì€ ë†€ëê²Œ ì˜ ì‘ë™!!  
  - Computer Vision ì—ì„œë„ CLIP, ALIGN ë“± ì´ë¯¸ì§€ ì¸ì½”ë”ë“¤ì´ ë“±ì¥, ì´ë¯¸ì§€ ìƒì„±ì—ë„ ì˜í–¥ì„ ë¯¸ì¹¨!!  
  - í•˜ì§€ë§Œ, Vision ë°ì´í„°ì˜ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ í•œê³„ê°€ ìˆì—ˆìŒ!!
- ê·¸ë˜ì„œ!! ì´ë²ˆ ì—°êµ¬ì˜ ëª©í‘œëŠ” "build a foundation model for image segmentation" ìœ¼ë¡œ ì •ì˜!!  
  - ê·¸ë¦¬ê³  ì„±ê³µì  ì—°êµ¬ë¥¼ ìœ„í•´ì„œ ì•„ë˜ 3ê°€ì§€ ìš”ì†Œë¥¼ ê³ ë¯¼í•¨!!  
    a. ê³¼ì œ : ì–´ë–¤ ê³¼ì œë¥¼ ì„¤ì •í• ê²ƒì¸ê°€!  
    b. ëª¨ë¸ : ì–´ë–¤ ëª¨ë¸ì„ ì“¸ê²ƒì¸ê°€!  
    c. ë°ì´í„° : ì–´ë–¤ ë°ì´í„°ë¥¼ ì“¸ê²ƒì¸ê°€!  

### SAM ì—°êµ¬ì˜ ê³¼ì œ(Task)ì˜ ì •ì˜

- ê¸°ì¡´ Segmentation ëª¨ë¸ì˜ í•œê³„
  - ëŒ€ë¶€ë¶„ì˜ segmentation ëª¨ë¸ì€ **ì‚¬ì „ ì •ì˜ëœ í´ë˜ìŠ¤(class)**ê°€ ìˆì–´ì•¼ í•™ìŠµ ê°€ëŠ¥
  - íŠ¹ì • ê°ì²´(ex: ê³ ì–‘ì´, ê°œ, ìë™ì°¨)ë§Œ ë¶„í•  ê°€ëŠ¥í•˜ë©°, **ë¼ë²¨ë§ ë°ì´í„°ì— ë§¤ìš° ì˜ì¡´**
  - ìƒˆë¡œìš´ í´ë˜ìŠ¤ì— ëŒ€í•´ì„  **ì¬í•™ìŠµ(fine-tuning)**ì´ í•„ìš”

- Open-Vocabulary, Prompt ê¸°ë°˜ ëª¨ë¸ í•„ìš”ì„±
  - ìµœê·¼ CLIP ë“± ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì˜ ë“±ì¥ê³¼ í•¨ê»˜,  
  - "í…ìŠ¤íŠ¸"ë‚˜ "í¬ì¸íŠ¸" ë“±ì„ í†µí•´ **ì‚¬ìš©ì ì¤‘ì‹¬ìœ¼ë¡œ ê°ì²´ë¥¼ ì§€ì •í•˜ê³  ë¶„í• í•˜ëŠ” ëª¨ë¸**ì´ ìš”êµ¬ë¨

- ê·¸ë˜ì„œ!! **"ë¬´ì—‡ì´ë“  ì˜ë¼ë‚´ëŠ” ë²”ìš© ë¶„í• ê¸°"**ë¥¼ ê³¼ì œë¡œ ì •ì˜!

--- 

### âš™ï¸ SAMì˜ ëª¨ë¸ êµ¬ì¡°

![architecture](https://github.com/user-attachments/assets/c26a899b-8f94-449f-a1f4-499be174c044)

| êµ¬ì„± ìš”ì†Œ       | ì„¤ëª… |
|----------------|------|
| **Image Encoder** | ì´ë¯¸ì§€ ì „ì²´ë¥¼ ì¸ì½”ë”©í•˜ì—¬ ê³ ì •ëœ **image embedding** ìƒì„± (í•œ ë²ˆë§Œ ìˆ˜í–‰) |
| **Prompt Encoder** | ì , ë°•ìŠ¤, ë§ˆìŠ¤í¬ ë“± ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì¸ì½”ë”© |
| **Mask Decoder** | ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©ì„ ê²°í•©í•˜ì—¬ **ë§ˆìŠ¤í¬ ì˜ˆì¸¡** ìˆ˜í–‰ |

#### SAMì€ ì„¸ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œ ë° ê·¸ ê¸°ëŠ¥!!  

1. **Image Encoder** (ViT-H ê¸°ë°˜)
   - MAE (Masked Autoencoders) ë°©ì‹ì˜ ViTë¥¼ ì‚¬ìš©! - MAEê°€ ë­”ì§€ ê³µë¶€í•´ë³´ì!!  
    | Masked Autoencoders Are Scalable Vision Learners (CVPR, 2022)  
   - ì´ë¯¸ì§€ë¥¼ ê³ í•´ìƒë„ë¡œ ì¸ì½”ë”©í•˜ì—¬ í’ë¶€í•œ ì‹œê° í‘œí˜„ ìƒì„±
   - í•œë²ˆ ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ëŠ” ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ì—ë„ ì¬ì‚¬ìš© ê°€ëŠ¥

2. **Prompt Encoder** 
   - ì‚¬ìš©ìì˜ ì…ë ¥ì„ ì¸ì½”ë”©
   - ì…ë ¥ ì¢…ë¥˜ - í¬ê²Œ 2ê°€ì§€!  

    | ì¢…ë¥˜ | ì˜ˆì‹œ | ì¸ì½”ë”© ë°©ì‹ | ì„¤ëª… |
    |------|------|-------------|------|
    | **í¬ì†Œ (Sparse)** | Point, Box, **Text** | ìœ„ì¹˜ + í•™ìŠµëœ ì„ë² ë”© / í…ìŠ¤íŠ¸ ì¸ì½”ë”(CLIP) | - ìœ„ì¹˜ ì •ë³´ì— Positional Encoding + í•™ìŠµëœ ì„ë² ë”© <br> - **í…ìŠ¤íŠ¸ëŠ” CLIP í…ìŠ¤íŠ¸ ì¸ì½”ë” ì‚¬ìš©** |
    | **ë°€ì§‘ (Dense)** | Mask | Convolution + Element-wise Sum | - ë§ˆìŠ¤í¬ë¥¼ Convë¡œ ì„ë² ë”© í›„ <br> ì´ë¯¸ì§€ ì„ë² ë”©ê³¼ ì›ì†Œ ë‹¨ìœ„ í•©ì‚° |

3. **Mask Decoder**
  ![mask_encoder](https://github.com/user-attachments/assets/48fb43bb-1972-4f70-94d5-210ab9984432)

   - ì¸ì½”ë” ì¶œë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë§ˆìŠ¤í¬ë¥¼ ìƒì„±
   - ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ëŠ” í•µì‹¬ êµ¬ì„±
   - ğŸ”§ ì£¼ìš” êµ¬ì„± ìš”ì†Œ ë° ì²˜ë¦¬ ê³¼ì •

    | ë‹¨ê³„ | ì„¤ëª… |
    |------|------|
    | 1. ì…ë ¥ | - Image Embedding <br> - Prompt Embedding (Point, Box, Text ë“±) <br> - Output Token |
    | 2. ë””ì½”ë” ë¸”ë¡ (Ã—2) | - **Transformer Decoder** ë³€í˜• ë²„ì „ ì‚¬ìš© <br> - **Prompt Self-Attention** <br> - **Cross-Attention** (Prompt â†” Image ì„ë² ë”©) ì–‘ë°©í–¥ ìˆ˜í–‰ |
    | 3. ì—…ìƒ˜í”Œë§ | - ë””ì½”ë” ì¶œë ¥ì—ì„œ **Image Embeddingì„ ì—…ìƒ˜í”Œë§** |
    | 4. ë™ì  ë§ˆìŠ¤í¬ ì˜ˆì¸¡ | - Output Token â†’ **MLP â†’ ë™ì  Linear Classifier** <br> - ê° í”½ì…€ ìœ„ì¹˜ë§ˆë‹¤ **Foreground í™•ë¥  ê³„ì‚°** |
    | 5. ìµœì¢… ì¶œë ¥ | - ì „ê²½ í™•ë¥ (foreground probability) ë§µ â†’ **Binary Mask** ì¶œë ¥ |

    ï¼ëª¨í˜¸ì„±ì˜ í•´ê²°ì„ ìœ„í•˜ì—¬! : **ì„¸ ê°œì˜ í›„ë³´ ë§ˆìŠ¤í¬**ë¥¼ ì¶œë ¥, ê° ë§ˆìŠ¤í¬ ë³„ í™•ì‹ ë„(uncertainty score) ì œê³µ

---


### ğŸ—ï¸ SAMì˜ ë°ì´í„° (SA-1B) ë°  ë°ì´í„° ì—”ì§„

![datasets](https://github.com/user-attachments/assets/9c67fe6a-7497-415c-a512-e34e232a1595)

- **SA-1B**: SAM í•™ìŠµì„ ìœ„í•´ Metaê°€ ë§Œë“  **ì´ˆëŒ€ê·œëª¨ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°ì´í„°ì…‹**  
- ì´ **11Mê°œì˜ ì´ë¯¸ì§€**ì—ì„œ ìë™ìœ¼ë¡œ ìˆ˜ì§‘ëœ **1B+ ë§ˆìŠ¤í¬**  
- ê¸°ì¡´ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°ì´í„°ì…‹ë³´ë‹¤ **400ë°° ë” ë§ì€ ë§ˆìŠ¤í¬ ë³´ìœ **  
- âœ… **ì™„ì „ ìë™ ìˆ˜ì§‘**, âœ… **ê³ í’ˆì§ˆ & ë‹¤ì–‘ì„± ë³´ì¥**  
- SAMì˜ **ë²”ìš©ì„± ë° ê²¬ê³ ì„± í™•ë³´**ì— í•µì‹¬ ì—­í•   
- ğŸ“š í–¥í›„ **íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ ì—°êµ¬ë¥¼ ìœ„í•œ ê³µê³µ ìì›**ìœ¼ë¡œ í™œìš© ê°€ëŠ¥  
- SA-1B ë°ì´í„°ì…‹ ìƒì„± ì ˆì°¨ ìš”ì•½ í‘œ

| ë‹¨ê³„ | ëª…ì¹­ | ì£¼ì²´ | ì£¼ìš” ì‘ì—… | íŠ¹ì§• |
|------|------|------|-----------|------|
| 1ï¸âƒ£ | **ë³´ì¡° ìˆ˜ë™ ì£¼ì„ (Assisted-manual)** | ì‚¬ëŒ + SAM | ì‚¬ëŒì´ ë§ˆìŠ¤í¬ë¥¼ ì§ì ‘ ë§Œë“¤ê³ , SAMì´ ë³´ì¡° | ì¸í„°ë™í‹°ë¸Œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ì‹, ì´ˆê¸° í’ˆì§ˆ í™•ë³´ |
| 2ï¸âƒ£ | **ë°˜ìë™ ì£¼ì„ (Semi-automatic)** | SAM + ì‚¬ëŒ | SAMì´ ì¼ë¶€ ê°ì²´ ë§ˆìŠ¤í¬ ìƒì„±, ì‚¬ëŒì€ ë‚˜ë¨¸ì§€ë¥¼ ì£¼ì„ | ë‹¤ì–‘ì„± í–¥ìƒ, ì‹œê°„ íš¨ìœ¨ ì¦ê°€ |
| 3ï¸âƒ£ | **ì™„ì „ ìë™ ì£¼ì„ (Fully automatic)** | SAM | SAMì´ í¬ì¸íŠ¸ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ ë§ˆìŠ¤í¬ ìƒì„± | ì´ë¯¸ì§€ë‹¹ í‰ê·  100ê°œ ë§ˆìŠ¤í¬, SA-1B ëŒ€ë¶€ë¶„ êµ¬ì„± |


#### 1ë‹¨ê³„: Assisted-Manual Stage  
- ë¸Œë¼ìš°ì € ê¸°ë°˜ ì¸í„°ë™í‹°ë¸Œ íˆ´ì—ì„œ **SAMì´ ì‹¤ì‹œê°„ìœ¼ë¡œ ì£¼ì„ ì§€ì›**  
- ì „ë¬¸ ì£¼ì„ìê°€ **ì „ê²½/ë°°ê²½ í¬ì¸íŠ¸ í´ë¦­**í•˜ì—¬ ë§ˆìŠ¤í¬ ìƒì„±  
- ë¸ŒëŸ¬ì‹œ & ì§€ìš°ê°œë¡œ ì •ë°€ ìˆ˜ì • ê°€ëŠ¥  
- **"ì„¤ëª… ê°€ëŠ¥í•œ" ê°ì²´ ì¤‘ì‹¬**ìœ¼ë¡œ ììœ ë¡­ê²Œ ë¼ë²¨ë§ (semantic ì œí•œ ì—†ìŒ)  
- ë§ˆìŠ¤í¬ì— ì´ë¦„/ì„¤ëª…ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ  
- **30ì´ˆ ì´ìƒ ê±¸ë¦¬ë©´ ë‹¤ìŒ ì´ë¯¸ì§€ë¡œ ë„˜ì–´ê°**  
- ìˆ˜ì§‘ëœ ë§ˆìŠ¤í¬ë¡œ 6íšŒ ì¬í•™ìŠµ!!  

##### ğŸ” ëª¨ë¸ í–¥ìƒ ê³¼ì •

| í•­ëª© | ë‚´ìš© |
|------|------|
| ì´ˆê¸° ëª¨ë¸ | ê³µê°œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°ì´í„°ë¡œ í•™ìŠµëœ SAM |
| ë°˜ë³µ í•™ìŠµ | ìˆ˜ì§‘ëœ ë§ˆìŠ¤í¬ë§Œìœ¼ë¡œ ì´ **6íšŒ ì¬í•™ìŠµ** |
| ViT ë°±ë³¸ | ViT-B â†’ ViT-Hë¡œ ì ì§„ì  í™•ì¥ |
| êµ¬ì¡° ê°œì„  | ë‹¤ì–‘í•œ ì„¸ë¶€ êµ¬ì¡° ì§„í™” í¬í•¨ |

##### ğŸ“ˆ ì„±ëŠ¥ ê°œì„  ì§€í‘œ

| ì§€í‘œ | ë³€í™” |
|------|------|
| í‰ê·  ì£¼ì„ ì‹œê°„ | 34ì´ˆ â†’ 14ì´ˆ (COCOë³´ë‹¤ 6.5ë°° ë¹ ë¦„) |
| í‰ê·  ë§ˆìŠ¤í¬ ìˆ˜ | ì´ë¯¸ì§€ë‹¹ 20ê°œ â†’ 44ê°œ |
| ìˆ˜ì§‘ëŸ‰ | 12ë§Œ ì´ë¯¸ì§€, **430ë§Œ ë§ˆìŠ¤í¬** ìˆ˜ì§‘ ì™„ë£Œ |


#### 2ë‹¨ê³„: Semi-Automatic Stage
- ì´ ë‹¨ê³„ëŠ” **"ìë™ + ìˆ˜ë™ í˜‘ì—… êµ¬ì¡°"**ë¡œ, **ë” ì–´ë ¤ìš´ ê°ì²´**, **ë” ë‹¤ì–‘í•œ ê°ì²´**ë¥¼ ì»¤ë²„í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ìˆ˜í–‰
- **ë§ˆìŠ¤í¬ ë‹¤ì–‘ì„± í–¥ìƒ**ì„ í†µí•´ SAMì˜ ë²”ìš© ë¶„í•  ëŠ¥ë ¥ ê°•í™”
  1. 1ë‹¨ê³„ ë§ˆìŠ¤í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **"object" í´ë˜ìŠ¤ í•˜ë‚˜ë¡œ ë°”ìš´ë”© ë°•ìŠ¤ íƒì§€ê¸°** í•™ìŠµ
  2. **ìë™ íƒì§€ëœ ë§ˆìŠ¤í¬(confident masks)**ë¥¼ ì´ë¯¸ì§€ì— ë¯¸ë¦¬ ì‚½ì…
  3. ì£¼ì„ìëŠ” **ìë™ ë§ˆìŠ¤í¬ ì™¸ì˜ ëˆ„ë½ëœ ê°ì²´ë§Œ ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€ ì£¼ì„**

##### ğŸ“ˆ ì„±ëŠ¥ ë° ìˆ˜ì¹˜

| í•­ëª© | ë‚´ìš© |
|------|------|
| ìˆ˜ì§‘ ë§ˆìŠ¤í¬ ìˆ˜ | 590ë§Œ ê°œ ì¶”ê°€ ìˆ˜ì§‘ (ì´ 1,020ë§Œ ê°œ ë„ë‹¬) |
| ì´ë¯¸ì§€ ìˆ˜ | 18ë§Œ ì¥ |
| SAM ì¬í•™ìŠµ íšŸìˆ˜ | 5íšŒ ë°˜ë³µ í•™ìŠµ |
| í‰ê·  ì£¼ì„ ì‹œê°„ | 34ì´ˆ (ìë™ ë§ˆìŠ¤í¬ ì œì™¸) |
| ì´ë¯¸ì§€ë‹¹ í‰ê·  ë§ˆìŠ¤í¬ ìˆ˜ | 44ê°œ â†’ 72ê°œ (ìë™ + ìˆ˜ë™ í¬í•¨) |

#### 3ë‹¨ê³„: Fully Automatic Stage

  - 2ë‹¨ê³„ê¹Œì§€ ë°ì´í„°ë¡œ í•™ìŠµëœ ëª¨ë¸ë¡œ, ì™„ì „ ìë™ìœ¼ë¡œ!!, ë°ì´í„°ì…‹ ìƒì„±!!
  - ì´ë¡œì¨ **Segment Anythingì˜ SA-1B ë°ì´í„°ì…‹**ì´ ì™„ì„±
  - SAM ëª¨ë¸ë„ ì¤‘ìš”í•˜ì§€ë§Œ, ì´ì²˜ëŸ¼ ë²”ìš© ë¶„í•  ëª¨ë¸ í•™ìŠµì— ìˆì–´ ì „ë¡€ ì—†ëŠ” ë¦¬ì†ŒìŠ¤ ì œê³µí–ˆë‹¤ëŠ”ì ë„ í° ì˜ë¯¸!!

##### ğŸ”§ ìë™ ìƒì„± ì ˆì°¨

1. **32Ã—32 í¬ì¸íŠ¸ ê·¸ë¦¬ë“œ**ë¡œ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸
2. ê° í¬ì¸íŠ¸ì— ëŒ€í•´ **ë‹¤ì¤‘ ë§ˆìŠ¤í¬ ì˜ˆì¸¡**
   - ì˜ˆ: "íŒ”" í¬ì¸íŠ¸ â†’ íŒ” / íŒ”+ëª¸í†µ / ì „ì²´ ì‚¬ëŒ ë§ˆìŠ¤í¬
3. **IoU ì˜ˆì¸¡ ëª¨ë“ˆ**ë¡œ **ì‹ ë¢°ë„ ë†’ì€ ë§ˆìŠ¤í¬ë§Œ ì„ íƒ**
4. **ì•ˆì •ì„± ê²€ì‚¬**:
   - í™•ë¥ ë§µì„ 0.5, 0.55 ë“±ìœ¼ë¡œ thresholdí•´ë„ ë¹„ìŠ·í•˜ë©´ "ì•ˆì •ëœ ë§ˆìŠ¤í¬"
5. **NMS (Non-Max Suppression)**ë¡œ **ì¤‘ë³µ ì œê±°**
6. **ì‘ì€ ê°ì²´ ë³´ì™„**ì„ ìœ„í•´ **í™•ëŒ€ëœ ì´ë¯¸ì§€ crop**ë„ ë³‘ë ¬ ì²˜ë¦¬


#### ìµœì¢… ìƒì„±ëœ ë°ì´í„°(SA-1B)ëŠ”!? 

##### ğŸ–¼ï¸ ì´ë¯¸ì§€ êµ¬ì„±

| í•­ëª© | ë‚´ìš© |
|------|------|
| ì´ë¯¸ì§€ ìˆ˜ | **11,000,000ì¥** |
| í•´ìƒë„ | í‰ê·  **3300 Ã— 4950** í”½ì…€ |
| ì¶œì²˜ | **ì‚¬ì§„ì‘ê°€ì™€ ì§ì ‘ í˜‘ì—…í•˜ëŠ” ê³µê¸‰ì—…ì²´**ë¡œë¶€í„° ë¼ì´ì„ ìŠ¤ íšë“ |
| ë³´í˜¸ ì¡°ì¹˜ | ì–¼êµ´ ë° ì°¨ëŸ‰ ë²ˆí˜¸íŒ **ë¸”ëŸ¬ ì²˜ë¦¬** í¬í•¨ |
| ë°°í¬ í˜•ì‹ | ìµœë‹¨ ë³€ ê¸°ì¤€ **1500í”½ì…€ ë‹¤ìš´ìƒ˜í”Œ ë²„ì „** ì œê³µ |
| ë¹„êµ | COCO: 480Ã—640 â†’ SA-1BëŠ” í›¨ì”¬ ë” ê³ í•´ìƒë„ |

##### ğŸ§© ë§ˆìŠ¤í¬ êµ¬ì„±

| í•­ëª© | ë‚´ìš© |
|------|------|
| ì´ ë§ˆìŠ¤í¬ ìˆ˜ | **1.1B (11ì–µ ê°œ)** |
| ìƒì„± ë°©ì‹ | **99.1% ìë™ ìƒì„±** (Fully Automatic Stage) |
| í¬í•¨ ë§ˆìŠ¤í¬ | ìµœì¢…ì ìœ¼ë¡œëŠ” **ìë™ ìƒì„±ëœ ë§ˆìŠ¤í¬ë§Œ í¬í•¨**ë¨ |
| í’ˆì§ˆ í‰ê°€ | ì „ë¬¸ê°€ ë³´ì • ëŒ€ë¹„ 94%ê°€ **IoU > 90%** ìˆ˜ì¤€ì˜ ì¼ì¹˜ìœ¨ |

##### ğŸ” í’ˆì§ˆ ê²€ì¦: ìë™ vs ì „ë¬¸ê°€ì˜ ê²€ì¦!!

- ë¬´ì‘ìœ„ 500ê°œ ì´ë¯¸ì§€(ì´ 5ë§Œ ë§ˆìŠ¤í¬)ë¥¼ ìƒ˜í”Œë§í•˜ì—¬  
  ì „ë¬¸ê°€ê°€ ë¸ŒëŸ¬ì‹œ & ì§€ìš°ê°œë¡œ ë§ˆìŠ¤í¬ë¥¼ **ì •êµí•˜ê²Œ ë³´ì •**
- ê·¸ ê²°ê³¼:
  - **94%ì˜ ë§ˆìŠ¤í¬ ìŒì´ IoU > 90%**
  - **97%ëŠ” IoU > 75%**
- ì°¸ê³ : ê¸°ì¡´ ë…¼ë¬¸ ê¸°ì¤€ **ì‚¬ëŒ ê°„ IoU ì¼ì¹˜ìœ¨ì€ 85~91%** ìˆ˜ì¤€
- â‡’ SAMì˜ ìë™ ë§ˆìŠ¤í¬ëŠ” **ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ í’ˆì§ˆ í™•ë³´**


##### PCì˜ ë°ì´í„°!!  
- ë‚¨ì„± ì—¬ì„±, ìœ ëŸ½ ì•„ì‹œì•„ ì•„í”„ë¼ì¹´ ë“± ì–´ë–¤ì ì—ì„œë„ ì¹˜ìš°ì¹˜ì§€ ì•Šì€ Fairness ë°ì´í„°ì—ìš”!!^^*  

---

### Zero-Shot Transfer Experiments (SAMì˜ ë²”ìš©ì„± ì‹¤í—˜)

SAM(Segment Anything Model)ì€ ë‹¨ìˆœíˆ ì´ë¯¸ì§€ì— ë§ˆìŠ¤í¬ë¥¼ ê·¸ë¦¬ëŠ” ë„êµ¬ë¥¼ ë„˜ì–´!!    
**ì¶”ê°€ í•™ìŠµ ì—†ì´ ë‹¤ì–‘í•œ ë¹„ì „ ê³¼ì œì— ì§ì ‘ ì ìš© ê°€ëŠ¥í•œ ë²”ìš© ëª¨ë¸**ì´ë¼ëŠ” ì ì„ ì‹¤í—˜ì„ í†µí•´ ì…ì¦  
ì´ **5ê°€ì§€ ì‹¤í—˜**ì„ í†µí•´ SAMì˜ Zero-Shot ì„±ëŠ¥ì„ ì¸¡ì • 

---

#### ğŸ§­ Zero-shot ì‹¤í—˜ ê°œìš”

- **Zero-Shot Transfer**: SAMì€ í•™ìŠµì— ì‚¬ìš©ë˜ì§€ ì•Šì€ ë°ì´í„°ì…‹ê³¼ ì‘ì—…ì— ëŒ€í•´ ì§ì ‘ ì ìš©
- **í‰ê°€ ëŒ€ìƒ ê³¼ì œ 5ì¢…**:
  1. Single-Point Valid Mask (ë‹¨ì¼ í¬ì¸íŠ¸ ê°ì²´ ë¶„í• )
  2. Edge Detection (ì—ì§€ ê°ì§€)
  3. Object Proposal Generation (ê°ì²´ ì œì•ˆ)
  4. Instance Segmentation (ì¸ìŠ¤í„´ìŠ¤ ë¶„í• )
  5. Text-to-Mask (í…ìŠ¤íŠ¸ â†’ ë§ˆìŠ¤í¬)

- ì‹¤í—˜ ìš”ì•½!  

| ì‹¤í—˜ | ê²°ê³¼ ìš”ì•½ |
|------|------------|
| Single-Point Mask | RITM ëŒ€ë¹„ ì •ì„±Â·ì •ëŸ‰ ì„±ëŠ¥ ëª¨ë‘ ìš°ìˆ˜ |
| Edge Detection | í•™ìŠµ ì—†ì´ë„ ì˜ë¯¸ ìˆëŠ” ì—ì§€ ì¶”ì¶œ ê°€ëŠ¥ |
| Object Proposal | ì¤‘ê°„/í¬ê·€ ê°ì²´ ì œì•ˆì—ì„œ ìµœê³  ìˆ˜ì¤€ ì„±ëŠ¥ |
| Instance Segmentation | APëŠ” ë‚®ì§€ë§Œ ì‹œê°ì  í’ˆì§ˆê³¼ ì‚¬ìš©ì í‰ê°€ ìš°ìˆ˜ |
| Text-to-Mask | CLIP ì„ë² ë”© í™œìš©í•´ ìì—°ì–´ ë¶„í• ê¹Œì§€ í™•ì¥ ì„±ê³µ |

---

##### 1ï¸âƒ£ ë‹¨ì¼ í¬ì¸íŠ¸ ê°ì²´ ë¶„í•  (Single-Point Valid Mask)

- **ì„¤ì •**: ì „ê²½ í¬ì¸íŠ¸ í•˜ë‚˜ë§Œìœ¼ë¡œ ê°ì²´ ë¶„í• 
- **í‰ê°€**: mIoU + ì‚¬ëŒ ì£¼ì„ì í‰ê°€ (1~10ì )
- **ê²°ê³¼**:  
  - 23ê°œ ì¤‘ 16ê°œ ë°ì´í„°ì…‹ì—ì„œ RITM ëŒ€ë¹„ mIoU ìš°ìœ„
  - Oracle ì„ íƒ ì‹œ ì „ ë°ì´í„°ì…‹ì—ì„œ RITM ëŠ¥ê°€
  - ì‚¬ëŒ í‰ê°€ëŠ” **7~9ì **ìœ¼ë¡œ RITMë³´ë‹¤ ì¼ê´€ë˜ê²Œ ë†’ìŒ
  - SAMì€ ëª¨í˜¸í•œ ì…ë ¥ì—ì„œë„ ìœ íš¨í•œ ë§ˆìŠ¤í¬ ìƒì„± ëŠ¥ë ¥ ì…ì¦

---

##### 2ï¸âƒ£ ì—£ì§€ ê°ì§€ (Zero-Shot Edge Detection)

- **ì„¤ì •**: BSDS500ì—ì„œ ì—ì§€ ê°ì§€ ìˆ˜í–‰
  - 16Ã—16 í¬ì¸íŠ¸ë¡œ SAMì„ í”„ë¡¬í”„íŠ¸ â†’ Sobel í•„í„°ë¡œ ê²½ê³„ ì¶”ì¶œ
- **ê²°ê³¼**:
  - ì—ì§€ ê°ì§€ìš© í•™ìŠµ ì—†ì´ë„ ì˜ë¯¸ ìˆëŠ” ì—ì§€ ë§µ ìƒì„±
  - ìµœì‹  ê¸°ë²•ë³´ë‹¨ ì •ë°€ë„ëŠ” ë‚®ì§€ë§Œ, HED ë“± ì´ˆê¸° ë”¥ëŸ¬ë‹ ëª¨ë¸ ìˆ˜ì¤€ ì´ìƒ
  - Zero-shot ì¹˜ê³  ë§¤ìš° ìš°ìˆ˜í•œ ì„±ëŠ¥

---

##### 3ï¸âƒ£ ê°ì²´ ì œì•ˆ (Zero-Shot Object Proposal)

- **ì„¤ì •**: LVISì—ì„œ ì œì•ˆëœ ë§ˆìŠ¤í¬ë“¤ë¡œ ê°ì²´ ì œì•ˆ
- **ë¹„êµ**: ViTDet-H + Mask R-CNN (DMP ë°©ë²•)
- **í‰ê°€ ì§€í‘œ**: Average Recall (AR@1000)
- **ê²°ê³¼**:
  - **ì¤‘ê°„/í° ê°ì²´, í¬ê·€/ì¼ë°˜ ê°ì²´**ì—ì„œ ViTDet-Hë³´ë‹¤ ìš°ìˆ˜
  - **ì‘ì€ ê°ì²´**ì—ì„œëŠ” ViTDet-Hê°€ ìš°ì„¸ (LVISì— íŠ¹í™”ëœ í•™ìŠµ ë•Œë¬¸)
  - Ambiguity-aware ë²„ì „ì´ ì••ë„ì  ì„±ëŠ¥ í–¥ìƒ ì œê³µ

##### 4ï¸âƒ£ ì¸ìŠ¤í„´ìŠ¤ ì„¸ë¶„í™” (Zero-Shot Instance Segmentation)

- **ì„¤ì •**: ê°ì§€ê¸°(ViTDet) ë°•ìŠ¤ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ SAMì— ë§ˆìŠ¤í¬ ìƒì„±
- **ê²°ê³¼**:
  - COCO/LVISì—ì„œ APëŠ” ViTDetë³´ë‹¤ ë‚®ì§€ë§Œ
  - **ê²½ê³„ í’ˆì§ˆì€ SAMì´ ë” ìš°ìˆ˜**
  - ì‚¬ëŒ í‰ê°€ì—ì„œë„ SAM ë§ˆìŠ¤í¬ê°€ ë” ë†’ê²Œ í‰ê°€ë¨
- **ë¶„ì„**:
  - COCOëŠ” í’ˆì§ˆ ë‚®ì€ GT â†’ ViTDetëŠ” ë°ì´í„° í¸í–¥ í•™ìŠµ
  - SAMì€ ê·¸ëŸ° í¸í–¥ ì—†ì´ **ë³´ë‹¤ ì¼ë°˜ì ì¸ ë¶„í•  ìˆ˜í–‰**

##### 5ï¸âƒ£ í…ìŠ¤íŠ¸ â†’ ë§ˆìŠ¤í¬ (Zero-Shot Text-to-Mask)

- **ì„¤ì •**: í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë§Œìœ¼ë¡œ ê°ì²´ ë¶„í•   
  - CLIPì˜ ì´ë¯¸ì§€ ì„ë² ë”© â†” í…ìŠ¤íŠ¸ ì„ë² ë”© ì •ë ¬ì„ ì´ìš©í•´ í•™ìŠµ
- **ê²°ê³¼**:
  - "a wheel", "beaver tooth grille" ë“± ìì—°ì–´ë¡œ ê°ì²´ ë¶„í•  ê°€ëŠ¥
  - í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ì˜ ì•ˆë  ê²½ìš°, í¬ì¸íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€í•˜ë©´ ê°œì„ ë¨
- **ì‹œì‚¬ì **:
  - **SAMì€ ë©€í‹°ëª¨ë‹¬ ì¸í„°í˜ì´ìŠ¤**ë¡œ ë°œì „ ê°€ëŠ¥ì„±ì´ í¼


---

### âœ¨ ë§ˆë¬´ë¦¬í•˜ë©°

ë‹¨ìˆœíˆ ì—°êµ¬ë§Œ í•˜ê¸°ë„ ë°”ìœë°,, ë¹…í…Œí¬ ê¸°ì—…ì—ì„œ ì—°êµ¬ ëª¨ë¸ + ë°ì´í„°ì…‹ì„ ê³µê°œí•´ì¤€ë‹¤ëŠ”ê²ƒì€!  
ê²Œë‹¤ê°€ Fairnessë¥¼ ê°–ì¶˜ ì¢‹ì€ë°ì´í„°ë¥¼ ì œê³µí•´ì¤€ë‹¤ëŠ” ê²ƒì€ ì°¸ ê³ ë§ˆìš´ ì¼ì¸ê²ƒ ê°™ìŠµë‹ˆë‹¤!  
ì–¸ì  ê°„! ìš°ë¦¬ë„ ë†’ì€ í’ˆì§ˆì˜ ë°ì´í„°ì…‹ê³¼ ê³ ì„±ëŠ¥ì˜ ëª¨ë¸ì„ ê³µê°œí•˜ëŠ” ë‚ ì´ ì˜¤ê¸°ë¥¼!@!!  

---

