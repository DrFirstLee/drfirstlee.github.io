---
layout: post
title: "Segment Anything, You are amazing! - SAM의 등장!! (ICCV, 2023)"
author: [DrFirst]
date: 2025-05-04 09:00:00 +0900
categories: [AI, Research]
tags: [SAM, Segment Anything, Vision, Meta AI, ICCV, ICCV 2023, Segmentation, FAIR, SA-1B]
lastmod : 2025-05-04 09:00:00
sitemap :
  changefreq : weekly
  priority : 0.9
---

## (한국어) 🧠 SAM이란 무엇인가?
_『Segment Anything』(ICCV, 2023) 공부_

![SAM_paper](https://github.com/user-attachments/assets/sample-sam-image.png)

📖 **논문 제목**: Segment Anything  
✍️ **저자**: Meta AI Research (Kirillov, Alexey et al.)  
🌟 **한줄 요약**: 어떤 객체든, 어떤 이미지든, 어떤 입력이든 "무엇이든" 잘라내는 범용 세그멘테이션 모델의 등장!!

---

### 📚 핵심 아이디어

- SAM은 **Segment Anything Model**의 약자로,  
- 기존의 영역 분할(Segmentation) 모델들과는 달리,  
- **어떤 객체든 사전 학습된 모델 하나로 잘라낼 수 있는 범용 Segmentation 인공지능**입니다!  
- 즉, 미리 정의된 클래스가 없어도, **"사용자 입력(Prompt)"만으로 원하는 대상을 분리**할 수 있어요.  
- SAM은 "Segmentation을 위한 GPT"라고 불릴 정도로 범용성이 강력합니다.

---

### 🔍 SAM 연구의 배경

- 바야흐로 Foundation Model의 시대!!  
  - 대량의 데이터로 Language Model들은 놀랍게 잘 작동!!  
  - Computer Vision 에서도 CLIP, ALIGN 등 이미지 인코더들이 등장, 이미지 생성에도 영향을 미침!!  
  - 하지만, Vision 데이터의 부족으로 인해 한계가 있었음!!
- 그래서!! 이번 연구의 목표는 "build a foundation model for image segmentation" 으로 정의!!  
  - 그리고 성공적 연구를 위해서 아래 3가지 요소를 고민함!!  
    a. 과제 : 어떤 과제를 설정할것인가!  
    b. 모델 : 어떤 모델을 쓸것인가!  
    c. 데이터 : 어떤 데이터를 쓸것인가!  

### SAM 연구의 과제(Task)의 정의

- 기존 Segmentation 모델의 한계
  - 대부분의 segmentation 모델은 **사전 정의된 클래스(class)**가 있어야 학습 가능
  - 특정 객체(ex: 고양이, 개, 자동차)만 분할 가능하며, **라벨링 데이터에 매우 의존**
  - 새로운 클래스에 대해선 **재학습(fine-tuning)**이 필요

- Open-Vocabulary, Prompt 기반 모델 필요성
  - 최근 CLIP 등 멀티모달 모델의 등장과 함께,  
  - "텍스트"나 "포인트" 등을 통해 **사용자 중심으로 객체를 지정하고 분할하는 모델**이 요구됨

- 그래서!! **"무엇이든 잘라내는 범용 분할기"**를 과제로 정의!

--- 

### ⚙️ SAM의 모델 구조

![architecture](https://github.com/user-attachments/assets/sample-sam-arch.png)

| 구성 요소       | 설명 |
|----------------|------|
| **Image Encoder** | 이미지 전체를 인코딩하여 고정된 **image embedding** 생성 (한 번만 수행) |
| **Prompt Encoder** | 점, 박스, 마스크 등 다양한 프롬프트를 인코딩 |
| **Mask Decoder** | 이미지와 프롬프트 인코딩을 결합하여 **마스크 예측** 수행 |

#### SAM은 세 가지 주요 구성 요소 및 그 기능!!  

1. **Image Encoder** (ViT-H 기반)
   - MAE (Masked Autoencoders) 방식의 ViT를 사용! - MAE가 뭔지 공부해보자!!
    | Masked Autoencoders Are Scalable Vision Learners (CVPR, 2022)  
   - 이미지를 고해상도로 인코딩하여 풍부한 시각 표현 생성
   - 한번 인코딩된 이미지는 여러 프롬프트에도 재사용 가능

2. **Prompt Encoder**
   - 사용자의 입력을 인코딩
   - 입력 종류 - 크게 2가지!  

    | 종류 | 예시 | 인코딩 방식 | 설명 |
    |------|------|-------------|------|
    | **희소 (Sparse)** | Point, Box, **Text** | 위치 + 학습된 임베딩 / 텍스트 인코더(CLIP) | - 위치 정보에 Positional Encoding + 학습된 임베딩 <br> - **텍스트는 CLIP 텍스트 인코더 사용** |
    | **밀집 (Dense)** | Mask | Convolution + Element-wise Sum | - 마스크를 Conv로 임베딩 후 <br> 이미지 임베딩과 원소 단위 합산 |

3. **Mask Decoder**
  ![mask_encoder]()

   - 인코더 출력을 기반으로 최종 마스크를 생성
   - 이미지와 프롬프트 정보를 결합하여 마스크를 생성하는 핵심 구성
   - 🔧 주요 구성 요소 및 처리 과정

    | 단계 | 설명 |
    |------|------|
    | 1. 입력 | - Image Embedding <br> - Prompt Embedding (Point, Box, Text 등) <br> - Output Token |
    | 2. 디코더 블록 (×2) | - **Transformer Decoder** 변형 버전 사용 <br> - **Prompt Self-Attention** <br> - **Cross-Attention** (Prompt ↔ Image 임베딩) 양방향 수행 |
    | 3. 업샘플링 | - 디코더 출력에서 **Image Embedding을 업샘플링** |
    | 4. 동적 마스크 예측 | - Output Token → **MLP → 동적 Linear Classifier** <br> - 각 픽셀 위치마다 **Foreground 확률 계산** |
    | 5. 최종 출력 | - 전경 확률(foreground probability) 맵 → **Binary Mask** 출력 |

    －모호성의 해결을 위하여! : **세 개의 후보 마스크**를 출력, 각 마스크 별 확신도(uncertainty score) 제공

---


### 🏗️ SAM의 데이터 (SA-1B) 및  데이터 엔진

![datasets](https://github.com/user-attachments/assets/sample-sam-image.png)

- **SA-1B**: SAM 학습을 위해 Meta가 만든 **초대규모 세그멘테이션 데이터셋**  
- 총 **11M개의 이미지**에서 자동으로 수집된 **1B+ 마스크**  
- 기존 세그멘테이션 데이터셋보다 **400배 더 많은 마스크 보유**  
- ✅ **완전 자동 수집**, ✅ **고품질 & 다양성 보장**  
- SAM의 **범용성 및 견고성 확보**에 핵심 역할  
- 📚 향후 **파운데이션 모델 연구를 위한 공공 자원**으로 활용 가능  
- SA-1B 데이터셋 생성 절차 요약 표

| 단계 | 명칭 | 주체 | 주요 작업 | 특징 |
|------|------|------|-----------|------|
| 1️⃣ | **보조 수동 주석 (Assisted-manual)** | 사람 + SAM | 사람이 마스크를 직접 만들고, SAM이 보조 | 인터랙티브 세그멘테이션 방식, 초기 품질 확보 |
| 2️⃣ | **반자동 주석 (Semi-automatic)** | SAM + 사람 | SAM이 일부 객체 마스크 생성, 사람은 나머지를 주석 | 다양성 향상, 시간 효율 증가 |
| 3️⃣ | **완전 자동 주석 (Fully automatic)** | SAM | SAM이 포인트 프롬프트 기반으로 전체 마스크 생성 | 이미지당 평균 100개 마스크, SA-1B 대부분 구성 |


#### 1단계: Assisted-Manual Stage  
- 브라우저 기반 인터랙티브 툴에서 **SAM이 실시간으로 주석 지원**  
- 전문 주석자가 **전경/배경 포인트 클릭**하여 마스크 생성  
- 브러시 & 지우개로 정밀 수정 가능  
- **"설명 가능한" 객체 중심**으로 자유롭게 라벨링 (semantic 제한 없음)  
- 마스크에 이름/설명은 저장하지 않음  
- **30초 이상 걸리면 다음 이미지로 넘어감**  
- 수집된 마스크로 6회 재학습!!  

##### 🔁 모델 향상 과정
| 항목 | 내용 |
|------|------|
| 초기 모델 | 공개 세그멘테이션 데이터로 학습된 SAM |
| 반복 학습 | 수집된 마스크만으로 총 **6회 재학습** |
| ViT 백본 | ViT-B → ViT-H로 점진적 확장 |
| 구조 개선 | 다양한 세부 구조 진화 포함 |

##### 📈 성능 개선 지표
| 지표 | 변화 |
|------|------|
| 평균 주석 시간 | 34초 → 14초 (COCO보다 6.5배 빠름) |
| 평균 마스크 수 | 이미지당 20개 → 44개 |
| 수집량 | 12만 이미지, **430만 마스크** 수집 완료 |


#### 2단계: Semi-Automatic Stage
- 이 단계는 **"자동 + 수동 협업 구조"**로, **더 어려운 객체**, **더 다양한 객체**를 커버하는 데 중요한 역할수행
- **마스크 다양성 향상**을 통해 SAM의 범용 분할 능력 강화
  1. 1단계 마스크를 기반으로 **"object" 클래스 하나로 바운딩 박스 탐지기** 학습
  2. **자동 탐지된 마스크(confident masks)**를 이미지에 미리 삽입
  3. 주석자는 **자동 마스크 외의 누락된 객체만 수동으로 추가 주석**

##### 📈 성능 및 수치

| 항목 | 내용 |
|------|------|
| 수집 마스크 수 | 590만 개 추가 수집 (총 1,020만 개 도달) |
| 이미지 수 | 18만 장 |
| SAM 재학습 횟수 | 5회 반복 학습 |
| 평균 주석 시간 | 34초 (자동 마스크 제외) |
| 이미지당 평균 마스크 수 | 44개 → 72개 (자동 + 수동 포함) |

#### 3단계: Fully Automatic Stage

  - 2단계까지 데이터로 학습된 모델로, 완전 자동으로!!, 데이터셋 생성!!
  - 이로써 **Segment Anything의 SA-1B 데이터셋**이 완성
  - SAM 모델도 중요하지만, 이처럼 범용 분할 모델 학습에 있어 전례 없는 리소스 제공했다는점도 큰 의미!!

##### 🔧 자동 생성 절차

1. **32×32 포인트 그리드**로 이미지 프롬프트
2. 각 포인트에 대해 **다중 마스크 예측**
   - 예: "팔" 포인트 → 팔 / 팔+몸통 / 전체 사람 마스크
3. **IoU 예측 모듈**로 **신뢰도 높은 마스크만 선택**
4. **안정성 검사**:
   - 확률맵을 0.5, 0.55 등으로 threshold해도 비슷하면 "안정된 마스크"
5. **NMS (Non-Max Suppression)**로 **중복 제거**
6. **작은 객체 보완**을 위해 **확대된 이미지 crop**도 병렬 처리


#### 최종 생성된 데이터(SA-1B)는!? 

##### 🖼️ 이미지 구성

| 항목 | 내용 |
|------|------|
| 이미지 수 | **11,000,000장** |
| 해상도 | 평균 **3300 × 4950** 픽셀 |
| 출처 | **사진작가와 직접 협업하는 공급업체**로부터 라이선스 획득 |
| 보호 조치 | 얼굴 및 차량 번호판 **블러 처리** 포함 |
| 배포 형식 | 최단 변 기준 **1500픽셀 다운샘플 버전** 제공 |
| 비교 | COCO: 480×640 → SA-1B는 훨씬 더 고해상도 |

##### 🧩 마스크 구성

| 항목 | 내용 |
|------|------|
| 총 마스크 수 | **1.1B (11억 개)** |
| 생성 방식 | **99.1% 자동 생성** (Fully Automatic Stage) |
| 포함 마스크 | 최종적으로는 **자동 생성된 마스크만 포함**됨 |
| 품질 평가 | 전문가 보정 대비 94%가 **IoU > 90%** 수준의 일치율 |

##### 🔍 품질 검증: 자동 vs 전문가의 검증!!

- 무작위 500개 이미지(총 5만 마스크)를 샘플링하여  
  전문가가 브러시 & 지우개로 마스크를 **정교하게 보정**
- 그 결과:
  - **94%의 마스크 쌍이 IoU > 90%**
  - **97%는 IoU > 75%**
- 참고: 기존 논문 기준 **사람 간 IoU 일치율은 85~91%** 수준
- ⇒ SAM의 자동 마스크는 **전문가 수준의 품질 확보**

##### PC의 데이터!!  
- 남성 여성, 유럽 아시아 아프라카 등 어떤점에서도 치우치지 않은 Fairness 데이터에요!!^^*  


---





---

### 🧠 다양한 Prompt 예시

| Prompt 유형 | 설명 |
|-------------|------|
| Positive Point | 사용자가 관심 있는 영역을 클릭 |
| Negative Point | 분할에서 제외하고 싶은 지점 지정 |
| Box | 대략적인 관심 객체의 범위 박스 입력 |
| Mask | 기존 마스크를 사용하여 refinement |

> 🎯 핵심: "Prompt"만 다르면, SAM은 같은 이미지에서도 **다른 분할 결과**를 만들어냄!

---

### 🧪 SAM의 강점

- **Zero-Shot Segmentation**  
  - 어떤 클래스든 사전 학습된 모델로 **즉시 분할 가능**
- **High-Resolution 대응**  
  - 이미지 전체를 고해상도로 인코딩 (ViT-H 기반)
- **대규모 학습 데이터**  
  - Meta는 Segment Anything을 위해 **1억 개의 마스크를 가진 SA-1B 데이터셋**을 자체 구축
- **빠른 응답 속도**  
  - 한번 인코딩한 이미지 위에서 Prompt만 바꿔가며 빠르게 결과 생성 가능

---

### 🧪 SAM의 실험 결과

- SA-1B 외에도 COCO, LVIS 등 다양한 벤치마크에서도 강력한 성능
- 다양한 Prompt에 대해 **일관되며 세밀한 분할 결과**
- 특히 사람의 실루엣, 손, 복잡한 배경을 포함한 물체도 잘 분할 가능

---

### ✅ SAM vs 기존 Segmentation 모델

| 항목 | 기존 Segmentation | SAM |
|------|--------------------|-----|
| 클래스 기반 | 필요함 (ex: 고양이, 강아지) | 불필요 (Prompt 기반) |
| 라벨 필요 | 학습 및 추론 모두에 필요 | 학습만 대규모 라벨 사용 |
| 사용자 조작 | 불가능 또는 제한적 | 가능 (클릭, 박스 등) |
| 유연성 | 제한적 | 매우 유연 |
| Zero-shot | 불가능 | 가능!! |

---

### ✨ 마무리하며

SAM은 단순히 segmentation 모델이 아니라, **인간-모델 인터랙션을 자연스럽게 만든 범용 플랫폼**으로 볼 수 있습니다.  
텍스트, 클릭, 마스크 등 다양한 입력에 대응하며, 향후 multimodal foundation model로서 큰 잠재력을 가지고 있습니다!

요즘 LLM과의 결합, 3D segmentation, 영상 분할 등 다양한 응용이 진행되고 있다고 하니!!  
다음에는 SAM + LLM 기반의 multi-modal reasoning도 공부해 봐야겠네요~! 😊  

---

