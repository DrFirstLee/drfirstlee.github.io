---
layout: post
title: "Understanding Grounding DINO!! - Grounding DINO ë…¼ë¬¸ ê³µë¶€!"
author: [DrFirst]
date: 2025-05-15 15:00:00 +0900
categories: [AI, Research]
tags: [grounding, grounding dino, grounded sam, DINO, computer vision, AI ,ECCV, ECCV 2024, DETR]
sitemap :
  changefreq : monthly
  priority : 0.8
---

## ğŸ“ Understanding Grounding DINO!!
_Studying ã€Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detectionã€ (ECCV, 2024)_

![manhwa](https://github.com/user-attachments/assets/75d77acb-31e2-455e-a1c2-30864afccf27)

ğŸ“– **Paper Title**: Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection  
âœï¸ **Authors**: Xinyu Chen, Xueyan Zou, Ze Liu, et al.  
ğŸŒŸ **One-line Summary**: A text-prompt-based object detector!

- Today, Iâ€™m studying the Grounding DINO paper, which I previously [experimented with in practice](https://drfirstlee.github.io/posts/groundingDINO_Detection_usage/)!

---

### ğŸ§  Core Ideas Summary

#### 1ï¸âƒ£ DINO-based Structure with Enhanced Modality Fusion

![detector_structure](https://github.com/user-attachments/assets/8b718698-a4bf-4347-99de-0c428ba597f2)

- Grounding DINO is based on the **Transformer-based object detector DINO**.
- Unlike Faster R-CNN, DINOâ€™s structure naturally allows **layer-level fusion between text and image**.
- Grounding DINO performs **cross-modality fusion** in **Neck (Phase A), Query Initialization (Phase B), and Head (Phase C)** stages to boost text-conditioned detection performance.

#### 2ï¸âƒ£ Generalization through Grounded Pretraining
- CLIP excels at global image-text alignment but struggles with region-level grounding.
- To overcome CLIP-style zero-shot limitations, Grounding DINO introduces **contrastive pretraining on region-text pairs**.
- It enhances GLIPâ€™s phrase grounding approach with **sub-sentence level text processing** to reduce category interference.
- This allows Grounding DINO to become a true **â€œtext â†’ detectionâ€ open-set detector**, achieving new zero-shot benchmarks on COCO and ODinW.

---

### ğŸ” Background of the Grounding DINO Research

Grounding DINO was proposed to go **beyond the limitations of fixed class object detection**.  
Hereâ€™s the previous evolution of related models:

---

#### ğŸ§© From DETR to DINO â€” Still Bound by Fixed Classes

- **[DETR](https://drfirstlee.github.io/posts/DETR/) (2020, Facebook AI)**  
  The first Transformer-based end-to-end object detector  
  â†’ But it only detects **predefined classes**, like those in COCO.

- **[DINO](https://drfirstlee.github.io/posts/DINO_Detection/) (ICLR 2023)**  
  Improves DETRâ€™s training stability and accuracy  
  â†’ Great detection, but still limited to **fixed class tokens**

â¡ï¸ DINO detects well, but only if you already know what to detect.

---

#### ğŸ§© Open-Set Object Detection â€” Breaking Free from Fixed Classes

##### ğŸ” GLIP, OV-DETR, etc.

Traditional detectors are **closed-set**, trained only to recognize predefined classes via bounding box annotations.  

To break that limitation, Microsoft proposed **GLIP (Grounded Language-Image Pretraining)**:

- **Open-set Object Detection**  
- Detecting **arbitrary categories**  
- Using **natural language generalization** to understand new objects

Similarly, **OV-DETR** uses Transformer-based structure with **language-aware object queries injected into the decoder** for open-vocabulary detection.

##### âš ï¸ Limitations of Prior Work

These models mostly fused image and text features at **limited stages**, leading to **sub-optimal generalization**.

##### ğŸ“Š Multimodal Fusion Comparison

| Model      | Fusion Location               | Description                                       | Limitation                      |
|------------|-------------------------------|---------------------------------------------------|----------------------------------|
| **GLIP**   | Phase A (Feature Enhancement) | Fuses text-image in the neck module               | Lacks fusion in later modules   |
| **OV-DETR**| Phase B (Decoder Input)       | Injects language-aware queries into the decoder   | Limited fusion with early vision |

â¡ï¸ These limited fusions can lead to **weaker alignment** and **lower performance** in open-vocabulary detection.

---

#### ğŸ—£ï¸ SAMâ€™s Possibility and Limitation: Segmentation by Prompt

- **SAM (Segment Anything Model, 2023)**  
  A universal segmentation model based on point, box, and mask prompts  
  â†’ Can â€œsegment anythingâ€ as the name implies

- But SAM **couldnâ€™t take natural language prompts directly**  
  (Text prompts were only conceptually proposed â€” no actual interpretation)

---

### ğŸ’¡ Enter Grounding DINO!

Grounding DINO bridges both worlds:

- **Detection power of DINO** + **Text interpretation ability of CLIP**
- â†’ Resulting in a **text-prompt-based open-vocabulary object detector**

It is then combined with SAM into **Grounded-SAM**, completing a full pipeline of:  
**â€œText â†’ Detection â†’ Segmentationâ€**

---

### ğŸ§ª Grounding DINO Architecture

![full_structure](https://github.com/user-attachments/assets/07a52f52-89bd-4a9c-bc39-66aefe0a7046)

#### ğŸ“ Architecture Overview

Grounding DINO uses a **dual-encoder + single-decoder** design:

1. **Image Backbone**: Extracts visual features
2. **Text Backbone**: Extracts language features
3. **Feature Enhancer**: Fuses image-text features (Sec. 3.1)
4. **Language-Guided Query Selection**: Initializes decoder queries (Sec. 3.2)
5. **Cross-Modality Decoder**: Refines detections (Sec. 3.3)

---

##### 3.1 ğŸ”§ Feature Extraction and Enhancer

- Image features via Swin Transformer (multi-scale)
- Text features via BERT
- Fusion includes:
  - **Deformable Self-Attention** for image
  - **Vanilla Self-Attention** for text
  - **Image-to-Text** and **Text-to-Image Cross-Attention**
- Multiple stacked fusion layers

---

##### 3.2 ğŸ¯ Language-Guided Query Selection

Grounding DINO dynamically selects decoder queries **based on the input text**.  
Unlike fixed queries in DETR, it scores the similarity between text and image patches.

ğŸ” Process Overview:

1. ğŸ“¸ Extract image patch features  
2. ğŸ“ Extract text features from the sentence  
3. ğŸ” Measure how well each image patch matches text tokens  
4. â­ Select top 900 image patches as detection queries  
5. â†’ Used to predict bounding boxes and labels

Query =  
- **Positional Part**: Anchor box information  
- **Content Part**: Learnable feature vector

---

##### 3.3 ğŸ”„ Cross-Modality Decoder

Each decoder layer includes:

1. **Self-Attention**  
2. **Image Cross-Attention**  
3. **Text Cross-Attention** (added)  
4. **Feed-Forward Network**

â¤ Added **text cross-attention** allows better text-image fusion during decoding.

---

##### 3.4 âœ‚ï¸ Sub-Sentence Level Text Feature

![subsentence](https://github.com/user-attachments/assets/b701cf93-287b-48e5-8e4c-f0d995172a4b)

Existing approaches:
- **Sentence-level**: Encodes whole sentence â†’ loses fine details
- **Word-level**: Encodes all class names together â†’ unintended word interference

Grounding DINO proposes:  
â¡ï¸ **Sub-sentence level encoding** with attention masks  
â†’ Removes interference between unrelated words  
â†’ Preserves fine-grained per-word features

---

#### ğŸ¯ Loss Function Design

---

##### ğŸ”§ 3.5 Loss Function

Grounding DINO combines multiple loss components:

##### ğŸ“¦ 1. Bounding Box Regression
- **L1 Loss**  
- **GIoU Loss** for location accuracy

---

##### ğŸ·ï¸ 2. Classification (Text-based)
- **Contrastive Loss**: Matches text tokens with predicted boxes
- Uses:
  - **Dot product** between queries and text features
  - **Focal Loss** on logits for robust learning

---

##### ğŸ”„ 3. Matching and Final Loss

- Bipartite matching aligns predictions with ground truth  
- Final loss = Box loss + Classification loss

---

##### ğŸ§± 4. Auxiliary Loss

- Added at:
  - **Each decoder layer**
  - **Encoder outputs**
- Helps stabilize early-stage training and convergence

---

### ğŸ“Š Ablation Study Summary

Grounding DINO evaluates the importance of each design by removing or altering modules.  
Evaluated on COCO and LVIS (minival) for Zero-Shot and Fine-Tune settings.

---

#### ğŸ“‹ Results (Table 7)

| ID | Model Variant                         | COCO (Zero-Shot) | COCO (Fine-Tune) | LVIS (Zero-Shot) |
|----|---------------------------------------|------------------|------------------|------------------|
| 0  | âœ… Full Model                          | **46.7**         | **56.9**         | **16.1**         |
| 1  | âŒ No Encoder Fusion                   | 45.8             | 56.1             | 13.1             |
| 2  | âŒ Static Query Selection              | 46.3             | 56.6             | 13.6             |
| 3  | âŒ No Text Cross-Attention             | 46.1             | 56.3             | 14.3             |
| 4  | âŒ Word-Level Prompt (vs. Sub-sentence)| 46.4             | 56.6             | 15.6             |

---

#### ğŸ” Interpretation

1. **Encoder Fusion** (ID #1) is most critical  
   - Drop of **-0.9 AP (COCO)** and **-3.0 AP (LVIS)**  
2. **Static Query Selection** (ID #2) hurts zero-shot performance  
3. **Text Cross-Attention** (ID #3) improves grounding
4. **Sub-sentence Prompt** is more effective than word-level

---

#### âœ… Conclusion

- **Encoder Fusion** is the biggest performance booster  
- **Query Selection & Text Attention** matter especially in open-vocabulary settings  
- **Sub-sentence prompts** improve fine-grained alignment

---

### ğŸ’¡ Takeaways

Grounding DINO is not just a better detector â€”  
itâ€™s a model that **connects language and vision meaningfully**.

I was especially impressed by how it finds objects not from fixed labels,  
but from **free-form text prompts**!

---

### ğŸ“š References

1. Paper: https://arxiv.org/abs/2303.05499  
2. Code: https://github.com/IDEA-Research/GroundingDINO  
3. Thanks to ChatGPT for summarization ğŸ™

---

## (í•œêµ­ì–´) ğŸ“ Grounding DINO ì•Œì•„ë³´ê¸°!!
_ã€Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detectionã€(ECCV, 2024) ê³µë¶€_

![manhwa](https://github.com/user-attachments/assets/75d77acb-31e2-455e-a1c2-30864afccf27)

ğŸ“– **ë…¼ë¬¸ ì œëª©**: Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection  
âœï¸ **ì €ì**: Xinyu Chen, Xueyan Zou, Ze Liu, et al.  
ğŸŒŸ **í•œì¤„ ìš”ì•½**: ì œì‹œëœ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ê°ì²´ íƒì§€ê¸°!

- ì˜¤ëŠ˜ì€ [ì‹¤ìŠµì„ ë¨¼ì € ì§„í–‰í•´ë³´ì•˜ë˜](https://drfirstlee.github.io/posts/groundingDINO_Detection_usage/) Grounding DINO ëª¨ë¸ì˜ ë…¼ë¬¸ì„ ê³µë¶€í•´ë³´ê³ ìí•©ë‹ˆë‹¤!!  

---

### ğŸ§  í•µì‹¬ ì•„ì´ë””ì–´ ìš”ì•½

#### 1ï¸âƒ£ DINO ê¸°ë°˜ êµ¬ì¡°ì™€ ëª¨ë‹¬ ìœµí•© ê°•í™”

![detector_structure](https://github.com/user-attachments/assets/8b718698-a4bf-4347-99de-0c428ba597f2)


- Grounding DINOëŠ” **Transformer ê¸°ë°˜ ê°ì²´ íƒì§€ê¸°ì¸ DINO**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ë¨.
- ê¸°ì¡´ Faster R-CNN êµ¬ì¡°ì™€ ë‹¬ë¦¬, DINOëŠ” **í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê°„ layer-level ìœµí•©**ì´ ìì—°ìŠ¤ëŸ½ê²Œ ê°€ëŠ¥í•œ êµ¬ì¡°ë¥¼ ê°€ì§.
- **Neck(phase A), Query Initialization(phase B), Head(phase C)** ë‹¨ê³„ ëª¨ë‘ì—ì„œ **cross-modality fusion**ì´ ì´ë£¨ì–´ì§€ë„ë¡ ì„¤ê³„í•˜ì—¬, í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°ì²´ íƒì§€ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚´.

#### 2ï¸âƒ£ Grounded Pretrainingì„ í†µí•œ Open-Set ì¼ë°˜í™”
- CLIPì€ ì´ë¯¸ì§€ ì „ì²´ ìˆ˜ì¤€ì—ì„œëŠ” ë›°ì–´ë‚˜ì§€ë§Œ, ì˜ì—­(region) ìˆ˜ì¤€ í…ìŠ¤íŠ¸ ëŒ€ì‘ì—ëŠ” í•œê³„ê°€ ì¡´ì¬.  
- ì´ëŸ° CLIP ê¸°ë°˜ zero-shot ë°©ì‹ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, **region-text ìŒì— ëŒ€í•œ contrastive pretraining**ì„ ë„ì….
- GLIPì˜ phrase grounding ë°©ì‹ì„ ê°œì„ í•˜ì—¬, **sub-sentence ë‹¨ìœ„ í…ìŠ¤íŠ¸ ì²˜ë¦¬**ë¥¼ í†µí•´ í´ë˜ìŠ¤ ê°„ ê°„ì„­ì„ ì¤„ì„.
- ì´ë¡œì¨ Grounding DINOëŠ” **â€œí…ìŠ¤íŠ¸ â†’ íƒì§€â€ê°€ ê°€ëŠ¥í•œ open-set object detector**ë¡œì„œ, COCO ë° ODinW ë“±ì—ì„œ **zero-shot ì„±ëŠ¥ì˜ ìƒˆë¡œìš´ ê¸°ì¤€**ì„ ì œì‹œí•¨.


---

### ğŸ” Grounding DINO ì—°êµ¬ì˜ ë°°ê²½

Grounding DINOëŠ” ê¸°ì¡´ì˜ ê°ì²´ íƒì§€(Object Detection) ëª¨ë¸ë“¤ì´ ê°€ì§„ **ê³ ì •ëœ í´ë˜ìŠ¤ ì œí•œ**ì„ ë›°ì–´ë„˜ê¸° ìœ„í•´ ì œì•ˆëœ ëª¨ë¸ì…ë‹ˆë‹¤.  
ì´ì „ê¹Œì§€ì˜ íë¦„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

---

#### ğŸ§© DETR ì´í›„ DINO, í•˜ì§€ë§Œ ì—¬ì „íˆ í´ë˜ìŠ¤ëŠ” ê³ ì •

- **[DETR](https://drfirstlee.github.io/posts/DETR/) (2020, Facebook AI)**  
  Transformer ê¸°ë°˜ìœ¼ë¡œ ê°ì²´ íƒì§€ë¥¼ ìˆ˜í–‰í•œ ìµœì´ˆì˜ end-to-end ëª¨ë¸  
  â†’ í•˜ì§€ë§Œ í´ë˜ìŠ¤ëŠ” COCOì²˜ëŸ¼ **ì‚¬ì „ ì •ì˜ëœ í´ë˜ìŠ¤ì…‹ì— í•œì •**ë¨

- **[DINO](https://drfirstlee.github.io/posts/DINO_Detection/) (ICLR 2023)**  
  DETR êµ¬ì¡°ë¥¼ ê°œì„ í•´ í•™ìŠµ ì•ˆì •ì„±ê³¼ ì •í™•ë„ë¥¼ ë†’ì¸ ëª¨ë¸  
  â†’ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ì§€ë§Œ **ì—¬ì „íˆ ê³ ì •ëœ í´ë˜ìŠ¤(class token)**ë§Œ íƒì§€ ê°€ëŠ¥

ì¦‰, DINOëŠ” **íƒì§€ëŠ” ì˜í•˜ì§€ë§Œ 'ë¬´ì—‡ì„ íƒì§€í• ì§€'ëŠ” ì´ë¯¸ ì •í•´ì ¸ ìˆì–´ì•¼** í–ˆìŠµë‹ˆë‹¤.

---

#### ğŸ§© Open-Set Object Detection, ì¦‰ ê³ ì •ëœ ê°ì²´ í•œê³„ë¥¼ ë„˜ì–´ì„œëŠ” ì—°êµ¬ë“¤  

##### ğŸ” GLIP, OV-DETR* ì—°êµ¬

ê¸°ì¡´ ê°ì²´ íƒì§€ëŠ” ì‚¬ì „ì— ì •ì˜ëœ í´ë˜ìŠ¤(bounding box ì–´ë…¸í…Œì´ì…˜)ì—ë§Œ ë°˜ì‘í•˜ëŠ”  
**ê³ ì • í´ë˜ìŠ¤ ê¸°ë°˜(closed-set)** íƒì§€ ë°©ì‹ì— í•œì •ë˜ì–´ ìˆì—ˆìŠµë‹ˆë‹¤.  

ì´ì— ëŒ€í•´ **GLIP**(Grounded Language-Image Pre-training, Microsoft)ì€ ë‹¤ìŒê³¼ ê°™ì€ ë°©í–¥ì„ ì œì‹œí–ˆìŒ:

- **ì˜¤í”ˆì…‹ ê°ì²´ íƒì§€ (Open-Set Object Detection)**  
- **ì„ì˜ì˜ í´ë˜ìŠ¤ (arbitrary class)**ì— ëŒ€í•œ íƒì§€ ìˆ˜í–‰  
- **ìì—°ì–´ ê¸°ë°˜ ì¼ë°˜í™” (language generalization)**ë¥¼ í†µí•´ ìƒˆë¡œìš´ ê°ì²´ë¥¼ ì´í•´í•˜ê³  íƒì§€

ì¦‰, ì •í•´ì§„ ë¼ë²¨ ì—†ì´ë„ **í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ê°ì²´ë¥¼ íƒì§€í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥**ì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

í•œí¸, **OV-DETR**ì€ Transformer êµ¬ì¡° ê¸°ë°˜ì˜ ê°ì²´ íƒì§€ê¸°ë¡œ,  
ì–¸ì–´ ì •ë³´ê°€ í¬í•¨ëœ ì¿¼ë¦¬(query)ë¥¼ ë””ì½”ë”ì— ì§ì ‘ ì£¼ì…í•˜ì—¬ open-vocabulary íƒì§€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.


##### âš ï¸ ê¸°ì¡´ ì—°êµ¬ë“¤ì˜ í•œê³„ì 

ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì€ ëª¨ë‘ **ì´ë¯¸ì§€ì™€ ì–¸ì–´ë¼ëŠ” ë©€í‹°ëª¨ë‹¬ ì •ë³´**ë¥¼  
**ì¼ë¶€ ëª¨ë“ˆì—ë§Œ êµ­í•œí•˜ì—¬ ìœµí•©(fusion)**í•¨ì— ë”°ë¼,  
**ì–¸ì–´ ê¸°ë°˜ ì¼ë°˜í™” ì„±ëŠ¥ì´ ìµœì ë³´ë‹¤ ë‚®ê²Œ(sub-optimal) ì‘ë™í•  ê°€ëŠ¥ì„±**ì´ ì¡´ì¬í•©ë‹ˆë‹¤.


##### ğŸ“Š ì˜ˆì‹œ: ë©€í‹°ëª¨ë‹¬ ê²°í•© ìœ„ì¹˜ ë¹„êµ

| ëª¨ë¸        | ë©€í‹°ëª¨ë‹¬ ê²°í•© ìœ„ì¹˜              | ì„¤ëª…                                          | í•œê³„ì  |
|-------------|----------------------------------|-----------------------------------------------|--------|
| **GLIP**    | Phase A (Feature Enhancement)    | ë°±ë³¸ ì´í›„ neck ë‹¨ê³„ì—ì„œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ íŠ¹ì§• ìœµí•© | ì´í›„ ë””ì½”ë”ì™€ì˜ ì—°ê²°ì„± ë¶€ì¡± |
| **OV-DETR** | Phase B (Decoder Input)          | ë””ì½”ë”ì— ì–¸ì–´ ì¿¼ë¦¬(query)ë¥¼ ì§ì ‘ ì‚½ì…           | ì´ˆê¸° ì‹œê° ì •ë³´ì™€ì˜ ê¹Šì€ ìœµí•© ë¶€ì¡± |


â¡ï¸ ì´ëŸ¬í•œ êµ¬ì¡°ì  ì œì•½ì€,  
**í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê°„ì˜ ê¹Šì´ ìˆëŠ” ì •ë ¬(alignment)ì´ ìš”êµ¬ë˜ëŠ” open-vocabulary íƒì§€**ì—ì„œ  
**ì„±ëŠ¥ ì €í•˜** ë˜ëŠ” **ì¼ë°˜í™” í•œê³„**ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.


---

#### ğŸ—£ï¸ SAMì´ ì œì‹œí•œí•œ ê°€ëŠ¥ì„±ê³¼ í•œê³„: í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë¶„í•  ì•„ë””ì´ë””ì–´  

- **SAM (Segment Anything Model, 2023)**  
  í¬ì¸íŠ¸, ë°•ìŠ¤, ë§ˆìŠ¤í¬ ê¸°ë°˜ì˜ **ë²”ìš© ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸**  
  â†’ *Segment Anything*ì´ë¼ëŠ” ì´ë¦„ì— ê±¸ë§ê²Œ ì–´ë–¤ ê°ì²´ë“  ì˜ë¼ë‚¼ ìˆ˜ ìˆìŒ

- ê·¸ëŸ¬ë‚˜ SAMì€ **í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì…ë ¥í•´ segmentationì„ ìˆ˜í–‰í•  ìˆ˜ëŠ” ì—†ì—ˆìŒ**  
  (í…ìŠ¤íŠ¸ëŠ” ê°œë…ì ìœ¼ë¡œ ì œì‹œë˜ì—ˆì§€ë§Œ, ì‹¤ì œ í…ìŠ¤íŠ¸ ì¸ì‹ì„ í•˜ì§€ ì•ŠìŒ)

---

### ğŸ’¡ ê·¸ë˜ì„œ ë“±ì¥í•œ Grounding DINO!

Grounding DINOëŠ” ì´ëŸ¬í•œ ë‘ íë¦„ì„ **ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°**í•©ë‹ˆë‹¤:

- **DINOì˜ ê°ì²´ íƒì§€ ëŠ¥ë ¥** + **í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ í•´ì„ ëŠ¥ë ¥(CLIP ê¸°ë°˜)**  
- â†’ ê²°êµ­ **"ë§ë¡œ íƒì§€í•˜ëŠ”(open-vocabulary) ê°ì²´ íƒì§€ê¸°"**ê°€ ëœ ê²ƒ!!  

ì´í›„ SAMê³¼ ê²°í•©í•˜ì—¬ **Grounded SAM**ìœ¼ë¡œ í™•ì¥ë˜ë©°,  
"í…ìŠ¤íŠ¸ â†’ íƒì§€ â†’ ë¶„í• "ì´ë¼ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì´ ì™„ì„±ë©ë‹ˆë‹¤.  

---

### ğŸ§ª Grounding DINOì˜ êµ¬ì„±

![full_structure](https://github.com/user-attachments/assets/07a52f52-89bd-4a9c-bc39-66aefe0a7046)

#### ğŸ“ ì•„í‚¤í…ì²˜ ê°œìš”

Grounding DINOëŠ” **dual-encoder + single-decoder êµ¬ì¡°**ë¥¼ ì±„íƒí•©ë‹ˆë‹¤.  

êµ¬ì„± ìš”ì†ŒëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. **Image Backbone**: ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
2. **Text Backbone**: í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ
3. **Feature Enhancer**: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ íŠ¹ì§• ìœµí•© (Sec. 3.1)
4. **Language-Guided Query Selection**: ì¿¼ë¦¬ ì´ˆê¸°í™” (Sec. 3.2)
5. **Cross-Modality Decoder**: ë°•ìŠ¤ refinement ìˆ˜í–‰ (Sec. 3.3)

---

##### 3.1 ğŸ”§ Feature Extraction and Enhancer

- **ì´ë¯¸ì§€ Feature**: Swin Transformerì™€ ê°™ì€ ë°±ë³¸ì„ í†µí•´ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ì¶”ì¶œ
- **í…ìŠ¤íŠ¸ Feature**: BERT ê¸°ë°˜ì˜ ë°±ë³¸ìœ¼ë¡œ ì¶”ì¶œ
- **ìœµí•© ë°©ì‹**:
  - ì´ë¯¸ì§€: **Deformable self-attention**
  - í…ìŠ¤íŠ¸: **Vanilla self-attention**
  - í¬ë¡œìŠ¤ëª¨ë‹¬ ìœµí•©: 
    - **Image-to-Text Cross-Attention**
    - **Text-to-Image Cross-Attention**
  - ë‹¤ìˆ˜ì˜ Feature Enhancer Layerë¡œ êµ¬ì„±

ğŸ‘‰ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹°ì˜ íŠ¹ì§• ì •ë ¬(alignment)ì„ ìœ„í•œ í•µì‹¬ ëª¨ë“ˆ

---

##### 3.2 ğŸ¯ Language-Guided Query Selection

Grounding DINOëŠ” **ì…ë ¥ í…ìŠ¤íŠ¸ì— ë”°ë¼ íƒì§€ ì¿¼ë¦¬ë¥¼ ë™ì ìœ¼ë¡œ ì„ íƒ**í•˜ëŠ” êµ¬ì¡°ë¥¼ ê°–ê³  ìˆìŠµë‹ˆë‹¤.  
ê¸°ì¡´ì˜ DETR ê³„ì—´ ëª¨ë¸ë“¤ì´ ê³ ì •ëœ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬,  
**ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ì¿¼ë¦¬ë“¤ì„ ì„ íƒ**í•©ë‹ˆë‹¤.

- ğŸ” ì‘ë™ ë°©ì‹  

1. ğŸ“¸ **ì´ë¯¸ì§€ë¥¼ ì¡°ê°ì¡°ê° ë‚˜ëˆ ì„œ(=íŒ¨ì¹˜ë¡œ)** íŠ¹ì§•ì„ ë½‘ê³ ,   
2. ğŸ“ **ì…ë ¥ ë¬¸ì¥(ì˜ˆ: "a red umbrella")ë„** ë‹¨ì–´ë³„ë¡œ íŠ¹ì§•ì„ ì¶”ì¶œ!  
3. ğŸ” **ì´ë¯¸ì§€ì˜ ê° ì¡°ê°ì´ í…ìŠ¤íŠ¸ì˜ ì–´ë–¤ ë‹¨ì–´ì™€ ì˜ ë§ëŠ”ì§€** ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³ ,  
4. â­ ì ìˆ˜ê°€ ë†’ì€ ì´ë¯¸ì§€ ì¡°ê° 900ê°œë¥¼ **"íƒì§€ ì¿¼ë¦¬"ë¡œ ì„ íƒ**  
5. ì´ ì¿¼ë¦¬ë“¤ì€ ë””ì½”ë”ì— ë“¤ì–´ê°€ì„œ **bounding boxì™€ ë ˆì´ë¸”ì„ ì˜ˆì¸¡**  

- "ì¿¼ë¦¬"ì˜ êµ¬ì„±ì€? :  ì¿¼ë¦¬ëŠ” ë‘ ê°€ì§€ ì •ë³´ë¡œ êµ¬ì„±ë¨  

- **ìœ„ì¹˜ ì •ë³´ (Positional Part)**: ì¿¼ë¦¬ê°€ ì´ë¯¸ì§€ ì–´ë””ë¥¼ ê°€ë¦¬í‚¤ëŠ”ì§€(encoder ì¶œë ¥ìœ¼ë¡œë¶€í„° anchor box ì´ˆê¸°í™”)  
- **ë‚´ìš© ì •ë³´ (Content Part)**: ì–´ë–¤ ê°ì²´ë¥¼ ì°¾ìœ¼ë ¤ê³  í•˜ëŠ”ì§€


---

##### 3.3 ğŸ”„ Cross-Modality Decoder

- ê° ë””ì½”ë” ë ˆì´ì–´ëŠ” ë‹¤ìŒ ë¸”ë¡ìœ¼ë¡œ êµ¬ì„±ë¨:
1. **Self-Attention**
2. **Image Cross-Attention**
3. **Text Cross-Attention**
4. **Feed-Forward Network (FFN)**

- DINOì˜ ë””ì½”ë” êµ¬ì¡°ì— ë¹„í•´ **Text Cross-Attention ë¸”ë¡ì´ ì¶”ê°€**ë¨  
â†’ í…ìŠ¤íŠ¸ ì •ë³´ê°€ ì¿¼ë¦¬ ì—…ë°ì´íŠ¸ì— ë” ê°•í•˜ê²Œ ë°˜ì˜ë¨

---

##### 3.4 âœ‚ï¸ Sub-Sentence Level Text Feature

![subsentence](https://github.com/user-attachments/assets/b701cf93-287b-48e5-8e4c-f0d995172a4b)
  

- ê¸°ì¡´ í…ìŠ¤íŠ¸ ì¸ì½”ë”© ë°©ì‹:
- **Sentence-level**: ë¬¸ì¥ì„ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ì²˜ë¦¬ â†’ ì •ë°€ë„ ì†ì‹¤
- **Word-level**: ì—¬ëŸ¬ ë‹¨ì–´ë¥¼ í•œ ë²ˆì— ì¸ì½”ë”© â†’ ë‹¨ì–´ ê°„ ë¶ˆí•„ìš”í•œ ìƒí˜¸ì‘ìš© ë°œìƒ

- ë¬¸ì œ: í…ìŠ¤íŠ¸ê°€ ì—¬ëŸ¬ í´ë˜ìŠ¤ëª…ì„ í¬í•¨í•  ê²½ìš°, **ë¬´ê´€í•œ ë‹¨ì–´ ê°„ ìƒí˜¸ì‘ìš©(attention)ì´ ìƒê¹€**

- í•´ê²°:  
**Sub-sentence level representation** ë„ì…  
â†’ ì„œë¡œ ë‹¤ë¥¸ í´ë˜ìŠ¤ëª… ì‚¬ì´ì˜ attentionì„ **mask**í•˜ì—¬ ë¶ˆí•„ìš”í•œ ìƒí˜¸ì‘ìš© ì œê±°  
â†’ ë‹¨ì–´ ë‹¨ìœ„ ì •ë°€ í‘œí˜„ ìœ ì§€ + ìƒí˜¸ ê°„ì„­ ë°©ì§€


---
#### ğŸ¯ Lossì˜ êµ¬ì„±

---

##### ğŸ”§ 3.5 Loss Function

Grounding DINOëŠ” ê¸°ì¡´ì˜ DETR ê³„ì—´ ëª¨ë¸ë“¤ê³¼ ìœ ì‚¬í•˜ê²Œ,  
ë‹¤ìŒ ì„¸ ê°€ì§€ ì£¼ìš” ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì¡°í•©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤:


##### ğŸ“¦ 1. Bounding Box Regression
- **L1 Loss**  
- **GIoU Loss** (Generalized Intersection over Union)  
- â†’ ë°•ìŠ¤ ìœ„ì¹˜ ì˜ˆì¸¡ ì •ë°€ë„ í–¥ìƒì— ì‚¬ìš©  
- ì°¸ê³ : DETR, Deformable DETR ë“±ì—ì„œ ì‚¬ìš©ëœ ë°©ì‹ê³¼ ë™ì¼

---

##### ğŸ·ï¸ 2. Classification (í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ë¥˜)
- **Contrastive Loss** (GLIP ë°©ì‹ ì±„íƒ)  
  - ì˜ˆì¸¡ëœ ê°ì²´ì™€ í…ìŠ¤íŠ¸ í† í° ê°„ì˜ ëŒ€ì‘ ê´€ê³„ í•™ìŠµ
- **ë°©ì‹**:
  - ê° ì¿¼ë¦¬ì™€ í…ìŠ¤íŠ¸ íŠ¹ì§• ê°„ì˜ **dot product** â†’ logits ê³„ì‚°
  - ê° í…ìŠ¤íŠ¸ í† í°ë³„ë¡œ **Focal Loss** ì ìš©í•˜ì—¬ ë¶„ë¥˜ í•™ìŠµ

---

##### ğŸ”„ 3. ë§¤ì¹­ ë° ì´í•© ê³„ì‚°

- ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µ ê°„ **ì´ì¤‘ ì´ë¶„ ë§¤ì¹­ (bipartite matching)** ìˆ˜í–‰  
  â†’ ë°•ìŠ¤ regression cost + classification cost ê¸°ë°˜
- ë§¤ì¹­ í›„ ìµœì¢… ì†ì‹¤ì€ ë‹¤ìŒì„ í•©ì‚°í•˜ì—¬ ê³„ì‚°:
  - **Bounding Box Loss (L1 + GIoU)**  
  - **Classification Loss (Focal + Contrastive)**

---

##### ğŸ§± 4. Auxiliary Loss

- **DETR ê³„ì—´ êµ¬ì¡°ë¥¼ ë”°ë¥´ê¸° ë•Œë¬¸ì—**, ë‹¤ìŒ ë‘ ìœ„ì¹˜ì— ë³´ì¡° ì†ì‹¤(auxiliary loss)ì„ ì¶”ê°€í•©ë‹ˆë‹¤:
  - **ê° ë””ì½”ë” ë ˆì´ì–´ ì¶œë ¥**
  - **ì¸ì½”ë” ì¶œë ¥ (encoder outputs)**

â¡ï¸ ì´ ë³´ì¡° ì†ì‹¤ì€ í•™ìŠµ ì´ˆê¸° ì•ˆì •ì„±ê³¼ ìˆ˜ë ´ ê°€ì†ì— ê¸°ì—¬í•©ë‹ˆë‹¤.

---

### ğŸ“Š Grounding DINO Ablation ì‹¤í—˜ ì •ë¦¬  

Grounding DINOì˜ ì£¼ìš” ì„¤ê³„ ìš”ì†Œë“¤ì´ ì‹¤ì œ ì„±ëŠ¥ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ë¥¼ ë¶„ì„í•˜ê¸° ìœ„í•´,  
ì—¬ëŸ¬ êµ¬ì„± ìš”ì†Œë¥¼ ì œê±°í•˜ê±°ë‚˜ ë³€ê²½í•œ **Ablation ì‹¤í—˜**ì„ ìˆ˜í–‰í•˜ì˜€ìŒ   
ì‹¤í—˜ ê²°ê³¼ëŠ” **COCO (minival)**ì™€ **LVIS (minival)** ë°ì´í„°ì…‹ì—ì„œì˜  
**Zero-Shot** ë° **Fine-Tune** ì¡°ê±´ì„ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€

---

#### ğŸ“‹ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ (Table 7)

| ID | ëª¨ë¸ êµ¬ì„±                           | COCO (Zero-Shot) | COCO (Fine-Tune) | LVIS (Zero-Shot) |
|----|-------------------------------------|------------------|------------------|------------------|
| 0  | âœ… Grounding DINO (Full Model)       | **46.7**         | **56.9**         | **16.1**         |
| 1  | âŒ w/o Encoder Fusion                | 45.8             | 56.1             | 13.1             |
| 2  | âŒ Static Query Selection            | 46.3             | 56.6             | 13.6             |
| 3  | âŒ w/o Text Cross-Attention          | 46.1             | 56.3             | 14.3             |
| 4  | âŒ Word-Level Text Prompt (vs. Sub-sentence) | 46.4    | 56.6             | 15.6             |

---

#### ğŸ” í•´ì„ ë° êµ¬ì„± ìš”ì†Œë³„ ì˜í–¥ ë¶„ì„

1. **Encoder Fusion ì œê±° (ëª¨ë¸ #1)**
   - COCO: **-0.9 AP**
   - LVIS: **-3.0 AP**
   - â¤ **ê°€ì¥ í° ì„±ëŠ¥ ì €í•˜** â†’ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ê¹Šì€ ìœµí•©ì´ í•µì‹¬ ì—­í• 

2. **Static Query Selection (ëª¨ë¸ #2)**
   - ì¿¼ë¦¬ë¥¼ ë™ì ìœ¼ë¡œ ì„ íƒí•˜ì§€ ì•Šê³  ê³ ì •ëœ ë°©ì‹ ì‚¬ìš©
   - LVIS ì„±ëŠ¥ **-2.5 AP í•˜ë½**
   - â¤ ë™ì  ì¿¼ë¦¬ ì„ íƒì´ **ì œë¡œìƒ· íƒì§€ì— ìœ ì˜ë¯¸í•œ ê¸°ì—¬**

3. **Text Cross-Attention ì œê±° (ëª¨ë¸ #3)**
   - COCO/Fine-Tune ì˜í–¥ ì‘ì§€ë§Œ, LVISì—ì„œëŠ” **-1.8 AP**
   - â¤ í…ìŠ¤íŠ¸ ì •ë³´ê°€ ë””ì½”ë”ì— ì§ì ‘ ë°˜ì˜ë  ë•Œ íš¨ê³¼ ì¡´ì¬

4. **Word-level Prompt ì‚¬ìš© (ëª¨ë¸ #4)**
   - Sub-sentence ëŒ€ì‹  ì „ì²´ ë¬¸ì¥ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
   - LVIS ì„±ëŠ¥ **-0.5 AP**
   - â¤ Sub-sentence ë°©ì‹ì´ **fine-grained í‘œí˜„ì— ìœ ë¦¬**

---

#### âœ… ê²°ë¡  ìš”ì•½

- **Encoder Fusion**ì´ ê°€ì¥ í° ì„±ëŠ¥ í–¥ìƒì„ ì£¼ëŠ” í•µì‹¬ êµ¬ì„± ìš”ì†Œì„ì´ í™•ì¸ë¨  
- **Query Selection**ê³¼ **Text Cross-Attention**ì€ íŠ¹íˆ **LVISì™€ ê°™ì€ ì„¸ë¶„í™”ëœ ì˜¤í”ˆì…‹ ë°ì´í„°ì…‹**ì—ì„œ íš¨ê³¼ì 
- **Sub-sentence í…ìŠ¤íŠ¸ ì²˜ë¦¬**ëŠ” Word-level ë°©ì‹ë³´ë‹¤ ì •ë°€í•œ í‘œí˜„ë ¥ì„ ì œê³µ

---


### ğŸ’¡ ëŠë‚€ì 

Grounding DINOëŠ” ë‹¨ìˆœíˆ íƒì§€ë¥¼ ì˜í•˜ëŠ” ê²ƒì„ ë„˜ì–´,  
**í…ìŠ¤íŠ¸ì™€ ì‹œê° ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì—°ê²°í•˜ëŠ” ë°©ì‹**ì„ ì˜ ë³´ì—¬ì£¼ëŠ” ë…¼ë¬¸
ê¸°ì¡´ í•™ìŠµëœ ê°ì±„ë¥¼ í…€ì–´ í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ ê°ì±„ íƒìƒ‰ì„ í•œë‹¤ëŠ” ê²ƒì´ ì¸ìƒì ì´ì—‡ë‹¤!  

---

### ğŸ“š ì°¸ê³  ì‚¬í•­

1. Grounding DINO paper: https://arxiv.org/abs/2303.05499  
2. Grounding DINO GitHub: https://github.com/IDEA-Research/GroundingDINO  
3. chatGPTì˜ ìš”ì•½ëŠ¥ë ¥!!

