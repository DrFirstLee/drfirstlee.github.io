---
layout: post
title: "Image classification using ViT with Python - íŒŒì´ì¬ìœ¼ë¡œ ViT ëª¨ë¸ì„ í™œìš©, ì´ë¯¸ì§€ ë¶„ë¥˜í•˜ê¸°"
author: [DrFirst]
date: 2025-04-27 11:00:00 +0900
categories: [AI,Experiment]
tags: [ ViT,AI, Python,Deep Learning, Image Embedding, ViT-B/32, torchvision,vit-base-patch16-224]
lastmod : 2025-04-27 11:00:00
sitemap :
  changefreq : weekly
  priority : 0.9

---

## (English) Exploring Image Classification with ViT Model in Python

Hello everyone! ğŸ˜Š

In the [previous post](https://drfirstlee.github.io/posts/ViT/#image-you-can-do-transformer-too---the-emergence-of-vit-iclr-2021), we delved into the theory behind ViT based on the original paper! Today, we will actually download this ViT model and perform image classification in a Python environment!!

## 1. Importing ViT Model from torchvision! (The Simplest Way)

You can easily import the Vision Transformer (ViT) model through **torchvision**, a core library for image-related tasks in the PyTorch ecosystem.

### What kind of package is torchvision that provides models?

**torchvision** is a package developed and maintained by the PyTorch team, providing commonly used datasets, image transformations (transforms), and **pre-trained model architectures** in the field of computer vision.

torchvision provides models for the following reasons:

* **Convenience:** It supports researchers and developers in easily utilizing models with verified performance without the hassle of implementing image-related deep learning models from scratch.
* **Rapid Prototyping:** Pre-trained models allow for quick experimentation with new ideas and development of prototypes.
* **Saving Learning Resources:** Using models pre-trained on large-scale datasets saves the time and computational resources required for direct training.
* **Leveraging Learned Representations:** Pre-trained models have already learned general image features, enabling good performance on specific tasks with less data (transfer learning).

### Types and Features of ViT Models Provided by torchvision

torchvision provides various CNN-based models as well as ViT models. Currently (as of April 28, 2025), the main types and features of ViT models provided by torchvision are as follows:

| Name       | Patch Size | Model Name | Features                                                                                                                               |
| :--------- | :---------- | :--------- | :--------------------------------------------------------------------------------------------------------------------------------------- |
| ViT-Base   | 16x16      | vit_b_16   | Offers a balanced size and performance.                                                                                                |
| ViT-Base   | 32x32      | vit_b_32   | Larger patch size can reduce computation but may miss fine-grained features.                                                            |
| ViT-Large  | 16x16      | vit_l_16   | Has more layers and a larger hidden dimension than the Base model, aiming for higher performance. Requires more computational resources. |
| ViT-Large  | 32x32      | vit_l_32   | A Large model with a larger patch size.                                                                                                |
| ViT-Huge   | 14x14      | vit_h_14   | One of the largest ViT models, aiming for top-level performance but requires very significant computational resources.                     |

These models all come with pre-trained weights on the ImageNet dataset, allowing for immediate use in image classification tasks.  
The letters 'b', 'l', and 'h' in the model names indicate the Base, Large, and Huge model sizes, respectively, and the number following indicates the image patch size.  
A larger patch size means the model looks at the image in larger chunks, which can lead to faster processing but potentially lower accuracy.

---

## 2. Today's Image!! ğŸ¶ Let's Start Classifying!

![dog](https://github.com/user-attachments/assets/0ad9326c-a64e-4d01-9e87-f53fe271c19a)

Today, we will use a cute dog image to see how the ViT model classifies it. The ViT model we will use today is pre-trained on the ImageNet dataset!

### What is imagenet\_classes?

`imagenet_classes` is a list of 1000 image classes used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). The pre-trained ViT models provided by torchvision are trained on this ImageNet dataset, so the model's output will be prediction probabilities for these 1000 classes. `imagenet_classes` serves to map these numerical prediction results to human-readable class names (e.g., "golden retriever", "poodle").

### imagenet\_classes.json: A JSON file containing imagenet\_classes information.

Since torchvision itself does not directly include the ImageNet class name list, you need to prepare a separate JSON file containing this information. You can obtain the `imagenet_classes.json` file in the following way:

```python
import requests
import json

# Read JSON file directly from URL
url = "[https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json](https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json)"

response = requests.get(url)
response.raise_for_status()  # Raise an error for bad status codes

# Load JSON data
imagenet_labels = response.json()

with open("imagenet_classes.json", "w") as f:
    json.dump(imagenet_labels, f)
```

## 3\. Let's Begin the Code\!\!

```python
import torch
import torchvision.models as models
from torchvision import transforms
from PIL import Image
import json

# 1. Load ViT model (ViT-Base, patch size 16)
vit_b_16 = models.vit_b_16(pretrained=True)
vit_b_16.eval()  # Set the model to evaluation mode

# 2. Define image preprocessing
# Resize images to 256 and then center crop to 224.
# Normalize using the mean and standard deviation of the ImageNet dataset.
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# 3. Load the dog image (replace with your image file path)
image_path = "dog.jpg"
try:
    image = Image.open(image_path).convert('RGB')
    input_tensor = transform(image).unsqueeze(0) # Add batch dimension
except FileNotFoundError:
    print(f"Error: Image file '{image_path}' not found.")
    exit()

# 4. Perform prediction
with torch.no_grad():
    output = vit_b_16(input_tensor)

# 5. Post-process the prediction results and print the class names
try:
    with open("imagenet_classes.json", "r") as f:
        imagenet_classes = json.load(f)

    _, predicted_idx = torch.sort(output, dim=1, descending=True)
    top_k = 5
    print(f"Top {top_k} prediction results:")
    for i in range(top_k):
        class_idx = predicted_idx[0, i].item()
        confidence = torch.softmax(output, dim=1)[0, class_idx].item()
        print(f"- {imagenet_classes[class_idx]}: {confidence:.4f}")
except FileNotFoundError:
    print("Error: 'imagenet_classes.json' file not found. Please prepare the file in step 2.")
    print("Predicted class indices:", predicted_idx[0, :5].tolist())
except Exception as e:
    print(f"Error during prediction processing: {e}")
```

When you run the code above\!\!\! You can see the Top 5 prediction results as below\~!

```text
Top 5 Prediction Results:
- Golden Retriever: 0.9126
- Labrador Retriever: 0.0104
- Kuvasz: 0.0032
- Airedale Terrier: 0.0014
- tennis ball: 0.0012
```

We can see that the Golden Retriever is predicted with the highest probability of 91.26%.

## 4\. Getting and Running the Model Directly from Hugging Face\! + Analysis (Less Simple, But Customizable)

This time, let's try importing the model directly from the [Hugging Face ViT model](https://huggingface.co/google/vit-base-patch16-224) and proceed\!

```python
import torch
import torchvision.models as models
from torchvision import transforms
from PIL import Image
import json

# 1. Load ViT model (ViT-Base, patch size 16)
vit_b_16 = models.vit_b_16(pretrained=True)
vit_b_16.eval()  # Set the model to evaluation mode

# 2. Define image preprocessing
# Resize images to 256 and then center crop to 224.
# Normalize using the mean and standard deviation of the ImageNet dataset.
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# 3. Load the dog image (replace with your image file path)
image_path = "dog.jpg"
try:
    image = Image.open(image_path).convert('RGB')
    input_tensor = transform(image).unsqueeze(0) # Add batch dimension
except FileNotFoundError:
    print(f"Error: Image file '{image_path}' not found.")
    exit()

# 4. Perform prediction
with torch.no_grad():
    output = vit_b_16(input_tensor)

# 5. Post-process the prediction results and print the class names
with open("imagenet_classes.json", "r") as f:
        imagenet_classes = json.load(f)

_, predicted_idx = torch.sort(output, dim=1, descending=True)
top_k = 5
print(f"Top {top_k} results:")
for i in range(top_k):
        class_idx = predicted_idx[0, i].item()
        confidence = torch.softmax(output, dim=1)[0, class_idx].item()
        print(f"- {imagenet_classes[class_idx]}: {confidence:.4f}")
```

Similarly, it was classified as number 207, Golden Retriever\!\!\!  
But\! Let's look at the differences from the existing torchvision and model customization here\!  

### a. Image Preprocessing Method\!\!

Looking at the preprocessing part below, `ViTFeatureExtractor` already knows the preprocessing method used when the model was trained, allowing you to perform image preprocessing simply without writing a complex `transforms.Compose` process directly\!

```python
feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')

# 3. preprocess : no need to  crop and resize
inputs = feature_extractor(images=image, return_tensors="pt")
```

### b. Viewing the CLS Token\!\!

In the previous theoretical learning post, we learned that it consists of 196 patches + 1 CLS token, totaling 197 patches\! We confirmed that the overall information of the image is contained in this first CLS token\! You can see the CLS Token with the following code\!



```python
from transformers import ViTModel, ViTImageProcessor
import torch
from PIL import Image

# 1. ViTModel (Pure model without classification head)
model = ViTModel.from_pretrained('google/vit-base-patch16-224')
model.eval()

# Feature Extractor â†’ Updated to ViTImageProcessor
processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')

# 2. Load Image
image = Image.open("dog.jpg").convert('RGB')
inputs = processor(images=image, return_tensors="pt")

# 3. Model Inference
with torch.no_grad():
    outputs = model(**inputs)

# 4. Extract CLS Token
last_hidden_state = outputs.last_hidden_state  # (batch_size, num_tokens, hidden_dim)
cls_token = last_hidden_state[:, 0, :]  # The 0th token is CLS

# 5. Print CLS Token
print("CLS token shape:", cls_token.shape)  # torch.Size([1, 768])
print("CLS token values (first 5):", cls_token[0, :5])
```

If you run the code above, you can see the 768-dimensional CLS token as expected\! Subsequent research uses this token for various other information\!

```text
CLS token shape: torch.Size([1, 768])
CLS token values (first 5): tensor([-0.5934, -0.3203, -0.0811,  0.3146, -0.7365])
```

### c. ViT's CAM\!\! Attention Rollout

In traditional CNN-based image classification, a CAM (Class Activation Map) was placed at the end of the model to visualize which parts became important\!\!\!

[CAM Theory Summary\!\!](https://drfirstlee.github.io/posts/CAM_research/)  
[CAM Practice\!\!](https://drfirstlee.github.io/posts/CAM_usage/)  

Our ViT model is different from CAM, so it's difficult to proceed in the same way\! However, you can visualize which of the remaining 196 patches the most important CLS package paid attention to using a method called **Attention Rollout**\!

Looking at the structure\!\!

As shown below, Attention is the process by which [CLS] assigns weights to each patch like "you're important" or "you're not important," and visualizing these attentions is Attention Rollout\!

```text
[CLS]   â†’ Patch_1   (Attention weight: 0.05)
[CLS]   â†’ Patch_2   (Attention weight: 0.02)
[CLS]   â†’ Patch_3   (Attention weight: 0.01)
...
[CLS]   â†’ Patch_196 (Attention weight: 0.03)
```

In the end\!\! You can see a visualization of which patches were considered important as below\!

  * Red areas â†’ Patches that [CLS] paid much attention to.
  * Blue areas â†’ Patches that [CLS] paid less attention to.

Looking at the code:


```python
from transformers import ViTModel, ViTFeatureExtractor
import torch
from PIL import Image
import requests
import matplotlib.pyplot as plt
import numpy as np

# 1. Load model and Feature Extractor
model = ViTModel.from_pretrained('google/vit-base-patch16-224', output_attentions=True)
model.eval()

feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')

# 2. Load Image
image = Image.open("dog.jpg").convert('RGB')
inputs = feature_extractor(images=image, return_tensors="pt")

# 3. Model Inference (output attention)
with torch.no_grad():
    outputs = model(**inputs)
    attentions = outputs.attentions  # list of (batch, heads, tokens, tokens)

# 4. Calculate Attention Rollout
def compute_rollout(attentions):
    # Multiply attention matrices across layers
    result = torch.eye(attentions[0].size(-1))
    for attention in attentions:
        attention_heads_fused = attention.mean(dim=1)[0]  # (tokens, tokens)
        attention_heads_fused += torch.eye(attention_heads_fused.size(-1))
        attention_heads_fused /= attention_heads_fused.sum(dim=-1, keepdim=True)
        result = torch.matmul(result, attention_heads_fused)
    return result

rollout = compute_rollout(attentions)

# 5. Extract Attention from [CLS] token to image patches
mask = rollout[0, 1:].reshape(14, 14).detach().cpu().numpy()

# 6. Visualization
def show_mask_on_image(img, mask):
    img = img.resize((224, 224))
    mask = (mask - mask.min()) / (mask.max() - mask.min())
    fig, ax = plt.subplots()
    ax.imshow(img)
    ax.imshow(mask, cmap='jet', alpha=0.5)
    ax.axis('off')
    plt.show()

show_mask_on_image(image, mask)

```
And the result is\!\!\!??

![patch](https://github.com/user-attachments/assets/82e9e668-d62a-4b06-9464-75e4eb3f967b)

Does it look right\~?

-----

## 5\. ğŸ’¡ Conclusion: Simple and Fast ViT

How was it? You ran the code directly, and it was possible to execute the code easily and quickly\!

Like this, ViT, which was theoretically significant\! Since models trained on large-scale datasets can also be easily implemented in code, research based on Transformers has exploded in the field of computer vision ever since\!

In the future, we will also explore and practice various Vision Transformer-based models such as DINO, DeiT, CLIP, Swin Transformer, etc.! ^^

Thank you!!! ğŸš€ğŸ”¥

---

## (í•œêµ­ì–´) íŒŒì´ì¬ìœ¼ë¡œ ViT ëª¨ë¸ì„ í™œìš©, ì´ë¯¸ì§€ ë¶„ë¥˜í•˜ë³´ê¸°

ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š  

[ì§€ë‚œ í¬ìŠ¤íŒ…](https://drfirstlee.github.io/posts/ViT/#image-you-can-do-transformer-too---the-emergence-of-vit-iclr-2021) ì—ì„œëŠ” ViTì˜ Paperë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ë¡ ì„ ì•Œì•„ë³´ì•˜ëŠ”ë°ìš”!  
ì˜¤ëŠ˜ì€ ì‹¤ì œ ì´ ViTë¸ì„ ë‹¤ìš´ë°›ì•„ Python í™˜ê²½ì—ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜ ì‘ì—…ì„ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤!!  

## 1. ViT ëª¨ë¸!! torchvision ì—ì„œ ì„í¬íŠ¸ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ! (ì œì¼ ê°„ë‹¨)

PyTorch ìƒíƒœê³„ì—ì„œ ì´ë¯¸ì§€ ê´€ë ¨ ì‘ì—…ì„ ìœ„í•œ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤‘ í•˜ë‚˜ì¸ **torchvision**ì„ í†µí•´ Vision Transformer (ViT) ëª¨ë¸ì„ ê°„í¸í•˜ê²Œ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### torchvision ì€ ë¬´ìŠ¨ íŒ¨í‚¤ì§€ì´ê¸¸ë˜ ëª¨ë¸ì„ ì œê³µí•´ì£¼ë‚˜?

**torchvision**ì€ PyTorch íŒ€ì—ì„œ ê°œë°œí•˜ê³  ìœ ì§€ ê´€ë¦¬í•˜ëŠ” íŒ¨í‚¤ì§€ë¡œ, ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹, ì´ë¯¸ì§€ ë³€í™˜(transforms), ê·¸ë¦¬ê³  **ë¯¸ë¦¬ í•™ìŠµëœ(pre-trained) ëª¨ë¸ ì•„í‚¤í…ì²˜**ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### torchvisionì—ì„œ ì œê³µí•˜ëŠ” ViT ëª¨ë¸ ì¢…ë¥˜ì™€ ê° ëª¨ë¸ì˜ íŠ¹ì§•

torchvisionì€ ë‹¤ì–‘í•œ CNN ê¸°ë°˜ ëª¨ë¸ë¿ë§Œ ì•„ë‹ˆë¼ ViT ëª¨ë¸ë„ ì œê³µí•©ë‹ˆë‹¤. í˜„ì¬ (2025ë…„ 4ì›” ê¸°ì¤€) torchvisionì—ì„œ ì œê³µí•˜ëŠ” ì£¼ìš” ViT ëª¨ë¸ ì¢…ë¥˜ì™€ íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

| ì´ë¦„       | íŒ¨ì¹˜ ì‚¬ì´ì¦ˆ | ëª¨ë¸ëª…      | íŠ¹ì§•                                                                                                                               |
| :--------- | :---------- | :---------- | :--------------------------------------------------------------------------------------------------------------------------------- |
| ViT-Base   | 16x16       | `vit_b_16`  | ê· í˜• ì¡íŒ í¬ê¸°ì™€ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.                                                                                                  |
| ViT-Base   | 32x32       | `vit_b_32`  | ë” í° íŒ¨ì¹˜ í¬ê¸°ë¡œ ì¸í•´ ê³„ì‚°ëŸ‰ì´ ì¤„ì–´ë“¤ ìˆ˜ ìˆì§€ë§Œ, ì„¸ë°€í•œ íŠ¹ì§•ì„ ë†“ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.                                                               |
| ViT-Large  | 16x16       | `vit_l_16`  | Base ëª¨ë¸ë³´ë‹¤ ë” ë§ì€ ë ˆì´ì–´ì™€ í° hidden dimensionì„ ê°€ì ¸ ë” ë†’ì€ ì„±ëŠ¥ì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ë” ë§ì€ ì»´í“¨íŒ… ìì›ì„ ìš”êµ¬í•©ë‹ˆë‹¤.           |
| ViT-Large  | 32x32       | `vit_l_32`  | Large ëª¨ë¸ì— í° íŒ¨ì¹˜ í¬ê¸°ë¥¼ ì ìš©í•œ ëª¨ë¸ì…ë‹ˆë‹¤.                                                                                     |
| ViT-Huge   | 14x14       | `vit_h_14`  | ê°€ì¥ í° ViT ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¡œ, ìµœê³  ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ëª©í‘œë¡œ í•˜ì§€ë§Œ ë§¤ìš° ë§ì€ ì»´í“¨íŒ… ìì›ì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.                                      |

ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì€ ëª¨ë‘ ImageNet ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ì™€ í•¨ê»˜ ì œê³µë˜ì–´, ì´ë¯¸ì§€ ë¶„ë¥˜ ì‘ì—…ì— ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
ëª¨ë¸ ì´ë¦„ì˜ `b`, `l`, `h`ëŠ” ê°ê° Base, Large, Huge ëª¨ë¸ í¬ê¸°ë¥¼ ë‚˜íƒ€ë‚´ë©°, ë’¤ì˜ ìˆ«ìëŠ” ì´ë¯¸ì§€ íŒ¨ì¹˜ì˜ í¬ê¸°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
íŒ¨ì¹˜ í¬ê¸°ê°€ í´ìˆ˜ë¡ ì´ë¯¸ì§€ë¥¼ í¬ê²Œí¬ê²Œ ë³´ëŠ”ê²ƒì´ë‹ˆ ì†ë„ëŠ” ë¹ ë¥´ì§€ë§Œ ì •í™•ë„ê°€ ë‚®ê² ì§€ìš”!?

---


## 2. ì˜¤ëŠ˜ì˜ ì´ë¯¸ì§€!! ğŸ¶  ë¶„ë¥˜ ì‹œì‘!

![dog](https://github.com/user-attachments/assets/0ad9326c-a64e-4d01-9e87-f53fe271c19a)
 
ì˜¤ëŠ˜ì€ ê·€ì—¬ìš´ ê°•ì•„ì§€ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ViT ëª¨ë¸ì´ ì–´ë–»ê²Œ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.  
ê·¸ë¦¬ê³  ì˜¤ëŠ˜ì˜ ViT ëª¨ë¸ì€ Imagenetì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìˆ©ëœ ëª¨ë¸ì„ í™œìš©í•  ì˜ˆì •ì…ë‹ˆë‹¤!!  


### imagenet_classes ì´ë€?

`imagenet_classes`ëŠ” ImageNet Large Scale Visual Recognition Challenge (ILSVRC)ì—ì„œ ì‚¬ìš©ëœ 1000ê°œì˜ ì´ë¯¸ì§€ í´ë˜ìŠ¤ ëª©ë¡ì…ë‹ˆë‹¤.  
torchvisionì—ì„œ ì œê³µí•˜ëŠ” ì‚¬ì „ í•™ìŠµëœ ViT ëª¨ë¸ì€ ì´ ImageNet ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµë˜ì—ˆê¸° ë•Œë¬¸ì—, ëª¨ë¸ì˜ ì¶œë ¥ì€ ì´ 1000ê°œì˜ í´ë˜ìŠ¤ì— ëŒ€í•œ ì˜ˆì¸¡ í™•ë¥ ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. 
`imagenet_classes`ëŠ” ì´ëŸ¬í•œ ìˆ«ì í˜•íƒœì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‚¬ëŒì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í´ë˜ìŠ¤ ì´ë¦„(ì˜ˆ: "golden retriever", "poodle")ìœ¼ë¡œ ë§¤í•‘í•´ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

### imagenet_classes.json : imagenet_classes ì •ë³´ë¥¼ ì €ì¥í•œ json ì…ë‹ˆë‹¤. 

torchvision ìì²´ì—ëŠ” ImageNet í´ë˜ìŠ¤ ì´ë¦„ ëª©ë¡ì´ ì§ì ‘ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šê¸°ì—,  
í•´ë‹¹ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” JSON íŒŒì¼ì„ ë³„ë„ë¡œ ì¤€ë¹„í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ìŒ ë°©ë²•ìœ¼ë¡œ `imagenet_classes.json` íŒŒì¼ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import requests
import json

# URLì—ì„œ ì§ì ‘ JSON íŒŒì¼ ì½ê¸°
url = "https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json"

response = requests.get(url)
response.raise_for_status()  # ìš”ì²­ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë°œìƒ

# JSON ë°ì´í„° ë¡œë“œ
imagenet_labels = response.json()


with open("imagenet_classes.json", "r") as f:
    imagenet_classes = json.load(f)
```

## 3. ì½”ë“œ ë³¸ê²© ì‹œì‘!!

```python
import torch
import torchvision.models as models
from torchvision import transforms
from PIL import Image
import json

# 1. ViT ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (ViT-Base, íŒ¨ì¹˜ í¬ê¸° 16 ì‚¬ìš©)
vit_b_16 = models.vit_b_16(pretrained=True)
vit_b_16.eval()  # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •

# 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì •ì˜
# ì´ë¯¸ì§€ í¬ê¸°ê°€ ë‹¤ ë‹¤ë¥´ë‹ˆ 256ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆí•˜ê³  224ë¡œ ì¤‘ì•™ ë¶€ë¶„ì„ íŒ¨ì¹˜í•©ë‹ˆë‹¤.
# ê·¸ë¦¬ê³  ImageNet ë°ì´í„°ì…‹ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# 3. ê°•ì•„ì§€ ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸° (ë³¸ì¸ì˜ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½í•´ì£¼ì„¸ìš”)
image_path = "dog.jpg"
try:
    image = Image.open(image_path).convert('RGB')
    input_tensor = transform(image).unsqueeze(0) # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
except FileNotFoundError:
    print(f"Error: ì´ë¯¸ì§€ íŒŒì¼ '{image_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# 4. ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ì˜ˆì¸¡ ìˆ˜í–‰
with torch.no_grad():
    output = vit_b_16(input_tensor)

# 5. ì˜ˆì¸¡ ê²°ê³¼ í›„ì²˜ë¦¬ ë° í´ë˜ìŠ¤ ì´ë¦„ ì¶œë ¥
try:
    with open("imagenet_classes.json", "r") as f:
        imagenet_classes = json.load(f)

    _, predicted_idx = torch.sort(output, dim=1, descending=True)
    top_k = 5
    print(f"Top {top_k} ì˜ˆì¸¡ ê²°ê³¼:")
    for i in range(top_k):
        class_idx = predicted_idx[0, i].item()
        confidence = torch.softmax(output, dim=1)[0, class_idx].item()
        print(f"- {imagenet_classes[class_idx]}: {confidence:.4f}")

except FileNotFoundError:
    print("Error: 'imagenet_classes.json' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 2ë‹¨ê³„ì—ì„œ íŒŒì¼ì„ ì¤€ë¹„í•´ì£¼ì„¸ìš”.")
    print("ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ì¸ë±ìŠ¤:", predicted_idx[0, :5].tolist())
except Exception as e:
    print(f"Error during prediction processing: {e}")
```

ìœ„ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´!!!  
ì•„ë˜ì™€ ê°™ì´  Top 5ê°œì˜ ì˜ˆì¸¡ê²°ê³¼ë¥¼ ë³¼ìˆ˜ ìˆëŠ”ë°ìš”~!

```text
Top 5 ì˜ˆì¸¡ ê²°ê³¼:
- Golden Retriever: 0.9126
- Labrador Retriever: 0.0104
- Kuvasz: 0.0032
- Airedale Terrier: 0.0014
- tennis ball: 0.0012
```

ê³¨ë“ ë¦¬íŠ¸ë¦¬ë²„ë¥¼ 91.26%ë¡œ ê°€ì¥ ë†’ì€ í™•ë¥ ë¡œ ì˜ˆì¸¡í•¨ì„ ë³¼ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤


## 4. Huggingface ì—ì„œ ì§ì ‘ ëª¨ë¸ì„ ë°›ì•„ì„œ ì‹¤í–‰í•˜ê¸°! + ë¶„ì„ (ëœ ê°„ë‹¨, but ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥)

ì´ë²ˆì—ëŠ” ì§ì ‘ [í—ˆê¹…í˜ì´ìŠ¤ì˜ ViT ëª¨ë¸](https://huggingface.co/google/vit-base-patch16-224)ë¡œë¶€í„° ì§ì ‘  
ëª¨ë¸ì„ ì„í¬íŠ¸í•˜ì—¬ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤~!  

```python
import torch
import torchvision.models as models
from torchvision import transforms
from PIL import Image
import json

# 1. ViT ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (ViT-Base, íŒ¨ì¹˜ í¬ê¸° 16 ì‚¬ìš©)
vit_b_16 = models.vit_b_16(pretrained=True)
vit_b_16.eval()  # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •

# 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì •ì˜
# ì´ë¯¸ì§€ í¬ê¸°ê°€ ë‹¤ ë‹¤ë¥´ë‹ˆ 256ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆí•˜ê³  224ë¡œ ì¤‘ì•™ ë¶€ë¶„ì„ íŒ¨ì¹˜í•©ë‹ˆë‹¤.
# ê·¸ë¦¬ê³  ImageNet ë°ì´í„°ì…‹ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# 3. ê°•ì•„ì§€ ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸° (ë³¸ì¸ì˜ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½í•´ì£¼ì„¸ìš”)
image_path = "dog.jpg"
try:
    image = Image.open(image_path).convert('RGB')
    input_tensor = transform(image).unsqueeze(0) # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
except FileNotFoundError:
    print(f"Error: ì´ë¯¸ì§€ íŒŒì¼ '{image_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# 4. ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ì˜ˆì¸¡ ìˆ˜í–‰
with torch.no_grad():
    output = vit_b_16(input_tensor)

# 5. ì˜ˆì¸¡ ê²°ê³¼ í›„ì²˜ë¦¬ ë° í´ë˜ìŠ¤ ì´ë¦„ ì¶œë ¥
with open("imagenet_classes.json", "r") as f:
       imagenet_classes = json.load(f)

_, predicted_idx = torch.sort(output, dim=1, descending=True)
top_k = 5
print(f"Top {top_k} ì˜ˆì¸¡ ê²°ê³¼:")
for i in range(top_k):
       class_idx = predicted_idx[0, i].item()
       confidence = torch.softmax(output, dim=1)[0, class_idx].item()
       print(f"- {imagenet_classes[class_idx]}: {confidence:.4f}")


```

ì—­ì‹œ ë§ˆì°¬ê°€ì§€ë¡œ~!! 207ë²ˆ, ê³¨ë“  ë¦¬íŠ¸ë¦¬ë²„ë¡œ êµ¬ë¶„ë˜ì—ˆìŠµë‹ˆë‹¤!!!  
ê·¸ëŸ°ë°! ì—¬ê¸°ì„œì˜ ê¸°ì¡´ torchvisionê³¼ ì°¨ì´ & ëª¨ë¸ ì»¤ìŠ¤í„°ë§ˆì´ì§• ë“±ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤!!

### a. ì´ë¯¸ì§€ì˜ ì „ì²˜ë¦¬ë°©ì‹!!

ì•„ë˜ì˜ ì „ì²˜ë¦¬ ë¶€ë¶„ì„ ë³´ë©´, ViTFeatureExtractorëŠ” í•´ë‹¹ ëª¨ë¸ì´ í•™ìŠµë  ë•Œ ì‚¬ìš©í–ˆë˜ ì „ì²˜ë¦¬ ë°©ì‹ì„ ë¯¸ë¦¬ ì•Œê³  ìˆì–´,  
ë³µì¡í•œ transforms.Compose ê³¼ì •ì„ ì§ì ‘ ì‘ì„±í•˜ì§€ ì•Šê³  ê°„ë‹¨í•˜ê²Œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹µë‹ˆë‹¤~!!

```python
feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')

# 3. ì „ì²˜ë¦¬ : ì§ì ‘ crop ë° resize í•  í•„ìš”ê°€ ì—†ì–´ìš”!
inputs = feature_extractor(images=image, return_tensors="pt")
```

### b. CLS í† í° ë³´ê¸°!!

ì§€ë‚œ ì´ë¡  í•™ìŠµê¸€ì—ì„œ 196ê°œì˜ íŒ¨ì¹˜ + 1ê°œì˜ CLS í† í°ìœ¼ë¡œ 197ê°œì˜ íŒ¨ì¹˜ë¡œ êµ¬ì„±ë¨ì„ ì•Œì•„ë³´ì•˜ëŠ”ë°ìš”~!  
ì´ ì²«ë²ˆì¨°ì˜ CLS í† í°ì— ì´ë¯¸ì§€ì˜ ì „ì²´ì ì¸ ì •ë³´ê°€ í¬í•¨ë¨ì„ í™•ì¸í–ˆì—ˆìŠµë‹ˆë‹¤!!  
ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¡œ CLS Tokenì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!!  


```python
from transformers import ViTModel, ViTImageProcessor
import torch
from PIL import Image

# 1. ViTModel (Classification head ì—†ëŠ” ìˆœìˆ˜ ëª¨ë¸)
model = ViTModel.from_pretrained('google/vit-base-patch16-224')
model.eval()

# Feature Extractor â†’ ViTImageProcessorë¡œ ìµœì‹ í™”
processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')

# 2. ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
image = Image.open("dog.jpg").convert('RGB')
inputs = processor(images=image, return_tensors="pt")

# 3. ëª¨ë¸ ì¶”ë¡ 
with torch.no_grad():
    outputs = model(**inputs)

# 4. CLS í† í° ì¶”ì¶œ
last_hidden_state = outputs.last_hidden_state  # (batch_size, num_tokens, hidden_dim)
cls_token = last_hidden_state[:, 0, :]  # 0ë²ˆì§¸ í† í°ì´ CLS

# 5. CLS í† í° ì¶œë ¥
print("CLS token shape:", cls_token.shape)  # torch.Size([1, 768])
print("CLS token values (ì• 5ê°œ):", cls_token[0, :5])
```

ìœ„ ì½”ë“œë¥¼ ì‹¤í–‰í•´ë³´ë©´, ì˜ˆìƒí•œëŒ€ë¡œ 768 ì°¨ì›ì˜CLS í† í°ì„ ë³¼ìˆ˜ ìˆì§€ìš”~~  
ì´í›„ ì—¬ëŸ¬ ì—°êµ¬ë“¤ì€ ì´ í† í°ì„ í™œìš©í•´ì„œ ë‹¤ë¥¸ ì •ë³´ë¡œ í™œìš©í•˜ê¸°ë„í•©ë‹ˆë‹¤!   

```text 
CLS token shape: torch.Size([1, 768])
CLS token values (ì• 5ê°œ): tensor([-0.5934, -0.3203, -0.0811,  0.3146, -0.7365])
```

### c. ViTì˜ CAM!! Attention Rollout  

ê¸°ì¡´ CNN ë°©ì‹ì˜ ì´ë¯¸ì§€ ë¶„ë¥˜ëŠ” ëª¨ë¸ì˜ ë§ˆì§€ë§‰ë‹¨ì— CAM(Class Activation Map)ì„ ë‘ì–´ì„œ ì–´ë–¤ ë¶€ë¶„ì´ ì¤‘ìš”í•˜ê²Œ ë˜ì—ˆëŠ”ì§€ ì‹œê°í™” í• ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤!!!  

[CAMì˜ ì´ë¡  ì •ë¦¬!!](https://drfirstlee.github.io/posts/CAM_research/)  
[CAM ì‹¤ìŠµ!!](https://drfirstlee.github.io/posts/CAM_usage/) 

ìš°ë¦¬ì˜ ViT ëª¨ë¸ì€ CAMê³¼ëŠ” ë‹¤ë¥´ê¸°ì— ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì§„í–‰ì€ ì–´ë µì§€ë§Œ!!  
**Attention Rollout** ì´ë¼ëŠ” ë°©ì‹ìœ¼ë¡œ ê°€ì¥ ì¤‘ìš”í•œ CLS íŒ¨í‚¤ì¹˜ê°€ ë‚˜ë¨¸ì§€ 196ê°œ íŒ¨ì¹˜ì¤‘ ì–´ë””ë¥¼ ì¤‘ìš”í•˜ê²Œ ë´¤ëŠ”ì§€!! ì‹œê°í™”í• ìˆ˜ ìˆì–´ìš”!!  

êµ¬ì¡°ë¥¼ ë³´ìë©´!!  

ì•„ë˜ì™€ ê°™ì´  [CLS]ê°€ ê° íŒ¨ì¹˜ì— ëŒ€í•´ "ë„ˆ ì¤‘ìš”í•´", "ë„ˆ ë³„ë¡œì•¼" ê°™ì€ ì‹ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ê±¸ Attentionì´ë¼ê³ í•˜ê³ , ê·¸ ì–´í…ì…˜ë“¤ì„ ì‹œê°í™”í•˜ëŠ”ê²ƒì´ì§€ìš”!

```text
[CLS]   â†’ Patch_1   (Attention weight: 0.05)
[CLS]   â†’ Patch_2   (Attention weight: 0.02)
[CLS]   â†’ Patch_3   (Attention weight: 0.01)
...
[CLS]   â†’ Patch_196 (Attention weight: 0.03)
```

ê²°êµ­!! ì–´ë–¤ íŒ¨ì¹˜ê°€ ì¤‘ìš”í•˜ê²Œ ê°„ì£¼ë˜ì—ˆëŠ”ì§€ ì•„ë˜ì™€ ê°™ì´ ì‹œê°í™”ê°€ ë˜ì§€ìš”~!!

- ë¹¨ê°›ê²Œ ë³´ì´ëŠ” ì˜ì—­ â†’ [CLS]ê°€ ë§ì´ ì£¼ëª©í•œ íŒ¨ì¹˜,  
- íŒŒë—ê²Œ ë³´ì´ëŠ” ì˜ì—­ â†’ [CLS]ê°€ ëœ ì£¼ëª©í•œ íŒ¨ì¹˜

ì½”ë“œë¡œ ë³´ë©´~~

```python
from transformers import ViTModel, ViTFeatureExtractor
import torch
from PIL import Image
import requests
import matplotlib.pyplot as plt
import numpy as np

# 1. ëª¨ë¸ê³¼ Feature Extractor ë¶ˆëŸ¬ì˜¤ê¸°
model = ViTModel.from_pretrained('google/vit-base-patch16-224', output_attentions=True)
model.eval()

feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')

# 2. ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
image = Image.open("dog.jpg").convert('RGB')
inputs = feature_extractor(images=image, return_tensors="pt")

# 3. ëª¨ë¸ ì¶”ë¡  (attention ì¶œë ¥)
with torch.no_grad():
    outputs = model(**inputs)
    attentions = outputs.attentions  # list of (batch, heads, tokens, tokens)

# 4. Attention Rollout ê³„ì‚°
def compute_rollout(attentions):
    # Multiply attention matrices across layers
    result = torch.eye(attentions[0].size(-1))
    for attention in attentions:
        attention_heads_fused = attention.mean(dim=1)[0]  # (tokens, tokens)
        attention_heads_fused += torch.eye(attention_heads_fused.size(-1))
        attention_heads_fused /= attention_heads_fused.sum(dim=-1, keepdim=True)
        result = torch.matmul(result, attention_heads_fused)
    return result

rollout = compute_rollout(attentions)

# 5. [CLS] í† í°ì—ì„œ ì´ë¯¸ì§€ íŒ¨ì¹˜ë¡œ ê°€ëŠ” Attention ì¶”ì¶œ
mask = rollout[0, 1:].reshape(14, 14).detach().cpu().numpy()

# 6. ì‹œê°í™”
def show_mask_on_image(img, mask):
    img = img.resize((224, 224))
    mask = (mask - mask.min()) / (mask.max() - mask.min())
    fig, ax = plt.subplots()
    ax.imshow(img)
    ax.imshow(mask, cmap='jet', alpha=0.5)
    ax.axis('off')
    plt.show()

show_mask_on_image(image, mask)

```

ì´ê³  ê·¸ ê²°ê³¼ëŠ”!!!??

![patch](https://github.com/user-attachments/assets/82e9e668-d62a-4b06-9464-75e4eb3f967b)

ì…ë‹ˆë‹¤~! ë§ëŠ”ê²ƒ ê°™ë‚˜ìš”~?

---

## 5. ğŸ’¡ ê²°ë¡  : ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ ViT

ì–´ë–¤ê°€ìš”? ì½”ë“œë¥¼ ì§ì ‘ ì‹¤í–‰í•´ë³´ì•˜ëŠ”ë°~!!  
í° ì–´ë ¤ì›€ì—†ì´, ê·¸ë¦¬ê³  ë¹ ë¥´ê²Œ ì½”ë“œë¥¼ ì‹¤í–‰í• ìˆ˜ ìˆì—ˆì§€ìš”!?

ì´ì²˜ëŸ¼ ì´ë¡ ì ìœ¼ë¡œë„ ìœ ì˜ë¯¸í–ˆë˜ ViT! 
ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ í•™ìŠµëœ ëª¨ë¸ì´ ì½”ë“œë¡œë„ ì‰½ê²Œ êµ¬í˜„ì´ ê°€ëŠ¥í•´ì„œ ì´í›„ë¡œ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œ Transformer ê¸°ë°˜ ì—°êµ¬ê°€ í­ë°œì ìœ¼ë¡œ ì¦ê°€í•˜ê²Œ ë˜ì—ˆë‹¤ê³ í•©ë‹ˆë‹¤!!  

ì•ìœ¼ë¡œ DINO, DeiT, CLIP, Swin Transformer ë“± ë‹¤ì–‘í•œ ë¹„ì „ Transformer ê¸°ë°˜ì˜ ëª¨ë¸ë„ ì•Œì•„ë³´ë©° ì‹¤ìŠµí•´ë³¼ ìˆ˜ ìˆë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤~! ^^

ê°ì‚¬í•©ë‹ˆë‹¤!!! ğŸš€ğŸ”¥
