---
layout: post
title: "Image classification using ViT with Python - ÌååÏù¥Ïç¨ÏúºÎ°ú ViT Î™®Îç∏ÏùÑ ÌôúÏö©, Ïù¥ÎØ∏ÏßÄ Î∂ÑÎ•òÌïòÍ∏∞"
author: [DrFirst]
date: 2025-04-27 11:00:00 +0900
categories: [AI,Experiment]
tags: [ ViT,AI, Python,Deep Learning, Image Embedding, ViT-B/32, torchvision,vit-base-patch16-224]
lastmod : 2025-04-27 11:00:00
sitemap :
  changefreq : weekly
  priority : 0.9

---

## (English) Exploring Image Classification with ViT Model in Python

Hello everyone! üòä

In the [previous post](https://drfirstlee.github.io/posts/ViT/#image-you-can-do-transformer-too---the-emergence-of-vit-iclr-2021), we delved into the theory behind ViT based on the original paper! Today, we will actually download this ViT model and perform image classification in a Python environment!!

## 1. Importing ViT Model from torchvision! (The Simplest Way)

You can easily import the Vision Transformer (ViT) model through **torchvision**, a core library for image-related tasks in the PyTorch ecosystem.

### What kind of package is torchvision that provides models?

**torchvision** is a package developed and maintained by the PyTorch team, providing commonly used datasets, image transformations (transforms), and **pre-trained model architectures** in the field of computer vision.

torchvision provides models for the following reasons:

* **Convenience:** It supports researchers and developers in easily utilizing models with verified performance without the hassle of implementing image-related deep learning models from scratch.
* **Rapid Prototyping:** Pre-trained models allow for quick experimentation with new ideas and development of prototypes.
* **Saving Learning Resources:** Using models pre-trained on large-scale datasets saves the time and computational resources required for direct training.
* **Leveraging Learned Representations:** Pre-trained models have already learned general image features, enabling good performance on specific tasks with less data (transfer learning).

### Types and Features of ViT Models Provided by torchvision

torchvision provides various CNN-based models as well as ViT models. Currently (as of April 28, 2025), the main types and features of ViT models provided by torchvision are as follows:

| Name       | Patch Size | Model Name | Features                                                                                                                               |
| :--------- | :---------- | :--------- | :--------------------------------------------------------------------------------------------------------------------------------------- |
| ViT-Base   | 16x16      | vit_b_16   | Offers a balanced size and performance.                                                                                                |
| ViT-Base   | 32x32      | vit_b_32   | Larger patch size can reduce computation but may miss fine-grained features.                                                            |
| ViT-Large  | 16x16      | vit_l_16   | Has more layers and a larger hidden dimension than the Base model, aiming for higher performance. Requires more computational resources. |
| ViT-Large  | 32x32      | vit_l_32   | A Large model with a larger patch size.                                                                                                |
| ViT-Huge   | 14x14      | vit_h_14   | One of the largest ViT models, aiming for top-level performance but requires very significant computational resources.                     |

These models all come with pre-trained weights on the ImageNet dataset, allowing for immediate use in image classification tasks.  
The letters 'b', 'l', and 'h' in the model names indicate the Base, Large, and Huge model sizes, respectively, and the number following indicates the image patch size.  
A larger patch size means the model looks at the image in larger chunks, which can lead to faster processing but potentially lower accuracy.

---

## 2. Today's Image!! üê∂ Let's Start Classifying!

![dog](https://github.com/user-attachments/assets/0ad9326c-a64e-4d01-9e87-f53fe271c19a)

Today, we will use a cute dog image to see how the ViT model classifies it. The ViT model we will use today is pre-trained on the ImageNet dataset!

### What is imagenet\_classes?

`imagenet_classes` is a list of 1000 image classes used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). The pre-trained ViT models provided by torchvision are trained on this ImageNet dataset, so the model's output will be prediction probabilities for these 1000 classes. `imagenet_classes` serves to map these numerical prediction results to human-readable class names (e.g., "golden retriever", "poodle").

### imagenet\_classes.json: A JSON file containing imagenet\_classes information.

Since torchvision itself does not directly include the ImageNet class name list, you need to prepare a separate JSON file containing this information. You can obtain the `imagenet_classes.json` file in the following way:

```python
import requests
import json

# Read JSON file directly from URL
url = "[https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json](https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json)"

response = requests.get(url)
response.raise_for_status()  # Raise an error for bad status codes

# Load JSON data
imagenet_labels = response.json()

with open("imagenet_classes.json", "w") as f:
    json.dump(imagenet_labels, f)
```

## 3\. Let's Begin the Code\!\!

```python
import torch
import torchvision.models as models
from torchvision import transforms
from PIL import Image
import json

# 1. Load ViT model (ViT-Base, patch size 16)
vit_b_16 = models.vit_b_16(pretrained=True)
vit_b_16.eval()  # Set the model to evaluation mode

# 2. Define image preprocessing
# Resize images to 256 and then center crop to 224.
# Normalize using the mean and standard deviation of the ImageNet dataset.
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# 3. Load the dog image (replace with your image file path)
image_path = "dog.jpg"
try:
    image = Image.open(image_path).convert('RGB')
    input_tensor = transform(image).unsqueeze(0) # Add batch dimension
except FileNotFoundError:
    print(f"Error: Image file '{image_path}' not found.")
    exit()

# 4. Perform prediction
with torch.no_grad():
    output = vit_b_16(input_tensor)

# 5. Post-process the prediction results and print the class names
try:
    with open("imagenet_classes.json", "r") as f:
        imagenet_classes = json.load(f)

    _, predicted_idx = torch.sort(output, dim=1, descending=True)
    top_k = 5
    print(f"Top {top_k} prediction results:")
    for i in range(top_k):
        class_idx = predicted_idx[0, i].item()
        confidence = torch.softmax(output, dim=1)[0, class_idx].item()
        print(f"- {imagenet_classes[class_idx]}: {confidence:.4f}")
except FileNotFoundError:
    print("Error: 'imagenet_classes.json' file not found. Please prepare the file in step 2.")
    print("Predicted class indices:", predicted_idx[0, :5].tolist())
except Exception as e:
    print(f"Error during prediction processing: {e}")
```

When you run the code above\!\!\! You can see the Top 5 prediction results as below\~!

```text
Top 5 Prediction Results:
- Golden Retriever: 0.9126
- Labrador Retriever: 0.0104
- Kuvasz: 0.0032
- Airedale Terrier: 0.0014
- tennis ball: 0.0012
```

We can see that the Golden Retriever is predicted with the highest probability of 91.26%.

## 4\. Getting and Running the Model Directly from Hugging Face\! + Analysis (Less Simple, But Customizable)

This time, let's try importing the model directly from the [Hugging Face ViT model](https://huggingface.co/google/vit-base-patch16-224) and proceed\!

```python
import torch
import torchvision.models as models
from torchvision import transforms
from PIL import Image
import json

# 1. Load ViT model (ViT-Base, patch size 16)
vit_b_16 = models.vit_b_16(pretrained=True)
vit_b_16.eval()  # Set the model to evaluation mode

# 2. Define image preprocessing
# Resize images to 256 and then center crop to 224.
# Normalize using the mean and standard deviation of the ImageNet dataset.
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# 3. Load the dog image (replace with your image file path)
image_path = "dog.jpg"
try:
    image = Image.open(image_path).convert('RGB')
    input_tensor = transform(image).unsqueeze(0) # Add batch dimension
except FileNotFoundError:
    print(f"Error: Image file '{image_path}' not found.")
    exit()

# 4. Perform prediction
with torch.no_grad():
    output = vit_b_16(input_tensor)

# 5. Post-process the prediction results and print the class names
with open("imagenet_classes.json", "r") as f:
        imagenet_classes = json.load(f)

_, predicted_idx = torch.sort(output, dim=1, descending=True)
top_k = 5
print(f"Top {top_k} results:")
for i in range(top_k):
        class_idx = predicted_idx[0, i].item()
        confidence = torch.softmax(output, dim=1)[0, class_idx].item()
        print(f"- {imagenet_classes[class_idx]}: {confidence:.4f}")
```

Similarly, it was classified as number 207, Golden Retriever\!\!\!  
But\! Let's look at the differences from the existing torchvision and model customization here\!  

### a. Image Preprocessing Method\!\!

Looking at the preprocessing part below, `ViTFeatureExtractor` already knows the preprocessing method used when the model was trained, allowing you to perform image preprocessing simply without writing a complex `transforms.Compose` process directly\!

```python
feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')

# 3. preprocess : no need to  crop and resize
inputs = feature_extractor(images=image, return_tensors="pt")
```

### b. Viewing the CLS Token\!\!

In the previous theoretical learning post, we learned that it consists of 196 patches + 1 CLS token, totaling 197 patches\! We confirmed that the overall information of the image is contained in this first CLS token\! You can see the CLS Token with the following code\!



```python
from transformers import ViTModel, ViTImageProcessor
import torch
from PIL import Image

# 1. ViTModel (Pure model without classification head)
model = ViTModel.from_pretrained('google/vit-base-patch16-224')
model.eval()

# Feature Extractor ‚Üí Updated to ViTImageProcessor
processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')

# 2. Load Image
image = Image.open("dog.jpg").convert('RGB')
inputs = processor(images=image, return_tensors="pt")

# 3. Model Inference
with torch.no_grad():
    outputs = model(**inputs)

# 4. Extract CLS Token
last_hidden_state = outputs.last_hidden_state  # (batch_size, num_tokens, hidden_dim)
cls_token = last_hidden_state[:, 0, :]  # The 0th token is CLS

# 5. Print CLS Token
print("CLS token shape:", cls_token.shape)  # torch.Size([1, 768])
print("CLS token values (first 5):", cls_token[0, :5])
```

If you run the code above, you can see the 768-dimensional CLS token as expected\! Subsequent research uses this token for various other information\!

```text
CLS token shape: torch.Size([1, 768])
CLS token values (first 5): tensor([-0.5934, -0.3203, -0.0811,  0.3146, -0.7365])
```

### c. ViT's CAM\!\! Attention Rollout

In traditional CNN-based image classification, a CAM (Class Activation Map) was placed at the end of the model to visualize which parts became important\!\!\!

[CAM Theory Summary\!\!](https://drfirstlee.github.io/posts/CAM_research/)  
[CAM Practice\!\!](https://drfirstlee.github.io/posts/CAM_usage/)  

Our ViT model is different from CAM, so it's difficult to proceed in the same way\! However, you can visualize which of the remaining 196 patches the most important CLS package paid attention to using a method called **Attention Rollout**\!

Looking at the structure\!\!

As shown below, Attention is the process by which [CLS] assigns weights to each patch like "you're important" or "you're not important," and visualizing these attentions is Attention Rollout\!

```text
[CLS]   ‚Üí Patch_1   (Attention weight: 0.05)
[CLS]   ‚Üí Patch_2   (Attention weight: 0.02)
[CLS]   ‚Üí Patch_3   (Attention weight: 0.01)
...
[CLS]   ‚Üí Patch_196 (Attention weight: 0.03)
```

In the end\!\! You can see a visualization of which patches were considered important as below\!

  * Red areas ‚Üí Patches that [CLS] paid much attention to.
  * Blue areas ‚Üí Patches that [CLS] paid less attention to.

Looking at the code:


```python
from transformers import ViTModel, ViTFeatureExtractor
import torch
from PIL import Image
import requests
import matplotlib.pyplot as plt
import numpy as np

# 1. Load model and Feature Extractor
model = ViTModel.from_pretrained('google/vit-base-patch16-224', output_attentions=True)
model.eval()

feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')

# 2. Load Image
image = Image.open("dog.jpg").convert('RGB')
inputs = feature_extractor(images=image, return_tensors="pt")

# 3. Model Inference (output attention)
with torch.no_grad():
    outputs = model(**inputs)
    attentions = outputs.attentions  # list of (batch, heads, tokens, tokens)

# 4. Calculate Attention Rollout
def compute_rollout(attentions):
    # Multiply attention matrices across layers
    result = torch.eye(attentions[0].size(-1))
    for attention in attentions:
        attention_heads_fused = attention.mean(dim=1)[0]  # (tokens, tokens)
        attention_heads_fused += torch.eye(attention_heads_fused.size(-1))
        attention_heads_fused /= attention_heads_fused.sum(dim=-1, keepdim=True)
        result = torch.matmul(result, attention_heads_fused)
    return result

rollout = compute_rollout(attentions)

# 5. Extract Attention from [CLS] token to image patches
mask = rollout[0, 1:].reshape(14, 14).detach().cpu().numpy()

# 6. Visualization
def show_mask_on_image(img, mask):
    img = img.resize((224, 224))
    mask = (mask - mask.min()) / (mask.max() - mask.min())
    fig, ax = plt.subplots()
    ax.imshow(img)
    ax.imshow(mask, cmap='jet', alpha=0.5)
    ax.axis('off')
    plt.show()

show_mask_on_image(image, mask)

```
And the result is\!\!\!??

![patch](https://github.com/user-attachments/assets/82e9e668-d62a-4b06-9464-75e4eb3f967b)

Does it look right\~?

-----

## 5\. üí° Conclusion: Simple and Fast ViT

How was it? You ran the code directly, and it was possible to execute the code easily and quickly\!

Like this, ViT, which was theoretically significant\! Since models trained on large-scale datasets can also be easily implemented in code, research based on Transformers has exploded in the field of computer vision ever since\!

In the future, we will also explore and practice various Vision Transformer-based models such as DINO, DeiT, CLIP, Swin Transformer, etc.! ^^

Thank you!!! üöÄüî•

---

## (ÌïúÍµ≠Ïñ¥) ÌååÏù¥Ïç¨ÏúºÎ°ú ViT Î™®Îç∏ÏùÑ ÌôúÏö©, Ïù¥ÎØ∏ÏßÄ Î∂ÑÎ•òÌïòÎ≥¥Í∏∞

ÏïàÎÖïÌïòÏÑ∏Ïöî! üòä  

[ÏßÄÎÇú Ìè¨Ïä§ÌåÖ](https://drfirstlee.github.io/posts/ViT/#image-you-can-do-transformer-too---the-emergence-of-vit-iclr-2021) ÏóêÏÑúÎäî ViTÏùò PaperÎ•º Î∞îÌÉïÏúºÎ°ú Ïù¥Î°†ÏùÑ ÏïåÏïÑÎ≥¥ÏïòÎäîÎç∞Ïöî!  
Ïò§ÎäòÏùÄ Ïã§Ï†ú Ïù¥ ViTÎç∏ÏùÑ Îã§Ïö¥Î∞õÏïÑ Python ÌôòÍ≤ΩÏóêÏÑú Ïù¥ÎØ∏ÏßÄ Î∂ÑÎ•ò ÏûëÏóÖÏùÑ ÏßÑÌñâÌï¥Î≥¥Í≤†ÏäµÎãàÎã§!!  

## 1. ViT Î™®Îç∏!! torchvision ÏóêÏÑú ÏûÑÌè¨Ìä∏ ÌïòÎäî Î∞©ÏãùÏúºÎ°ú! (Ï†úÏùº Í∞ÑÎã®)

PyTorch ÏÉùÌÉúÍ≥ÑÏóêÏÑú Ïù¥ÎØ∏ÏßÄ Í¥ÄÎ†® ÏûëÏóÖÏùÑ ÏúÑÌïú ÌïµÏã¨ ÎùºÏù¥Î∏åÎü¨Î¶¨ Ï§ë ÌïòÎÇòÏù∏ **torchvision**ÏùÑ ÌÜµÌï¥ Vision Transformer (ViT) Î™®Îç∏ÏùÑ Í∞ÑÌé∏ÌïòÍ≤å Î∂àÎü¨ÏôÄ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.

### torchvision ÏùÄ Î¨¥Ïä® Ìå®ÌÇ§ÏßÄÏù¥Í∏∏Îûò Î™®Îç∏ÏùÑ Ï†úÍ≥µÌï¥Ï£ºÎÇò?

**torchvision**ÏùÄ PyTorch ÌåÄÏóêÏÑú Í∞úÎ∞úÌïòÍ≥† Ïú†ÏßÄ Í¥ÄÎ¶¨ÌïòÎäî Ìå®ÌÇ§ÏßÄÎ°ú, Ïª¥Ìì®ÌÑ∞ ÎπÑÏ†Ñ Î∂ÑÏïºÏóêÏÑú ÏûêÏ£º ÏÇ¨Ïö©ÎêòÎäî Îç∞Ïù¥ÌÑ∞ÏÖã, Ïù¥ÎØ∏ÏßÄ Î≥ÄÌôò(transforms), Í∑∏Î¶¨Í≥† **ÎØ∏Î¶¨ ÌïôÏäµÎêú(pre-trained) Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò**Î•º Ï†úÍ≥µÌï©ÎãàÎã§.

### torchvisionÏóêÏÑú Ï†úÍ≥µÌïòÎäî ViT Î™®Îç∏ Ï¢ÖÎ•òÏôÄ Í∞Å Î™®Îç∏Ïùò ÌäπÏßï

torchvisionÏùÄ Îã§ÏñëÌïú CNN Í∏∞Î∞ò Î™®Îç∏ÎøêÎßå ÏïÑÎãàÎùº ViT Î™®Îç∏ÎèÑ Ï†úÍ≥µÌï©ÎãàÎã§. ÌòÑÏû¨ (2025ÎÖÑ 4Ïõî Í∏∞Ï§Ä) torchvisionÏóêÏÑú Ï†úÍ≥µÌïòÎäî Ï£ºÏöî ViT Î™®Îç∏ Ï¢ÖÎ•òÏôÄ ÌäπÏßïÏùÄ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§.

| Ïù¥Î¶Ñ       | Ìå®Ïπò ÏÇ¨Ïù¥Ï¶à | Î™®Îç∏Î™Ö      | ÌäπÏßï                                                                                                                               |
| :--------- | :---------- | :---------- | :--------------------------------------------------------------------------------------------------------------------------------- |
| ViT-Base   | 16x16       | `vit_b_16`  | Í∑†Ìòï Ïû°Ìûå ÌÅ¨Í∏∞ÏôÄ ÏÑ±Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.                                                                                                  |
| ViT-Base   | 32x32       | `vit_b_32`  | Îçî ÌÅ∞ Ìå®Ïπò ÌÅ¨Í∏∞Î°ú Ïù∏Ìï¥ Í≥ÑÏÇ∞ÎüâÏù¥ Ï§ÑÏñ¥Îì§ Ïàò ÏûàÏßÄÎßå, ÏÑ∏Î∞ÄÌïú ÌäπÏßïÏùÑ ÎÜìÏπ† Ïàò ÏûàÏäµÎãàÎã§.                                                               |
| ViT-Large  | 16x16       | `vit_l_16`  | Base Î™®Îç∏Î≥¥Îã§ Îçî ÎßéÏùÄ Î†àÏù¥Ïñ¥ÏôÄ ÌÅ∞ hidden dimensionÏùÑ Í∞ÄÏ†∏ Îçî ÎÜíÏùÄ ÏÑ±Îä•ÏùÑ Î™©ÌëúÎ°ú Ìï©ÎãàÎã§. Îçî ÎßéÏùÄ Ïª¥Ìì®ÌåÖ ÏûêÏõêÏùÑ ÏöîÍµ¨Ìï©ÎãàÎã§.           |
| ViT-Large  | 32x32       | `vit_l_32`  | Large Î™®Îç∏Ïóê ÌÅ∞ Ìå®Ïπò ÌÅ¨Í∏∞Î•º Ï†ÅÏö©Ìïú Î™®Îç∏ÏûÖÎãàÎã§.                                                                                     |
| ViT-Huge   | 14x14       | `vit_h_14`  | Í∞ÄÏû• ÌÅ∞ ViT Î™®Îç∏ Ï§ë ÌïòÎÇòÎ°ú, ÏµúÍ≥† ÏàòÏ§ÄÏùò ÏÑ±Îä•ÏùÑ Î™©ÌëúÎ°ú ÌïòÏßÄÎßå Îß§Ïö∞ ÎßéÏùÄ Ïª¥Ìì®ÌåÖ ÏûêÏõêÏùÑ ÌïÑÏöîÎ°ú Ìï©ÎãàÎã§.                                      |

Ïù¥Îü¨Ìïú Î™®Îç∏Îì§ÏùÄ Î™®Îëê ImageNet Îç∞Ïù¥ÌÑ∞ÏÖãÏúºÎ°ú ÏÇ¨Ï†Ñ ÌïôÏäµÎêú Í∞ÄÏ§ëÏπòÏôÄ Ìï®Íªò Ï†úÍ≥µÎêòÏñ¥, Ïù¥ÎØ∏ÏßÄ Î∂ÑÎ•ò ÏûëÏóÖÏóê Î∞îÎ°ú ÌôúÏö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.  
Î™®Îç∏ Ïù¥Î¶ÑÏùò `b`, `l`, `h`Îäî Í∞ÅÍ∞Å Base, Large, Huge Î™®Îç∏ ÌÅ¨Í∏∞Î•º ÎÇòÌÉÄÎÇ¥Î©∞, Îí§Ïùò Ïà´ÏûêÎäî Ïù¥ÎØ∏ÏßÄ Ìå®ÏπòÏùò ÌÅ¨Í∏∞Î•º ÏùòÎØ∏Ìï©ÎãàÎã§.
Ìå®Ïπò ÌÅ¨Í∏∞Í∞Ä ÌÅ¥ÏàòÎ°ù Ïù¥ÎØ∏ÏßÄÎ•º ÌÅ¨Í≤åÌÅ¨Í≤å Î≥¥ÎäîÍ≤ÉÏù¥Îãà ÏÜçÎèÑÎäî Îπ†Î•¥ÏßÄÎßå Ï†ïÌôïÎèÑÍ∞Ä ÎÇÆÍ≤†ÏßÄÏöî!?

---


## 2. Ïò§ÎäòÏùò Ïù¥ÎØ∏ÏßÄ!! üê∂  Î∂ÑÎ•ò ÏãúÏûë!

![dog](https://github.com/user-attachments/assets/0ad9326c-a64e-4d01-9e87-f53fe271c19a)
 
Ïò§ÎäòÏùÄ Í∑ÄÏó¨Ïö¥ Í∞ïÏïÑÏßÄ Ïù¥ÎØ∏ÏßÄÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ViT Î™®Îç∏Ïù¥ Ïñ¥ÎñªÍ≤å Ïù¥ÎØ∏ÏßÄÎ•º Î∂ÑÎ•òÌïòÎäîÏßÄ ÌôïÏù∏Ìï¥Î≥¥Í≤†ÏäµÎãàÎã§.  
Í∑∏Î¶¨Í≥† Ïò§ÎäòÏùò ViT Î™®Îç∏ÏùÄ ImagenetÏùò Îç∞Ïù¥ÌÑ∞ÏÖãÏúºÎ°ú ÌïôÏà©Îêú Î™®Îç∏ÏùÑ ÌôúÏö©Ìï† ÏòàÏ†ïÏûÖÎãàÎã§!!  


### imagenet_classes Ïù¥ÎûÄ?

`imagenet_classes`Îäî ImageNet Large Scale Visual Recognition Challenge (ILSVRC)ÏóêÏÑú ÏÇ¨Ïö©Îêú 1000Í∞úÏùò Ïù¥ÎØ∏ÏßÄ ÌÅ¥ÎûòÏä§ Î™©Î°ùÏûÖÎãàÎã§.  
torchvisionÏóêÏÑú Ï†úÍ≥µÌïòÎäî ÏÇ¨Ï†Ñ ÌïôÏäµÎêú ViT Î™®Îç∏ÏùÄ Ïù¥ ImageNet Îç∞Ïù¥ÌÑ∞ÏÖãÏúºÎ°ú ÌïôÏäµÎêòÏóàÍ∏∞ ÎïåÎ¨∏Ïóê, Î™®Îç∏Ïùò Ï∂úÎ†•ÏùÄ Ïù¥ 1000Í∞úÏùò ÌÅ¥ÎûòÏä§Ïóê ÎåÄÌïú ÏòàÏ∏° ÌôïÎ•†Î°ú ÎÇòÌÉÄÎÇ©ÎãàÎã§. 
`imagenet_classes`Îäî Ïù¥Îü¨Ìïú Ïà´Ïûê ÌòïÌÉúÏùò ÏòàÏ∏° Í≤∞Í≥ºÎ•º ÏÇ¨ÎûåÏù¥ Ïù¥Ìï¥Ìï† Ïàò ÏûàÎäî ÌÅ¥ÎûòÏä§ Ïù¥Î¶Ñ(Ïòà: "golden retriever", "poodle")ÏúºÎ°ú Îß§ÌïëÌï¥Ï£ºÎäî Ïó≠Ìï†ÏùÑ Ìï©ÎãàÎã§.

### imagenet_classes.json : imagenet_classes Ï†ïÎ≥¥Î•º Ï†ÄÏû•Ìïú json ÏûÖÎãàÎã§. 

torchvision ÏûêÏ≤¥ÏóêÎäî ImageNet ÌÅ¥ÎûòÏä§ Ïù¥Î¶Ñ Î™©Î°ùÏù¥ ÏßÅÏ†ë Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏßÄ ÏïäÍ∏∞Ïóê,  
Ìï¥Îãπ Ï†ïÎ≥¥Î•º Îã¥Í≥† ÏûàÎäî JSON ÌååÏùºÏùÑ Î≥ÑÎèÑÎ°ú Ï§ÄÎπÑÌï¥Ïïº Ìï©ÎãàÎã§. Îã§Ïùå Î∞©Î≤ïÏúºÎ°ú `imagenet_classes.json` ÌååÏùºÏùÑ ÏñªÏùÑ Ïàò ÏûàÏäµÎãàÎã§.

```python
import requests
import json

# URLÏóêÏÑú ÏßÅÏ†ë JSON ÌååÏùº ÏùΩÍ∏∞
url = "https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json"

response = requests.get(url)
response.raise_for_status()  # ÏöîÏ≤≠ Ïã§Ìå® Ïãú ÏóêÎü¨ Î∞úÏÉù

# JSON Îç∞Ïù¥ÌÑ∞ Î°úÎìú
imagenet_labels = response.json()


with open("imagenet_classes.json", "r") as f:
    imagenet_classes = json.load(f)
```

## 3. ÏΩîÎìú Î≥∏Í≤© ÏãúÏûë!!

```python
import torch
import torchvision.models as models
from torchvision import transforms
from PIL import Image
import json

# 1. ViT Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞ (ViT-Base, Ìå®Ïπò ÌÅ¨Í∏∞ 16 ÏÇ¨Ïö©)
vit_b_16 = models.vit_b_16(pretrained=True)
vit_b_16.eval()  # Ï∂îÎ°† Î™®ÎìúÎ°ú ÏÑ§Ï†ï

# 2. Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ï†ïÏùò
# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞Í∞Ä Îã§ Îã§Î•¥Îãà 256ÏúºÎ°ú Î¶¨ÏÇ¨Ïù¥Ï¶àÌïòÍ≥† 224Î°ú Ï§ëÏïô Î∂ÄÎ∂ÑÏùÑ Ìå®ÏπòÌï©ÎãàÎã§.
# Í∑∏Î¶¨Í≥† ImageNet Îç∞Ïù¥ÌÑ∞ÏÖãÏùò ÌèâÍ∑†Í≥º ÌëúÏ§ÄÌé∏Ï∞®Î°ú Ï†ïÍ∑úÌôîÌï©ÎãàÎã§.
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# 3. Í∞ïÏïÑÏßÄ Ïù¥ÎØ∏ÏßÄ Î∂àÎü¨Ïò§Í∏∞ (Î≥∏Ïù∏Ïùò Ïù¥ÎØ∏ÏßÄ ÌååÏùº Í≤ΩÎ°úÎ°ú Î≥ÄÍ≤ΩÌï¥Ï£ºÏÑ∏Ïöî)
image_path = "dog.jpg"
try:
    image = Image.open(image_path).convert('RGB')
    input_tensor = transform(image).unsqueeze(0) # Î∞∞Ïπò Ï∞®Ïõê Ï∂îÍ∞Ä
except FileNotFoundError:
    print(f"Error: Ïù¥ÎØ∏ÏßÄ ÌååÏùº '{image_path}'ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
    exit()

# 4. Î™®Îç∏Ïóê ÏûÖÎ†•ÌïòÏó¨ ÏòàÏ∏° ÏàòÌñâ
with torch.no_grad():
    output = vit_b_16(input_tensor)

# 5. ÏòàÏ∏° Í≤∞Í≥º ÌõÑÏ≤òÎ¶¨ Î∞è ÌÅ¥ÎûòÏä§ Ïù¥Î¶Ñ Ï∂úÎ†•
try:
    with open("imagenet_classes.json", "r") as f:
        imagenet_classes = json.load(f)

    _, predicted_idx = torch.sort(output, dim=1, descending=True)
    top_k = 5
    print(f"Top {top_k} ÏòàÏ∏° Í≤∞Í≥º:")
    for i in range(top_k):
        class_idx = predicted_idx[0, i].item()
        confidence = torch.softmax(output, dim=1)[0, class_idx].item()
        print(f"- {imagenet_classes[class_idx]}: {confidence:.4f}")

except FileNotFoundError:
    print("Error: 'imagenet_classes.json' ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. 2Îã®Í≥ÑÏóêÏÑú ÌååÏùºÏùÑ Ï§ÄÎπÑÌï¥Ï£ºÏÑ∏Ïöî.")
    print("ÏòàÏ∏°Îêú ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§:", predicted_idx[0, :5].tolist())
except Exception as e:
    print(f"Error during prediction processing: {e}")
```

ÏúÑ ÏΩîÎìúÎ•º Ïã§ÌñâÌïòÎ©¥!!!  
ÏïÑÎûòÏôÄ Í∞ôÏù¥  Top 5Í∞úÏùò ÏòàÏ∏°Í≤∞Í≥ºÎ•º Î≥ºÏàò ÏûàÎäîÎç∞Ïöî~!

```text
Top 5 ÏòàÏ∏° Í≤∞Í≥º:
- Golden Retriever: 0.9126
- Labrador Retriever: 0.0104
- Kuvasz: 0.0032
- Airedale Terrier: 0.0014
- tennis ball: 0.0012
```

Í≥®Îì†Î¶¨Ìä∏Î¶¨Î≤ÑÎ•º 91.26%Î°ú Í∞ÄÏû• ÎÜíÏùÄ ÌôïÎ•†Î°ú ÏòàÏ∏°Ìï®ÏùÑ Î≥ºÏàò ÏûàÏóàÏäµÎãàÎã§


## 4. Huggingface ÏóêÏÑú ÏßÅÏ†ë Î™®Îç∏ÏùÑ Î∞õÏïÑÏÑú Ïã§ÌñâÌïòÍ∏∞! + Î∂ÑÏÑù (Îçú Í∞ÑÎã®, but Ïª§Ïä§ÌÑ∞ÎßàÏù¥Ïßï Í∞ÄÎä•)

Ïù¥Î≤àÏóêÎäî ÏßÅÏ†ë [ÌóàÍπÖÌéòÏù¥Ïä§Ïùò ViT Î™®Îç∏](https://huggingface.co/google/vit-base-patch16-224)Î°úÎ∂ÄÌÑ∞ ÏßÅÏ†ë  
Î™®Îç∏ÏùÑ ÏûÑÌè¨Ìä∏ÌïòÏó¨ ÏßÑÌñâÌï¥Î≥¥Í≤†ÏäµÎãàÎã§~!  

```python
import torch
import torchvision.models as models
from torchvision import transforms
from PIL import Image
import json

# 1. ViT Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞ (ViT-Base, Ìå®Ïπò ÌÅ¨Í∏∞ 16 ÏÇ¨Ïö©)
vit_b_16 = models.vit_b_16(pretrained=True)
vit_b_16.eval()  # Ï∂îÎ°† Î™®ÎìúÎ°ú ÏÑ§Ï†ï

# 2. Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ï†ïÏùò
# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞Í∞Ä Îã§ Îã§Î•¥Îãà 256ÏúºÎ°ú Î¶¨ÏÇ¨Ïù¥Ï¶àÌïòÍ≥† 224Î°ú Ï§ëÏïô Î∂ÄÎ∂ÑÏùÑ Ìå®ÏπòÌï©ÎãàÎã§.
# Í∑∏Î¶¨Í≥† ImageNet Îç∞Ïù¥ÌÑ∞ÏÖãÏùò ÌèâÍ∑†Í≥º ÌëúÏ§ÄÌé∏Ï∞®Î°ú Ï†ïÍ∑úÌôîÌï©ÎãàÎã§.
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# 3. Í∞ïÏïÑÏßÄ Ïù¥ÎØ∏ÏßÄ Î∂àÎü¨Ïò§Í∏∞ (Î≥∏Ïù∏Ïùò Ïù¥ÎØ∏ÏßÄ ÌååÏùº Í≤ΩÎ°úÎ°ú Î≥ÄÍ≤ΩÌï¥Ï£ºÏÑ∏Ïöî)
image_path = "dog.jpg"
try:
    image = Image.open(image_path).convert('RGB')
    input_tensor = transform(image).unsqueeze(0) # Î∞∞Ïπò Ï∞®Ïõê Ï∂îÍ∞Ä
except FileNotFoundError:
    print(f"Error: Ïù¥ÎØ∏ÏßÄ ÌååÏùº '{image_path}'ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
    exit()

# 4. Î™®Îç∏Ïóê ÏûÖÎ†•ÌïòÏó¨ ÏòàÏ∏° ÏàòÌñâ
with torch.no_grad():
    output = vit_b_16(input_tensor)

# 5. ÏòàÏ∏° Í≤∞Í≥º ÌõÑÏ≤òÎ¶¨ Î∞è ÌÅ¥ÎûòÏä§ Ïù¥Î¶Ñ Ï∂úÎ†•
with open("imagenet_classes.json", "r") as f:
       imagenet_classes = json.load(f)

_, predicted_idx = torch.sort(output, dim=1, descending=True)
top_k = 5
print(f"Top {top_k} ÏòàÏ∏° Í≤∞Í≥º:")
for i in range(top_k):
       class_idx = predicted_idx[0, i].item()
       confidence = torch.softmax(output, dim=1)[0, class_idx].item()
       print(f"- {imagenet_classes[class_idx]}: {confidence:.4f}")


```

Ïó≠Ïãú ÎßàÏ∞¨Í∞ÄÏßÄÎ°ú~!! 207Î≤à, Í≥®Îì† Î¶¨Ìä∏Î¶¨Î≤ÑÎ°ú Íµ¨Î∂ÑÎêòÏóàÏäµÎãàÎã§!!!  
Í∑∏Îü∞Îç∞! Ïó¨Í∏∞ÏÑúÏùò Í∏∞Ï°¥ torchvisionÍ≥º Ï∞®Ïù¥ & Î™®Îç∏ Ïª§Ïä§ÌÑ∞ÎßàÏù¥Ïßï Îì±ÏùÑ ÏïåÏïÑÎ≥¥Í≤†ÏäµÎãàÎã§!!

### a. Ïù¥ÎØ∏ÏßÄÏùò Ï†ÑÏ≤òÎ¶¨Î∞©Ïãù!!

ÏïÑÎûòÏùò Ï†ÑÏ≤òÎ¶¨ Î∂ÄÎ∂ÑÏùÑ Î≥¥Î©¥, ViTFeatureExtractorÎäî Ìï¥Îãπ Î™®Îç∏Ïù¥ ÌïôÏäµÎê† Îïå ÏÇ¨Ïö©ÌñàÎçò Ï†ÑÏ≤òÎ¶¨ Î∞©ÏãùÏùÑ ÎØ∏Î¶¨ ÏïåÍ≥† ÏûàÏñ¥,  
Î≥µÏû°Ìïú transforms.Compose Í≥ºÏ†ïÏùÑ ÏßÅÏ†ë ÏûëÏÑ±ÌïòÏßÄ ÏïäÍ≥† Í∞ÑÎã®ÌïòÍ≤å Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨Î•º ÏàòÌñâÌï† Ïàò ÏûàÍ≤å Ìï¥Ï§ÄÎãµÎãàÎã§~!!

```python
feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')

# 3. Ï†ÑÏ≤òÎ¶¨ : ÏßÅÏ†ë crop Î∞è resize Ìï† ÌïÑÏöîÍ∞Ä ÏóÜÏñ¥Ïöî!
inputs = feature_extractor(images=image, return_tensors="pt")
```

### b. CLS ÌÜ†ÌÅ∞ Î≥¥Í∏∞!!

ÏßÄÎÇú Ïù¥Î°† ÌïôÏäµÍ∏ÄÏóêÏÑú 196Í∞úÏùò Ìå®Ïπò + 1Í∞úÏùò CLS ÌÜ†ÌÅ∞ÏúºÎ°ú 197Í∞úÏùò Ìå®ÏπòÎ°ú Íµ¨ÏÑ±Îê®ÏùÑ ÏïåÏïÑÎ≥¥ÏïòÎäîÎç∞Ïöî~!  
Ïù¥ Ï≤´Î≤àÏ®∞Ïùò CLS ÌÜ†ÌÅ∞Ïóê Ïù¥ÎØ∏ÏßÄÏùò Ï†ÑÏ≤¥Ï†ÅÏù∏ Ï†ïÎ≥¥Í∞Ä Ìè¨Ìï®Îê®ÏùÑ ÌôïÏù∏ÌñàÏóàÏäµÎãàÎã§!!  
ÏïÑÎûòÏôÄ Í∞ôÏùÄ ÏΩîÎìúÎ°ú CLS TokenÏùÑ Î≥º Ïàò ÏûàÏäµÎãàÎã§!!  


```python
from transformers import ViTModel, ViTImageProcessor
import torch
from PIL import Image

# 1. ViTModel (Classification head ÏóÜÎäî ÏàúÏàò Î™®Îç∏)
model = ViTModel.from_pretrained('google/vit-base-patch16-224')
model.eval()

# Feature Extractor ‚Üí ViTImageProcessorÎ°ú ÏµúÏã†Ìôî
processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')

# 2. Ïù¥ÎØ∏ÏßÄ Î∂àÎü¨Ïò§Í∏∞
image = Image.open("dog.jpg").convert('RGB')
inputs = processor(images=image, return_tensors="pt")

# 3. Î™®Îç∏ Ï∂îÎ°†
with torch.no_grad():
    outputs = model(**inputs)

# 4. CLS ÌÜ†ÌÅ∞ Ï∂îÏ∂ú
last_hidden_state = outputs.last_hidden_state  # (batch_size, num_tokens, hidden_dim)
cls_token = last_hidden_state[:, 0, :]  # 0Î≤àÏß∏ ÌÜ†ÌÅ∞Ïù¥ CLS

# 5. CLS ÌÜ†ÌÅ∞ Ï∂úÎ†•
print("CLS token shape:", cls_token.shape)  # torch.Size([1, 768])
print("CLS token values (Ïïû 5Í∞ú):", cls_token[0, :5])
```

ÏúÑ ÏΩîÎìúÎ•º Ïã§ÌñâÌï¥Î≥¥Î©¥, ÏòàÏÉÅÌïúÎåÄÎ°ú 768 Ï∞®ÏõêÏùòCLS ÌÜ†ÌÅ∞ÏùÑ Î≥ºÏàò ÏûàÏßÄÏöî~~  
Ïù¥ÌõÑ Ïó¨Îü¨ Ïó∞Íµ¨Îì§ÏùÄ Ïù¥ ÌÜ†ÌÅ∞ÏùÑ ÌôúÏö©Ìï¥ÏÑú Îã§Î•∏ Ï†ïÎ≥¥Î°ú ÌôúÏö©ÌïòÍ∏∞ÎèÑÌï©ÎãàÎã§!   

```text 
CLS token shape: torch.Size([1, 768])
CLS token values (Ïïû 5Í∞ú): tensor([-0.5934, -0.3203, -0.0811,  0.3146, -0.7365])
```

### c. ViTÏùò CAM!! Attention Rollout  

Í∏∞Ï°¥ CNN Î∞©ÏãùÏùò Ïù¥ÎØ∏ÏßÄ Î∂ÑÎ•òÎäî Î™®Îç∏Ïùò ÎßàÏßÄÎßâÎã®Ïóê CAM(Class Activation Map)ÏùÑ ÎëêÏñ¥ÏÑú Ïñ¥Îñ§ Î∂ÄÎ∂ÑÏù¥ Ï§ëÏöîÌïòÍ≤å ÎêòÏóàÎäîÏßÄ ÏãúÍ∞ÅÌôî Ìï†Ïàò ÏûàÏóàÏäµÎãàÎã§!!!  

[CAMÏùò Ïù¥Î°† Ï†ïÎ¶¨!!](https://drfirstlee.github.io/posts/CAM_research/)  
[CAM Ïã§Ïäµ!!](https://drfirstlee.github.io/posts/CAM_usage/) 

Ïö∞Î¶¨Ïùò ViT Î™®Îç∏ÏùÄ CAMÍ≥ºÎäî Îã§Î•¥Í∏∞Ïóê ÎèôÏùºÌïú Î∞©ÏãùÏúºÎ°ú ÏßÑÌñâÏùÄ Ïñ¥Î†µÏßÄÎßå!!  
**Attention Rollout** Ïù¥ÎùºÎäî Î∞©ÏãùÏúºÎ°ú Í∞ÄÏû• Ï§ëÏöîÌïú CLS Ìå®ÌÇ§ÏπòÍ∞Ä ÎÇòÎ®∏ÏßÄ 196Í∞ú Ìå®ÏπòÏ§ë Ïñ¥ÎîîÎ•º Ï§ëÏöîÌïòÍ≤å Î¥§ÎäîÏßÄ!! ÏãúÍ∞ÅÌôîÌï†Ïàò ÏûàÏñ¥Ïöî!!  

Íµ¨Ï°∞Î•º Î≥¥ÏûêÎ©¥!!  

ÏïÑÎûòÏôÄ Í∞ôÏù¥  [CLS]Í∞Ä Í∞Å Ìå®ÏπòÏóê ÎåÄÌï¥ "ÎÑà Ï§ëÏöîÌï¥", "ÎÑà Î≥ÑÎ°úÏïº" Í∞ôÏùÄ ÏãùÏúºÎ°ú Í∞ÄÏ§ëÏπòÎ•º Î∂ÄÏó¨ÌïòÎäî Í±∏ AttentionÏù¥ÎùºÍ≥†ÌïòÍ≥†, Í∑∏ Ïñ¥ÌÖêÏÖòÎì§ÏùÑ ÏãúÍ∞ÅÌôîÌïòÎäîÍ≤ÉÏù¥ÏßÄÏöî!

```text
[CLS]   ‚Üí Patch_1   (Attention weight: 0.05)
[CLS]   ‚Üí Patch_2   (Attention weight: 0.02)
[CLS]   ‚Üí Patch_3   (Attention weight: 0.01)
...
[CLS]   ‚Üí Patch_196 (Attention weight: 0.03)
```

Í≤∞Íµ≠!! Ïñ¥Îñ§ Ìå®ÏπòÍ∞Ä Ï§ëÏöîÌïòÍ≤å Í∞ÑÏ£ºÎêòÏóàÎäîÏßÄ ÏïÑÎûòÏôÄ Í∞ôÏù¥ ÏãúÍ∞ÅÌôîÍ∞Ä ÎêòÏßÄÏöî~!!

- Îπ®Í∞õÍ≤å Î≥¥Ïù¥Îäî ÏòÅÏó≠ ‚Üí [CLS]Í∞Ä ÎßéÏù¥ Ï£ºÎ™©Ìïú Ìå®Ïπò,  
- ÌååÎûóÍ≤å Î≥¥Ïù¥Îäî ÏòÅÏó≠ ‚Üí [CLS]Í∞Ä Îçú Ï£ºÎ™©Ìïú Ìå®Ïπò

ÏΩîÎìúÎ°ú Î≥¥Î©¥~~

```python
from transformers import ViTModel, ViTFeatureExtractor
import torch
from PIL import Image
import requests
import matplotlib.pyplot as plt
import numpy as np

# 1. Î™®Îç∏Í≥º Feature Extractor Î∂àÎü¨Ïò§Í∏∞
model = ViTModel.from_pretrained('google/vit-base-patch16-224', output_attentions=True)
model.eval()

feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')

# 2. Ïù¥ÎØ∏ÏßÄ Î∂àÎü¨Ïò§Í∏∞
image = Image.open("dog.jpg").convert('RGB')
inputs = feature_extractor(images=image, return_tensors="pt")

# 3. Î™®Îç∏ Ï∂îÎ°† (attention Ï∂úÎ†•)
with torch.no_grad():
    outputs = model(**inputs)
    attentions = outputs.attentions  # list of (batch, heads, tokens, tokens)

# 4. Attention Rollout Í≥ÑÏÇ∞
def compute_rollout(attentions):
    # Multiply attention matrices across layers
    result = torch.eye(attentions[0].size(-1))
    for attention in attentions:
        attention_heads_fused = attention.mean(dim=1)[0]  # (tokens, tokens)
        attention_heads_fused += torch.eye(attention_heads_fused.size(-1))
        attention_heads_fused /= attention_heads_fused.sum(dim=-1, keepdim=True)
        result = torch.matmul(result, attention_heads_fused)
    return result

rollout = compute_rollout(attentions)

# 5. [CLS] ÌÜ†ÌÅ∞ÏóêÏÑú Ïù¥ÎØ∏ÏßÄ Ìå®ÏπòÎ°ú Í∞ÄÎäî Attention Ï∂îÏ∂ú
mask = rollout[0, 1:].reshape(14, 14).detach().cpu().numpy()

# 6. ÏãúÍ∞ÅÌôî
def show_mask_on_image(img, mask):
    img = img.resize((224, 224))
    mask = (mask - mask.min()) / (mask.max() - mask.min())
    fig, ax = plt.subplots()
    ax.imshow(img)
    ax.imshow(mask, cmap='jet', alpha=0.5)
    ax.axis('off')
    plt.show()

show_mask_on_image(image, mask)

```

Ïù¥Í≥† Í∑∏ Í≤∞Í≥ºÎäî!!!??

![patch](https://github.com/user-attachments/assets/82e9e668-d62a-4b06-9464-75e4eb3f967b)

ÏûÖÎãàÎã§~! ÎßûÎäîÍ≤É Í∞ôÎÇòÏöî~?

---

## 5. üí° Í≤∞Î°† : Í∞ÑÎã®ÌïòÍ≥† Îπ†Î•∏ ViT

Ïñ¥Îñ§Í∞ÄÏöî? ÏΩîÎìúÎ•º ÏßÅÏ†ë Ïã§ÌñâÌï¥Î≥¥ÏïòÎäîÎç∞~!!  
ÌÅ∞ Ïñ¥Î†§ÏõÄÏóÜÏù¥, Í∑∏Î¶¨Í≥† Îπ†Î•¥Í≤å ÏΩîÎìúÎ•º Ïã§ÌñâÌï†Ïàò ÏûàÏóàÏßÄÏöî!?

Ïù¥Ï≤òÎüº Ïù¥Î°†Ï†ÅÏúºÎ°úÎèÑ Ïú†ÏùòÎØ∏ÌñàÎçò ViT! 
ÎåÄÍ∑úÎ™® Îç∞Ïù¥ÌÑ∞ÏÖãÏóêÏÑú ÌïôÏäµÎêú Î™®Îç∏Ïù¥ ÏΩîÎìúÎ°úÎèÑ ÏâΩÍ≤å Íµ¨ÌòÑÏù¥ Í∞ÄÎä•Ìï¥ÏÑú Ïù¥ÌõÑÎ°ú Ïª¥Ìì®ÌÑ∞ ÎπÑÏ†Ñ Î∂ÑÏïºÏóêÏÑú Transformer Í∏∞Î∞ò Ïó∞Íµ¨Í∞Ä Ìè≠Î∞úÏ†ÅÏúºÎ°ú Ï¶ùÍ∞ÄÌïòÍ≤å ÎêòÏóàÎã§Í≥†Ìï©ÎãàÎã§!!  

ÏïûÏúºÎ°ú DINO, DeiT, CLIP, Swin Transformer Îì± Îã§ÏñëÌïú ÎπÑÏ†Ñ Transformer Í∏∞Î∞òÏùò Î™®Îç∏ÎèÑ ÏïåÏïÑÎ≥¥Î©∞ Ïã§ÏäµÌï¥Î≥º Ïàò ÏûàÎèÑÎ°ù ÌïòÍ≤†ÏäµÎãàÎã§~! ^^

Í∞êÏÇ¨Ìï©ÎãàÎã§!!! üöÄüî•
