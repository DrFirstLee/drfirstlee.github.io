---
layout: post
title: "ğŸ”¤Understanding Tokenizers - Tokenizer ì•Œì•„ë³´ê¸°?!!"
author: [DrFirst]
date: 2025-07-01 07:00:00 +0900
categories: [AI, Experiment]
tags: [Tokenizer, NLP, BPE, WordPiece, SentencePiece, Subword]
sitemap :
  changefreq : monthly
  priority : 0.8
---

---

### ğŸ§  (í•œêµ­ì–´) Tokenizer ì•Œì•„ë³´ê¸°?!!  
_ğŸ” í…ìŠ¤íŠ¸ë¥¼ AIê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°!!!_

> ìš°ë¦¬ê°€ ë¬¸ì¥ì„ ë‹¨ì–´ë¡œ ë‚˜ëˆ„ì–´ ì´í•´í•˜ë“¯ì´,  
> AI ëª¨ë¸ë„ **í† í¬ë‚˜ì´ì €**ë¥¼ í†µí•´ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬ ê°€ëŠ¥í•œ ë‹¨ìœ„ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤!

> ì£¼ìš” ë…¼ë¬¸ë“¤: 
> - [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909) (BPE - 2016)
> - [SentencePiece: A simple and language independent subword tokenizer](https://arxiv.org/abs/1808.06226) (SentencePiece - 2018)

---

### ğŸ’¡ Tokenizerì˜ íŠ¹ì§• ìš”ì•½!!

1. **í…ìŠ¤íŠ¸ì™€ AI ì‚¬ì´ì˜ ë‹¤ë¦¬ ì—­í• **  
   - ì¸ê°„ì´ ì½ëŠ” í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìˆ«ì í† í°ìœ¼ë¡œ ë³€í™˜
2. **ì–´íœ˜ ë¬¸ì œ í•´ê²°**  
   - ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì œì´ì…˜ìœ¼ë¡œ ë¯¸ë“±ë¡ ë‹¨ì–´(OOV) ë¬¸ì œ í•´ê²°
3. **ì–¸ì–´ ë¬´ê´€ì„±**  
   - í˜„ëŒ€ í† í¬ë‚˜ì´ì €ëŠ” ë‹¤ì–‘í•œ ì–¸ì–´ì™€ ë¬¸ì ì²´ê³„ì—ì„œ ì‘ë™

---

### ğŸ§  Tokenizer ë“±ì¥ì˜ ë°°ê²½

> ì‚¬ëŒì€ ë‹¨ì–´ë¥¼ ì½ì§€ë§Œ, ì»´í“¨í„°ëŠ” ìˆ«ìê°€ í•„ìš”í•´!  
> í† í¬ë‚˜ì´ì €ëŠ” í…ìŠ¤íŠ¸ë¥¼ AIê°€ ì†Œí™”í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” í•„ìˆ˜ ë‹¤ë¦¬!!  

**í…ìŠ¤íŠ¸ ì²˜ë¦¬ì˜ ì§„í™”:**
- **1990ë…„ëŒ€**: ë‹¨ì–´ ìˆ˜ì¤€ í† í¬ë‚˜ì´ì œì´ì…˜ â†’ ê°„ë‹¨í•˜ì§€ë§Œ ê±°ëŒ€í•œ ì–´íœ˜
- **2000ë…„ëŒ€**: ë¬¸ì ìˆ˜ì¤€ â†’ ì‘ì€ ì–´íœ˜ì´ì§€ë§Œ ì˜ë¯¸ ìƒì‹¤
- **2010ë…„ëŒ€**: Sub-word í† í¬ë‚˜ì´ì œì´ì…˜ â†’ **ë‘ ì„¸ê³„ì˜ ì¥ì ì„ ìœµí•©!**

#### ğŸš¨ **ê¸°ì¡´ ë°©ë²•ë“¤ì˜ í•œê³„ì **

#### **1ï¸âƒ£ ë‹¨ì–´ ìˆ˜ì¤€ í† í¬ë‚˜ì´ì œì´ì…˜ì˜ ë¬¸ì œ** ğŸ“š
- **ì–´íœ˜ í­ë°œ**: "ë‹¬ë¦¬ë‹¤", "ë‹¬ë¦¬ê³ ", "ë‹¬ë ¸ë‹¤" â†’ ê°ê° ë‹¤ë¥¸ í† í°
- **OOV ë¬¸ì œ**: í•™ìŠµ ì–´íœ˜ì— ì—†ëŠ” ìƒˆë¡œìš´ ë‹¨ì–´ = `<UNK>`
- **ë©”ëª¨ë¦¬ ì§‘ì•½ì **: ì–´íœ˜ê°€ 10ë§Œ ê°œ ì´ìƒ ë„ë‹¬ ê°€ëŠ¥
- **ì–¸ì–´ ì˜ì¡´ì„±**: ê° ì–¸ì–´ë³„ë¡œ ë‹¤ë¥¸ ê·œì¹™ í•„ìš”

**ì˜ˆì‹œ:**
```
ì…ë ¥: "ë‹¬ë¦¬ëŠ” ì‚¬ëŒì´ ë¹ ë¥´ê²Œ ë‹¬ë¦°ë‹¤"
ë‹¨ì–´ í† í°: ["ë‹¬ë¦¬ëŠ”", "ì‚¬ëŒì´", "ë¹ ë¥´ê²Œ", "ë‹¬ë¦°ë‹¤"]
ë¬¸ì œ: "ë‹¬ë ¤ê°€ëŠ”" â†’ `<UNK>` (ê¸°ì¡´ ì–´íœ˜ì— ì—†ê¸°ì—!)
```

#### **2ï¸âƒ£ ë¬¸ì ìˆ˜ì¤€ì˜ í•œê³„** ğŸ”¤
- **ì˜ë¯¸ ìƒì‹¤**: "ê³ ì–‘ì´" â†’ ["ê³ ", "ì–‘", "ì´"] ë‹¨ì–´ ì˜ë¯¸ ì†ì‹¤
- **ê¸´ ì‹œí€€ìŠ¤**: ë” ê¸´ ì‹œí€€ìŠ¤ = ë” ë§ì€ ì—°ì‚°
- **ë§¥ë½ ì–´ë ¤ì›€**: ëª¨ë¸ì´ ë‹¨ì–´ ìˆ˜ì¤€ íŒ¨í„´ í•™ìŠµí•˜ê¸° ì–´ë ¤ì›€

**ì˜ˆì‹œ:**
```
ì…ë ¥: "ì•ˆë…• ì„¸ìƒ"
ë¬¸ì í† í°: ["ì•ˆ", "ë…•", " ", "ì„¸", "ìƒ"]
ë¬¸ì œ: 2ê°œ ë‹¨ì–´ì— 5ê°œ í† í°!
```

> ğŸ’¡ **í•´ê²°ì±…**: **Sub Word í† í¬ë‚˜ì´ì œì´ì…˜**ì´ ì–´íœ˜ í¬ê¸°ì™€ ì˜ë¯¸ì  ì˜ë¯¸ì˜ ê· í˜•ì„ ë§ì¶¥ë‹ˆë‹¤!

---

### ğŸ”§ í˜„ëŒ€ í† í¬ë‚˜ì´ì œì´ì…˜ ì•Œê³ ë¦¬ì¦˜

#### **ğŸ—ï¸ 1. BPE (Byte Pair Encoding)** ğŸ§©

**í•µì‹¬ ì•„ì´ë””ì–´**: ë¬¸ìì—ì„œ ì‹œì‘í•´ì„œ ê°€ì¥ ë¹ˆë²ˆí•œ ìŒì„ ë°˜ë³µì ìœ¼ë¡œ ë³‘í•©

**ì•Œê³ ë¦¬ì¦˜ ë‹¨ê³„:**
1. **ì´ˆê¸°í™”**: í…ìŠ¤íŠ¸ë¥¼ ë¬¸ìë¡œ ë¶„í•  (ë¬¸ììˆ˜ì¤€ í† í¬ë‚˜ì´ì œì´ì…˜)
2. **ìŒ ê³„ìˆ˜**: ê°€ì¥ ë¹ˆë²ˆí•œ ì¸ì ‘ ë¬¸ì ìŒ ì°¾ê¸°
3. **ë³‘í•©**: ê°€ì¥ ë¹ˆë²ˆí•œ ìŒì„ ìƒˆ í† í°ìœ¼ë¡œ êµì²´
4. **ë°˜ë³µ**: ì›í•˜ëŠ” ì–´íœ˜ í¬ê¸°ê¹Œì§€

**ì˜ˆì‹œ ê³¼ì •:**  
> ì œì¼ ê¸°ë³¸ì´ë˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì¤‘ìš”í•˜ë‹ˆ, ì§ì ‘ ì˜ˆì‹œë¥¼ ë”°ë¼í•´ë³´ì•˜ì–´ìš”!  

```python
# ê°„ë‹¨í•œ BPE(Byte Pair Encoding) ë¯¸ë‹ˆ ì‹¤ìŠµ ì½”ë“œ
from collections import Counter, defaultdict
import pandas as pd

# ìƒ˜í”Œ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
corpus = ["low", "lower", "newest", "widest"]

# Step 1: ê° ë‹¨ì–´ë¥¼ ë¬¸ì ë‹¨ìœ„ë¡œ ë¶„í•´í•˜ê³  ëì— íŠ¹ìˆ˜ë¬¸ì ì¶”ê°€
def split_word(word):
    return list(word) + ["</w>"]  # ë‹¨ì–´ì˜ ë í‘œì‹œ

# ì´ˆê¸° vocabulary
tokens = [split_word(word) for word in corpus]

# helper í•¨ìˆ˜: í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ í•©ì¹¨ (ê³µë°± ë‹¨ìœ„ë¡œ)
def get_vocab(tokens):
    vocab = Counter()
    for token in tokens:
        vocab[" ".join(token)] += 1
    return vocab

# ê°€ì¥ ìì£¼ ë“±ì¥í•˜ëŠ” pair ì°¾ê¸°
def get_most_common_pair(vocab):
    pairs = defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols) - 1):
            pair = (symbols[i], symbols[i + 1])
            pairs[pair] += freq
    return max(pairs, key=pairs.get), pairs

# ë³‘í•© ìˆ˜í–‰
def merge_pair(pair, tokens):
    new_tokens = []
    bigram = " ".join(pair)
    replacement = "".join(pair)
    for token in tokens:
        token_str = " ".join(token)
        token_str = token_str.replace(bigram, replacement)
        new_tokens.append(token_str.split())
    return new_tokens

# BPE í•™ìŠµ ìˆ˜í–‰ (5íšŒë§Œ ë°˜ë³µ)
merge_history = []
for i in range(5):
    vocab = get_vocab(tokens)
    pair, all_pairs = get_most_common_pair(vocab)
    merge_history.append((pair, all_pairs[pair]))
    tokens = merge_pair(pair, tokens)

# ê²°ê³¼ ì •ë¦¬
merge_df = pd.DataFrame(merge_history, columns=["Merged Pair", "Frequency"])
tokens_final = [" ".join(token) for token in tokens]
print("Final tokens:", tokens_final)
# Output: ['low </w>', 'lower </w>', 'newest </w>', 'widest </w>']

# ë‹¨ê³„ë³„ ë³‘í•© ê³¼ì • ë³´ê¸°
print("\në‹¨ê³„ë³„ ë³‘í•© ê³¼ì •:")
for i, (pair, freq) in enumerate(merge_history):
    print(f"ë‹¨ê³„ {i+1}: {pair[0]} + {pair[1]} â†’ {pair[0]+pair[1]} (ë¹ˆë„: {freq})")
# Output:
# ë‹¨ê³„ 1: e + s â†’ es (ë¹ˆë„: 2)
# ë‹¨ê³„ 2: es + t â†’ est (ë¹ˆë„: 2)  
# ë‹¨ê³„ 3: l + o â†’ lo (ë¹ˆë„: 2)
# ë‹¨ê³„ 4: lo + w â†’ low (ë¹ˆë„: 2)
# ë‹¨ê³„ 5: n + e â†’ ne (ë¹ˆë„: 1)
```

**BPE ì¥ì :**
- âœ… **í¬ê·€ ë‹¨ì–´ ì²˜ë¦¬** ì„œë¸Œì›Œë“œ ë¶„í•´ë¥¼ í†µí•´
- âœ… **ì¼ê´€ëœ í† í¬ë‚˜ì´ì œì´ì…˜** ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì— ëŒ€í•´
- âœ… **ì–¸ì–´ ë¬´ê´€** ì•Œê³ ë¦¬ì¦˜

#### **ğŸ—ï¸ 2. WordPiece** ğŸ§©

**í•µì‹¬ ì•„ì´ë””ì–´**: BPEì™€ ìœ ì‚¬í•˜ì§€ë§Œ ë³‘í•© ê¸°ì¤€ì— **ìš°ë„(Likelihood) ìµœëŒ€í™”** ì‚¬ìš©  
- WordPieceëŠ” BPEì™€ ë‹¬ë¦¬ ë‹¨ìˆœíˆ ê°€ì¥ ìì£¼ ë“±ì¥í•˜ëŠ” ìŒì„ ë³‘í•©í•˜ì§€ ì•Šê³ , ë³‘í•©í–ˆì„ ë•Œ ì „ì²´ ë§ë­‰ì¹˜ì˜ ìš°ë„ë¥¼ ì–¼ë§ˆë‚˜ ì˜¬ë¦´ ìˆ˜ ìˆëŠ”ì§€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²°ì •  
- ìš°ë„ê°€ë­ì§€? ì–´ë ¤ìš°ë‹ˆ ì˜ˆì‹œë¡œ ë³´ìŸˆ!!  
    - `"un"`ì´ë¼ëŠ” ì„œë¸Œì›Œë“œë¥¼ ë³‘í•©í•˜ë©´:
    - `unhappiness`
    - `unhappy`
    - `unusual`
    - `unfit`
    - `unseen`  

    - ì¦‰, `"un"`ì´ë¼ëŠ” subwordê°€ **ì—¬ëŸ¬ ë‹¨ì–´ì—ì„œ ì˜ë¯¸ ìˆëŠ” ë°˜ë³µ íŒ¨í„´**ì´ ëœë‹¤ë©´  
    â†’ ë³‘í•©ì„ í†µí•´ **ì „ì²´ ë¬¸ì¥ì—ì„œ ëª¨ë¸ ì˜ˆì¸¡ ì„±ëŠ¥ í–¥ìƒ**


#### **ğŸ—ï¸ 3. SentencePiece** ğŸŒ

**í•µì‹¬ ì•„ì´ë””ì–´**: ì‚¬ì „ í† í¬ë‚˜ì´ì œì´ì…˜ ì—†ëŠ” **ì–¸ì–´ ë…ë¦½ì ** í† í¬ë‚˜ì´ì œì´ì…˜  
- ê¸°ì¡´ BPEëŠ” ë¨¼ì¬ ê³µë°±ìœ¼ë¡œ ìë¥´ê³ , ê·¸ ì•ˆì—ì„œ ìª¼ê°œê¸°ì— ë„ì–´ì“°ê¸°ê°€ ì—†ìœ¼ë©´? ë¬¸ì œê°€ë¨!  
- ê·¸ë˜ì„œ! ê³µë°±ë„ ë¬¸ìì²˜ëŸ¼ ë³´ê³ , ëª¨ë“  ê¸€ìë¥¼ ìª¼ê°œì„œ ì–¸ì–´ì™€ ìƒê´€ì—†ì´ í•™ìŠµ  
- ê³µë°±ì„ `â–`ë¡œ ì¹˜í™˜!


**ì£¼ìš” ì¥ì :**
- âœ… **ì‚¬ì „ í† í¬ë‚˜ì´ì œì´ì…˜ ë¶ˆí•„ìš”** (ë‹¨ì–´ ê²½ê³„ ì—†ìŒ)
- âœ… **ëª¨ë“  ì–¸ì–´ ì²˜ë¦¬** ì¤‘êµ­ì–´, ì¼ë³¸ì–´, ì•„ëì–´ í¬í•¨
- âœ… **ê°€ì—­ì **: ì›ë³¸ í…ìŠ¤íŠ¸ ì™„ë²½ ì¬êµ¬ì„± ê°€ëŠ¥
- âœ… **T5, mT5, ALBERT**ì—ì„œ ì‚¬ìš©

**SentencePiece íŠ¹ì§•:**
```python
# ê³µë°±ì„ ì¼ë°˜ ë¬¸ìë¡œ ì²˜ë¦¬
ì…ë ¥: "ì•ˆë…• ì„¸ìƒ"
í† í°: ["â–ì•ˆë…•", "â–ì„¸ìƒ"]  # â–ëŠ” ê³µë°±ì„ ë‚˜íƒ€ëƒ„

# ê³µë°±ì´ ì—†ëŠ” ì–¸ì–´ ì²˜ë¦¬
ì…ë ¥: "ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ"  # ì¼ë³¸ì–´: "ì•ˆë…• ì„¸ìƒ"
í† í°: ["ã“ã‚“ã«", "ã¡ã¯", "ä¸–ç•Œ"]
```

---

### ğŸ“Š **í† í¬ë‚˜ì´ì € ë¹„êµ**

| ë°©ë²• | ì–´íœ˜ í¬ê¸° | OOV ì²˜ë¦¬ | ì–¸ì–´ ì§€ì› | ì‚¬ìš© ëª¨ë¸ |
|------|-----------|----------|-----------|-----------|
| **ë‹¨ì–´ ìˆ˜ì¤€** | 5ë§Œ-10ë§Œ+ | âŒ ë‚˜ì¨ | ğŸ”¤ ì–¸ì–´ë³„ íŠ¹í™” | ì „í†µì  NLP |
| **ë¬¸ì ìˆ˜ì¤€** | 100-1ì²œ | âœ… ì™„ë²½ | ğŸŒ ë²”ìš© | ì´ˆê¸° NMT |
| **BPE** | 3ë§Œ-5ë§Œ | âœ… ì¢‹ìŒ | ğŸŒ ë²”ìš© | GPT, RoBERTa |
| **WordPiece** | 3ë§Œ-5ë§Œ | âœ… ì¢‹ìŒ | ğŸŒ ë²”ìš© | BERT, DistilBERT |
| **SentencePiece** | 3ë§Œ-5ë§Œ | âœ… í›Œë¥­í•¨ | ğŸŒ ë²”ìš© | T5, mT5, ALBERT |

---

### ğŸ’» **ì‹¤ì œ êµ¬í˜„**

#### **ğŸ”§ Hugging Face Tokenizers ì‚¬ìš©**
> ëª¨ë¸ë“¤ì˜ í† í¬ë‚˜ì´ì ¸ë¥¼ ë‹¤ìš´ë°›ì„ìˆ˜ ìˆìŠµë‹ˆë‹¤!  

```python
from transformers import AutoTokenizer

# BPE (GPT-2)
bpe_tokenizer = AutoTokenizer.from_pretrained("gpt2")
text = "The runner runs quickly and efficiently"
bpe_tokens = bpe_tokenizer.tokenize(text)
print(f"BPE: {bpe_tokens}")
# Output: ['The', 'Ä runner', 'Ä runs', 'Ä quickly', 'Ä and', 'Ä efficiently']

# WordPiece (BERT)
wp_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
wp_tokens = wp_tokenizer.tokenize(text)
print(f"WordPiece: {wp_tokens}")
# Output: ['the', 'runner', 'runs', 'quickly', 'and', 'efficiently']

# SentencePiece (T5)
sp_tokenizer = AutoTokenizer.from_pretrained("t5-small")
sp_tokens = sp_tokenizer.tokenize(text)
print(f"SentencePiece: {sp_tokens}")
# Output: ['â–The', 'â–runner', 'â–runs', 'â–quickly', 'â–and', 'â–efficiently']
```

#### **ğŸ”§ ì»¤ìŠ¤í…€ BPE í† í¬ë‚˜ì´ì € í›ˆë ¨**

- ì•„ë˜ ë°©ì‹ì€ ì´ˆê¸°í™”ëœ tokenizerë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ”ê²ƒ!!  
- ì œì‹œëœ `english_texts` ë¥¼ ë°”íƒ•ìœ¼ë¡œ í† í¬ë‚˜ì´ì ¸ ë§Œë“¬!!  

> ì•„ë˜ ì˜ˆì‹œëŠ” ìµœëŒ€ í† í°ìˆ˜ëŠ” 5000ê°œ, 2ë²ˆ ì´ìƒ ë“±ì¥í•´ì•¼ í•¨!  
```python

from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

# English text data examples
english_texts = [
    "Hello world, how are you today?",
    "The weather is beautiful and sunny", 
    "We are building a custom tokenizer for English",
    "Natural language processing is a fascinating field",
    "Deep learning and machine learning are powerful technologies"
]

# Save texts to file
with open("english_corpus.txt", "w", encoding="utf-8") as f:
    for text in english_texts:
        f.write(text + "\n")

# Initialize BPE tokenizer
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
tokenizer.pre_tokenizer = Whitespace()

# Configure trainer for English
trainer = BpeTrainer(
    vocab_size=5000,  # Larger vocab for English
    min_frequency=2,  # Minimum frequency threshold
    special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"],
    show_progress=True
)

# Train on English text
tokenizer.train(["english_corpus.txt"], trainer)

# Test the tokenizer
test_text = "Natural language processing is fascinating!"
tokens = tokenizer.encode(test_text)
print(f"Input: {test_text}")
print(f"Token IDs: {tokens.ids}")
print(f"Tokens: {tokens.tokens}")

# Save tokenizer
tokenizer.save("my_english_bpe_tokenizer.json")
print("English BPE tokenizer saved successfully!")
```

í”„ë¦°íŠ¸ë˜ëŠ” ê²°ê³¼ê°’ì€!?  
> ëŠë‚Œí‘œëŠ” ì²˜ìŒë³´ëŠ”ê²ƒì´ê¸°ì— `[UNK]` ê°€ ëŒ„ë‹¤~!  

```text
Input: Natural language processing is fascinating
Token IDs: [10, 39, 31, 28, 13, 23, 23, 38, 19, 31, 13, 19, 17, 27, 28, 26, 15, 46, 29, 37, 41, 18, 13, 29, 15, 35, 39, 37]
Tokens: ['N', 'at', 'u', 'r', 'a', 'l', 'l', 'an', 'g', 'u', 'a', 'g', 'e', 'p', 'r', 'o', 'c', 'es', 's', 'ing', 'is', 'f', 'a', 's', 'c', 'in', 'at', 'ing', 'w', 'e', 'at', 'h', 'er', '[UNK]']
English BPE tokenizer saved successfully!
```
---

### ğŸ§© **ê³ ê¸‰ í† í¬ë‚˜ì´ì œì´ì…˜ ê°œë…**

#### **1ï¸âƒ£ Special Tokens** ğŸ¯
```python

# Example usage with BERT tokenizer
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
text = "The quick brown foxã…"

encoded = tokenizer(text, 
                   add_special_tokens=True,
                   max_length=10,
                   padding="max_length",
                   truncation=True)

print(f"Input: {text}")
print(f"Token IDs: {encoded['input_ids']}")
print(f"Tokens: {tokenizer.convert_ids_to_tokens(encoded['input_ids'])}")

# Expected output:
# Input: The quick brown fox
# Token IDs: [101, 1996, 4248, 2829, 4419, 102, 0, 0, 0, 0]
# Tokens: ['[CLS]', 'the', 'quick', 'brown', '[UNK]', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']
```

- ê²°ê³¼ ì„¤ëª…!!  
> max_length = 10 ì´ì—ˆê¸°ì—, ê¸¸ì´ë¥¼ ë§ì¶”ê³ ì PADê°€ 3ê°œ ë‚˜ì˜´!
> `ã…` ëŠ” í•œê¸€ë¡œ ëª¨ë¥´ê¸°ì—  `[UNK]`

| í† í°       | ì„¤ëª…                                                |
|------------|-----------------------------------------------------|
| `[PAD]`    | ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ íŒ¨ë”© í† í° (ì‹œí€€ìŠ¤ ê¸¸ì´ ë§ì¶¤)       |
| `[UNK]`    | ë¯¸ì§€ì˜ ë‹¨ì–´(ì–´íœ˜ì§‘ì— ì—†ëŠ” OOV ë‹¨ì–´)ë¥¼ ìœ„í•œ í† í°     |
| `[CLS]`    | ë¬¸ì¥ ë¶„ë¥˜ìš© ì‹œì‘ í† í° (BERTì—ì„œ ì‚¬ìš©)               |
| `[SEP]`    | ë¬¸ì¥ êµ¬ë¶„ í† í° (ë¬¸ì¥ ê°„ êµ¬ë¶„ ë˜ëŠ” ë¬¸ì¥ ë í‘œì‹œ)     |
| `[MASK]`   | ë§ˆìŠ¤í‚¹ëœ í† í° (Masked Language Modelingìš©)         |
| `<s>`      | ì‹œí€€ìŠ¤ ì‹œì‘ í† í° (GPT ë“±ì—ì„œ ì‚¬ìš©)                  |
| `</s>`     | ì‹œí€€ìŠ¤ ì¢…ë£Œ í† í° (GPT ë“±ì—ì„œ ì‚¬ìš©)                  |


#### **2ï¸âƒ£ Tokenization vs Encoding** ğŸ”„
```python
from transformers import AutoTokenizer

# ì‚¬ì „ í•™ìŠµëœ BERT tokenizer ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# ì…ë ¥ ë¬¸ì¥
text = "Hello, world!"

# 1. Tokenization: ë¬¸ì¥ì„ í† í° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
tokens = tokenizer.tokenize(text)
print(f"Tokens: {tokens}")
# ì˜ˆì‹œ ì¶œë ¥: ['hello', ',', 'world', '!']

# 2. Encoding: ë¬¸ì¥ì„ í† í° ID ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
token_ids = tokenizer.encode(text, add_special_tokens=False)
print(f"Token IDs: {token_ids}")
# ì˜ˆì‹œ ì¶œë ¥: [7592, 1010, 2088, 999]

# 3. Full encoding with special tokens
encoded = tokenizer(
    text,
    add_special_tokens=True,   # [CLS], [SEP] ì¶”ê°€
    padding=True,              # padding ì¶”ê°€ (ë‹¨ì¼ ë¬¸ì¥ì—ë„ ê°€ëŠ¥)
    truncation=True,           # ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ìë¥´ê¸°
    return_tensors="pt"        # PyTorch tensorë¡œ ë°˜í™˜
)

# ì¶œë ¥
print("Full Encoding (with special tokens, padding):")
print(encoded)
print("Input IDs:", encoded['input_ids'])
print("Attention Mask:", encoded['attention_mask'])
```

ê²°ê³¼ë¬¼ì€!?  
> Full encodingì—ì„œëŠ” [CLS] [SEP] ë“±ì´ ì¶”ê°€ëœê±°ì„!

```text
Tokens ìœ¼ë¡œ ìª¼ê° ê±°! : ['hello', ',', 'world', '!']
Token IDs: [7592, 1010, 2088, 999]
Full Encoding (with special tokens, padding):
{'input_ids': tensor([[ 101, 7592, 1010, 2088,  999,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}
Input IDs: tensor([[ 101, 7592, 1010, 2088,  999,  102]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1]])
```

#### **3ï¸âƒ£ ì—¬ëŸ¬ êµ­ê°€ì˜ ì–¸ì–´ì²˜ë¦¬!** ğŸŒ
```python
from transformers import AutoTokenizer

# Multilingual examples
texts = [
    "Hello world",           # English
    "Bonjour le monde",      # French
    "Hola mundo",           # Spanish  
    "Guten Tag Welt",       # German
    "Ciao mondo"            # Italian
]

# Different tokenizers handle multilingual text differently
multilingual_tokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-cased")
english_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

print("=== Multilingual Tokenizer ===")
for text in texts:
    tokens = multilingual_tokenizer.tokenize(text)
    print(f"{text} â†’ {tokens}")

print("\n=== English-only Tokenizer ===")
for text in texts:  # Only English and French
    tokens = english_tokenizer.tokenize(text)
    print(f"{text} â†’ {tokens}")

```

ê²°ê³¼ë¬¼ì€!?  
> ë‹¨ìˆœ ì˜ì–´ í† í¬ë‚˜ì´ì €ë‘ ë‹¤êµ­ì–´ í•™ìŠµëœê±°ë‘ ë‹¤ë¥´ì§€ìš”!?

```text
=== Multilingual Tokenizer ===
Hello world â†’ ['Hello', 'world']
Bonjour le monde â†’ ['Bon', '##jou', '##r', 'le', 'monde']
Hola mundo â†’ ['Ho', '##la', 'mundo']
Guten Tag Welt â†’ ['Gut', '##en', 'Tag', 'Welt']
Ciao mondo â†’ ['Ci', '##ao', 'mondo']

=== English-only Tokenizer ===
Hello world â†’ ['hello', 'world']
Bonjour le monde â†’ ['bon', '##jou', '##r', 'le', 'monde']
Hola mundo â†’ ['ho', '##la', 'mundo']
Guten Tag Welt â†’ ['gut', '##en', 'tag', 'we', '##lt']
Ciao mondo â†’ ['cia', '##o', 'mon', '##do']
```
---

### ğŸ“ˆ **ì„±ëŠ¥ ì˜í–¥ ë¶„ì„**

#### **ğŸ† ì–´íœ˜ í¬ê¸° ì˜í–¥**

| ì–´íœ˜ í¬ê¸° | ì¥ì  | ë‹¨ì  | ìµœì  ìš©ë„ |
|------------|------|------|----------|
| **ì†Œí˜• (1K-5K)** | ğŸ’¾ Memory efficient<br/>âš¡ Fast training | ğŸ”„ Many subwords<br/>ğŸ“ Long sequences | Resource-constrained |
| **ì¤‘í˜• (10K-30K)** | âš–ï¸ Balanced performance<br/>âœ… Good coverage | ğŸ“Š Standard choice | Most applications |
| **ëŒ€í˜• (50K+)** | ğŸ¯ Better semantic units<br/>ğŸ“ Shorter sequences | ğŸ’¾ Memory intensive<br/>â±ï¸ Slower training | Large-scale models |

#### **ğŸ”§ Tokenization ì†ë„ë¹„êµ!**

```python
import time
from transformers import AutoTokenizer

text = "This is a sample text " * 1000  # Long text

tokenizers = {
    "BPE (GPT-2)": AutoTokenizer.from_pretrained("gpt2"),
    "WordPiece (BERT)": AutoTokenizer.from_pretrained("bert-base-uncased"),
    "SentencePiece (T5)": AutoTokenizer.from_pretrained("t5-small")
}

for name, tokenizer in tokenizers.items():
    start_time = time.time()
    tokens = tokenizer.tokenize(text)
    end_time = time.time()
    
    print(f"{name}:")
    print(f"  Tokens: {len(tokens)}")
    print(f"  Time: {end_time - start_time:.4f}s")
    print(f"  Speed: {len(tokens)/(end_time - start_time):.0f} tokens/s")
```

ê²°ê³¼ë¬¼ì€!?  
> BPEê°€ ì œì¼ ë¹ ë¥´ê³ , T5ê°€ í† í°ìˆ˜ê°€ ì ¤ ë§ì•„ìœ !

| Tokenizer Type     | Tokens | Time (ì´ˆ) | Speed (tokens/s) |
|--------------------|--------|-----------|------------------|
| **BPE (GPT-2)**    | 5001   | 0.0125    | 401,257          |
| **WordPiece (BERT)** | 5000 | 0.0142    | 352,380          |
| **SentencePiece (T5)** | 6000 | 0.0145  | 413,069          |



---

### âš ï¸ **í•œê³„ì  & ë„ì „ê³¼ì œ**

#### **1ï¸âƒ£ ì¼ê´€ë˜ì§€ ì•Šì€ Tokenization** ğŸ”„
```python
from transformers import AutoTokenizer

# Same words in different contexts
tokenizer = AutoTokenizer.from_pretrained("gpt2")

text1 = "run"
text2 = "running" 
text3 = "I like to run fast"
text4 = "The running water is clean"

print(f"'{text1}' â†’ {tokenizer.tokenize(text1)}")
print(f"'{text2}' â†’ {tokenizer.tokenize(text2)}")
print(f"'{text3}' â†’ {tokenizer.tokenize(text3)}")
print(f"'{text4}' â†’ {tokenizer.tokenize(text4)}")

# Output examples:
# 'run' â†’ ['run']
# 'running' â†’ ['running'] 
# 'I like to run fast' â†’ ['I', 'Ä like', 'Ä to', 'Ä run', 'Ä fast']
# 'The running water is clean' â†’ ['The', 'Ä running', 'Ä water', 'Ä is', 'Ä clean']
```

- ìœ„ì˜ ê²°ê³¼ë¥¼ ë³´ë©´ run ì—ëŒ€í•˜ì—¬ ì—¬ëŸ¬ë°©ì‹ìœ¼ë¡œ Tokenization ëœë‹¤!!

#### **2ï¸âƒ£ ì„œë¸Œì›Œë“œ ê²½ê³„ ë¬¸ì œ** âœ‚ï¸
```python
# Examples of problematic subword splitting
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

problematic_words = [
    "unhappiness",
    "preprocessor", 
    "antidisestablishmentarianism",
    "biocompatibility"
]

for word in problematic_words:
    tokens = tokenizer.tokenize(word)
    print(f"'{word}' â†’ {tokens}")

# Output examples:
# 'unhappiness' â†’ ['un', '##hap', '##piness']  # Loses "happy" semantic unit
# 'preprocessor' â†’ ['pre', '##proc', '##ess', '##or']  # Splits "process"
# 'antidisestablishmentarianism' â†’ ['anti', '##dis', '##esta', '##blish', '##ment', '##arian', '##ism']
# 'biocompatibility' â†’ ['bio', '##com', '##pat', '##ibility']  # Loses "compatibility"
```

**ë¬¸ì œì **: ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„ê°€ ì˜ëª» ë¶„í• ë¨ (pre / processorë¡œ êµ¬ë¶„ë˜ëŠ”ê²Œ ì œì¼ ì¢‹ê² ì§€ë§Œ!?..)  
**ì˜í–¥**: ëª¨ë¸ì´ ë‹¨ì–´ ê´€ê³„ì™€ í˜•íƒœí•™ì„ ì´í•´í•˜ëŠ”ë° ì–´ë ¤ì›€ì„ ê²ªì„ ìˆ˜ ìˆìŒ

#### **3ï¸âƒ£ ë„ë©”ì¸ì— ëŒ€í•œ ì§€ì‹ ë¶€ì¡±ìœ¼ë¡œ í•œê³„!** ğŸ¥
```python
from transformers import AutoTokenizer

# Medical text examples
medical_texts = [
    "Patient presents with acute myocardial infarction and requires immediate intervention",
    "Blood pressure elevated, prescribing ACE inhibitors for hypertension management",
    "CT scan reveals suspicious pulmonary nodules, scheduling biopsy procedure"
]

# General vs Medical tokenizer comparison
general_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
# medical_tokenizer = AutoTokenizer.from_pretrained("clinical-bert")  # Hypothetical

for text in medical_texts[:1]:  # First example only
    general_tokens = general_tokenizer.tokenize(text)
    print(f"General Tokenizer:")
    print(f"  Input: {text}")
    print(f"  Tokens: {general_tokens}")
    print(f"  Token count: {len(general_tokens)}")
    
    # Medical tokenizer would handle medical terms as single tokens
    print(f"\nMedical Tokenizer (Expected):")
    print(f"  Tokens: ['patient', 'presents', 'with', 'acute_myocardial_infarction', 'and', 'requires', 'immediate', 'intervention']")
    print(f"  Token count: 8 (significant reduction)")
    
# General: ['patient', 'presents', 'with', 'acute', 'my', '##oc', '##ard', '##ial', 'in', '##far', '##ction', 'and', 'requires', 'immediate', 'intervention']
# Medical: ['patient', 'presents', 'with', 'acute_myocardial_infarction', 'and', 'requires', 'immediate', 'intervention']
```

- ì˜í•™ìš©ì–´ë¥¼ ì˜ ìª¼ê°œì§€ ëª»í•œë‹¤!!  

#### **4ï¸âƒ£ ì–¸ì–´ë³„ íŠ¹í™”í•˜ëŠ”ê²ƒì— ëŒ€í•œ ë¬¸ì œ** ğŸŒ  
- **í˜•íƒœí•™ì  ë³µì¡ì„±**: í’ë¶€í•œ í˜•íƒœí•™ì„ ê°€ì§„ ì–¸ì–´ë“¤ (ë…ì¼ì–´, í„°í‚¤ì–´, í•€ë€ë“œì–´)
- **êµì°©ì–´ì  íŠ¹ì„±**: í˜•íƒœì†Œ ê²°í•©ìœ¼ë¡œ ë‹¨ì–´ê°€ í˜•ì„±ë˜ëŠ” ì–¸ì–´ë“¤ (ì¼ë³¸ì–´, í•œêµ­ì–´, í—ê°€ë¦¬ì–´)
- **ë¬¸ì ì²´ê³„ í˜¼ìš©**: í•œ í…ìŠ¤íŠ¸ì—ì„œ ì—¬ëŸ¬ ë¬¸ì ì²´ê³„ ì‚¬ìš© (ì¼ë³¸ì–´: íˆë¼ê°€ë‚˜ + ê°€íƒ€ì¹´ë‚˜ + í•œì)
- **ë³µí•©ì–´**: ë…ì¼ì–´ "DonaudampfschifffahrtsgesellschaftskapitÃ¤n" (ë‹¤ë‰´ë¸Œ ì¦ê¸°ì„  íšŒì‚¬ ì„ ì¥)

---

### ğŸ”® **ë¯¸ë˜ ë°©í–¥**

#### **1ï¸âƒ£ ì‹ ê²½ë§ ê¸°ë°˜ í† í¬ë‚˜ì´ì œì´ì…˜** ğŸ§ 
- **ê°œë…**: ì—”ë“œíˆ¬ì—”ë“œ í•™ìŠµ ê°€ëŠ¥í•œ í† í¬ë‚˜ì´ì œì´ì…˜
- **ì¥ì **: íƒœìŠ¤í¬ë³„ ìµœì í™”
- **ë„ì „ê³¼ì œ**: ê³„ì‚° ë³µì¡ì„±

#### **2ï¸âƒ£ ë©€í‹°ëª¨ë‹¬ í† í¬ë‚˜ì´ì œì´ì…˜** ğŸ–¼ï¸
> í…ìŠ¤íŠ¸ëŠ” ë¬¸ìë¥¼ í† í¬ë‚˜ì´ì œì´ì…˜í•˜ë©´ì„œ ìˆ«ìë¡œ ë°”ê¾¸ì§€ë§Œ!!  
> ì´ë¯¸ì§€ëŠ” ê·¸ëŸ°ê±° ì—†ì´ ViTë¡œ ë°”ë¡œ ë²¡í„°í™”í•´ë²„ë¦½ë‹ˆë‹¤!!  
```python
# Future concept: unified text-image tokenization
multimodal_input = {
    "text": "A cat sitting on a chair",
    "image": cat_image_tensor
}

unified_tokens = multimodal_tokenizer.tokenize(multimodal_input)
# Outputs both text and visual tokens in same space

# Example output:
# [
#   ("A", "text_token"),
#   ("cat", "text_token"), 
#   ("<img_patch_1>", "visual_token"),
#   ("sitting", "text_token"),
#   ("<img_patch_2>", "visual_token"),
#   ("on", "text_token"),
#   ("chair", "text_token")
# ]
```

- ì´ëŸ°ë°©ì‹ìœ¼ë¡œí•´ì„œ êµ¬ê¸€ì´ Multi-modal large Model ì¸ `Gemini` ë¥¼ ê³µê°œí–ˆì§€ìœ !!  
- ì´ë¯¸ì§€ë¥¼ í† í°í™”í•˜ëŠ”ë²•!? ìš°ì„  [ViT ì— ëŒ€í•˜ì—¬ ê³µë¶€](https://drfirstlee.github.io/posts/ViT/)í•˜ë©´ ì•Œ ìˆ˜ ìˆì–´ìš”!  
- ì–¸ì–´ì˜ ê²½ìš°ëŠ” ì•„ë¬´ë¦¬ ë‹¨ì–´ê°€ ë§ì•„ë„ í•œë„ê°€ ìˆëŠ”ë°,  
- ê·¸ëŸ°ë°!? 
    - ë¬¸ì œ1. ì´ë¯¸ì§€ëŠ” í† í°ì´ ë¬´ì œí•œ ë‚˜ì˜¬ìˆ˜ ìˆë‹¤!! ê·¸ëŸ¼ ì–´ë–»í•˜ì§€!?  
    - ë¬¸ì œ2. ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ í† í°ì˜ ì •ë ¬ì€ ì–´ë–»ê²Œí• ê¹Œ!?

**ë¬¸ì œ1. í† í° ë‹¤ì–‘ì„± ë¬¸ì œ** ğŸŒˆ
- **í…ìŠ¤íŠ¸ í† í°**: ê³ ì •ëœ ì–´íœ˜ ì‚¬ì „ (ì˜ˆ: 50,000ê°œ)
- **ì´ë¯¸ì§€ í† í°**: ë¬´í•œëŒ€ì— ê°€ê¹Œìš´ ë‹¤ì–‘ì„± (ê° íŒ¨ì¹˜ë§ˆë‹¤ ì™„ì „íˆ ë‹¤ë¥¸ ê°’)!!!!  
- **ê²°ê³¼**: ëª¨ë¸ì´ ì´ë¯¸ì§€ í† í°ì„ í•™ìŠµí•˜ê³  ì¼ë°˜í™”í•˜ê¸° ë§¤ìš° ì–´ë µê³ ,ì—°ì‚°ëŸ‰ì´ ë§ì•„ì§    
**ğŸ’¡ í•´ê²°ë²•ë“¤:**  
> ì´ë¯¸ì§€ í† í°ì„ ìµœì†Œí™”í•˜ê¸°ìœ„í•´ ë…¸ë ¥í•˜ê±°ë‚˜ (1,2,3) íš¨ìœ¨í™”ë¥¼ í†µí•´ì„œ í•´ê²°í•˜ê³ ìí•©ë‹ˆë‹¤!(4,5)  

1. **ğŸ”² ê³ ì • íŒ¨ì¹˜ ë¶„í• **: ViT ë°©ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ í¬ê¸°ì™€ ìƒê´€ì—†ì´ ì¼ì •í•œ í† í° ìˆ˜ ë³´ì¥ (224Ã—224 â†’ 196ê°œ í† í°)
2. **ğŸ“‰ ì ì‘ì  ì••ì¶•**: ëª©í‘œ í† í° ìˆ˜ì— ë§ê²Œ ì´ë¯¸ì§€ ì••ì¶• (512Ã—512 â†’ 100ê°œ í† í°)  
3. **ğŸ¯ ì„ íƒì  í† í°í™”**: ì¤‘ìš”í•œ ì˜ì—­ë§Œ í† í°í™” (ê´€ì‹¬ ì˜ì—­ ê¸°ë°˜ìœ¼ë¡œ 128ê°œë§Œ ì„ íƒ)
4. **âš¡ íš¨ìœ¨ì  ì–´í…ì…˜**: Flash Attention, Sparse Attentionìœ¼ë¡œ ì—°ì‚°ëŸ‰ 2-4ë°° ì¤„ì„
5. **ğŸ—ï¸ ê³„ì¸µì  ì²˜ë¦¬**: ì´ˆê¸°ì—ëŠ” ë³„ë„ ì²˜ë¦¬, ì ì§„ì ìœ¼ë¡œ í¬ë¡œìŠ¤ ëª¨ë‹¬ ì–´í…ì…˜ ë„ì…

**ë¬¸ì œ2. ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ Tokenì˜ ì •ë ¬!! í¬ë¡œìŠ¤ ëª¨ë‹¬ ë§¤ì¹­ ë¬¸ì œ** ğŸ”—
- í…ìŠ¤íŠ¸ "ê³ ì–‘ì´"ì™€ ì´ë¯¸ì§€ íŒ¨ì¹˜ë“¤ ê°„ì˜ ì˜ë¯¸ì  ì—°ê²°ì´ ì–´ë ¤ì›€
- ê° ì´ë¯¸ì§€ íŒ¨ì¹˜ê°€ ì–´ë–¤ í…ìŠ¤íŠ¸ í† í°ê³¼ ê´€ë ¨ìˆëŠ”ì§€ ë¶ˆë¶„ëª…  
**ğŸ’¡ í•´ê²°ë²•ë“¤:**  
> ì—¬ê¸°ëŠ” ì´ì œ í† í°í™”ë¼ê¸°ë³´ë‹¨ Transformer ë¶€ë¶„ì—ì„œ ë” ìì„¸íˆ ì•Œì•„ë³´ì•„ìš”!  
1. **ğŸ¯ í†µí•© ì„ë² ë”© ê³µê°„**: ì´ë¯¸ì§€ íŒ¨ì¹˜ë¥¼ í…ìŠ¤íŠ¸ í† í°ê³¼ ë™ì¼í•œ ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (Gemini: 2048ì°¨ì› í†µí•©)
2. **ğŸ”„ í¬ë¡œìŠ¤ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜**: ì´ë¯¸ì§€ í† í°ì´ í…ìŠ¤íŠ¸ í† í°ê³¼ ì§ì ‘ ìƒí˜¸ì‘ìš©í•˜ë„ë¡ ì„¤ê³„
3. **ğŸ“š ëŒ€ê·œëª¨ ë©€í‹°ëª¨ë‹¬ í•™ìŠµ**: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒ ìˆ˜ì‹­ì–µ ê°œë¡œ ì •ë ¬ ê´€ê³„ í•™ìŠµ
4. **ğŸ§© í† í° ë ˆë²¨ ì •ë ¬**: CLIP ë°©ì‹ì²˜ëŸ¼ ì´ë¯¸ì§€ ì˜ì—­ê³¼ í…ìŠ¤íŠ¸ ë‹¨ì–´ë¥¼ ì§ì ‘ ë§¤ì¹­
5. **ğŸ¨ ì˜ë¯¸ì  ê·¸ë£¹í•‘**: ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ ì´ë¯¸ì§€ íŒ¨ì¹˜ë“¤ì„ í•˜ë‚˜ë¡œ ë¬¶ì–´ í…ìŠ¤íŠ¸ì™€ ë§¤ì¹­

**ğŸŒŸ ì‹¤ì œ ëª¨ë¸ë“¤ì˜ í•´ê²° ì „ëµ:**

| ëª¨ë¸ | ë¬¸ì œ1 í•´ê²°ë²• | ë¬¸ì œ2 í•´ê²°ë²• | íŠ¹ì§• |
|------|-------------|-------------|------|
| **GPT-4V** | ì ì‘ì  í† í°í™” | ê³„ì¸µì  í¬ë¡œìŠ¤ ì–´í…ì…˜ | ì´ë¯¸ì§€ ë³µì¡ë„ì— ë”°ë¼ ì¡°ì • |
| **Gemini Ultra** | ê³ íš¨ìœ¨ ì••ì¶• | í†µí•© ì„ë² ë”© ê³µê°„ | í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì™„ì „ í†µí•© |
| **Claude 3** | ì„ íƒì  ì²˜ë¦¬ | ì˜ë¯¸ì  ê·¸ë£¹í•‘ | ì¤‘ìš”í•œ ì˜ì—­ë§Œ ì§‘ì¤‘ ì²˜ë¦¬ |
| **LLaVA** | ê³ ì • íŒ¨ì¹˜ ë¶„í•  | CLIP ê¸°ë°˜ ì •ë ¬ | 576ê°œ í† í°ìœ¼ë¡œ ê³ ì • |

**ğŸ¯ í•µì‹¬ ê¹¨ë‹¬ìŒ**: 
- ë¬¸ì œ1ì€ **íš¨ìœ¨ì„±**ì˜ ë¬¸ì œ â†’ ìŠ¤ë§ˆíŠ¸í•œ í† í° ê´€ë¦¬ë¡œ í•´ê²°
- ë¬¸ì œ2ëŠ” **ì •ë ¬**ì˜ ë¬¸ì œ â†’ ëŒ€ê·œëª¨ í•™ìŠµê³¼ í†µí•© ì„ë² ë”©ìœ¼ë¡œ í•´ê²°
- ë‘ ë¬¸ì œ ëª¨ë‘ **"ë” ë§ì€ ë°ì´í„°"**ê°€ ì•„ë‹Œ **"ë” ë˜‘ë˜‘í•œ ë°©ë²•"**ìœ¼ë¡œ í•´ê²°! ğŸš€ 

### ğŸ§© ì‹¤ì„¸ ì‚¬ìš©ë˜ê³ ìˆëŠ” ì£¼ìš” ëª¨ë¸ë³„ Tokenizer ìš”ì•½   

| ëª¨ë¸                     | Tokenizer ì¢…ë¥˜              | íŠ¹ì§•                                                                 |
|--------------------------|-----------------------------|----------------------------------------------------------------------|
| GPT-2 / GPT-3 / GPT-4    | BPE (OpenAI GPT Tokenizer)  | `tiktoken` ì‚¬ìš©, ì˜ì–´ì— ìµœì í™”ë¨                                     |
| LLaMA / LLaMA2 / LLaMA3  | SentencePiece + BPE         | Metaê°€ ì§ì ‘ í•™ìŠµí•œ SentencePiece ê¸°ë°˜ êµ¬ì¡° ì‚¬ìš©                      |
| Gemini (Google)          | SentencePiece ê¸°ë°˜ ì¶”ì •     | PaLM/Flan ê³„ì—´ê³¼ ìœ ì‚¬í•œ êµ¬ì¡°, ì„¸ë¶€ í† í¬ë‚˜ì´ì € ë¯¸ê³µê°œ                |
| Claude (Anthropic)       | BPE ë³€í˜•                    | ì„¸ë¶€ êµ¬ì¡°ëŠ” ë¹„ê³µê°œ, ìì²´ í† í¬ë‚˜ì´ì € êµ¬ì¡° ì‚¬ìš©                        |
| Qwen (Alibaba)           | GPT-style BPE               | ì¤‘êµ­ì–´ ìµœì í™”, ì˜ì–´ë„ ì§€ì›, **Tokenizer ê³µê°œë¨**                    |
| Mistral / Mixtral        | SentencePiece               | open-source ëª¨ë¸, HuggingFace tokenizer êµ¬ì¡° ë”°ë¦„                    |
| **Qwen-VL (ë©€í‹°ëª¨ë‹¬)**   | GPT-style BPE + Vision íŠ¹í™” | í…ìŠ¤íŠ¸ëŠ” Qwenê³¼ ë™ì¼, ì´ë¯¸ì§€ ì…ë ¥ì€ CLIP-style íŒ¨ì¹˜ ë¶„í•  ì‚¬ìš©        |
| **Gemini (ë©€í‹°ëª¨ë‹¬)**    | SentencePiece + Vision      | ì •í™•í•œ êµ¬ì¡° ë¯¸ê³µê°œ, Flamingo-like êµ¬ì¡°ë¡œ ì¶”ì •                        |
| **Grok (xAI)**           | ë¹„ê³µê°œ                      | ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € êµ¬ì¡° ëŒ€ë¶€ë¶„ ë¹„ê³µê°œ, ì˜ì–´ ê¸°ë°˜ ì¶”ì •                |
