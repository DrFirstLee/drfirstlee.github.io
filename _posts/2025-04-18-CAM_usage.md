---
layout: post
title: "Studying CAM with Python! - íŒŒì´ì¬ìœ¼ë¡œ CAM ê³µë¶€í•˜ê¸° "
author: [DrFirst]
date: 2025-04-18 09:00:00 +0900
categories: [AI, Experiment]
tags: [CAM, Vision AI, GAP, CVPR, CVPR2016, XAI, Class Activation Map, Python, ì‹¤ìŠµ, experiments]
lastmod : 2025-04-18 09:00:00
sitemap :
  changefreq : weekly
  priority : 0.9
---
## (English) Studying CAM with Python

Today, we will delve into CAM (Class Activation Map) in detail using Python code!!
Before we begin, the necessary packages are as follows!!
Don't worry, it's all possible with CPU without a GPU~!^^

```python
import torch
from torchvision import models, transforms
from PIL import Image
import requests
from io import BytesIO
import matplotlib.pyplot as plt
import numpy as np
import cv2
```

CAM basically starts with image classification!
Today, we aim to create a CAM image based on a ResNet classification model trained on ImageNet!!!

### What is ImageNet!?
- Contains over 14 million images and is categorized into approximately 20,000+ noun hierarchical structures (based on WordNet).
- Made a significant contribution to the development of deep learning in the field of computer vision.
- ResNet is also trained based on this ImageNet data!!

### What is ResNet?
- Innovative model in the vision field: An important structure that greatly improved image recognition performance!! - Announced in 2015 by MS Research!!
- Overcame the difficulty of training deep neural networks with residual connections.
- Residual connections: Prevents gradient vanishing by adding the learned changes to the input!!
- Enables the formation of truly deep networks (DNNs): Effective learning is possible even in deep layers!

### Code Start!!

#### Preparing Related Data and Models
```python
# âœ… Loading ImageNet class index (for dog class identification)
import json
imagenet_url = "https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
imagenet_classes = requests.get(imagenet_url).text.splitlines()

# âœ… Loading test image (e.g., from the internet)
img_url = "https://images.unsplash.com/photo-1558788353-f76d92427f16"  # Dog photo
response = requests.get(img_url)
img = Image.open(BytesIO(response.content)).convert("RGB")

# âœ… Loading pre-trained ResNet18 (inference without training)
model = models.resnet18(pretrained=True)
model.eval()
```

Through the above process, we load the ImageNet class data, the dog photo, and finally the pre-trained ResNet18 model!
You can also see the model structure through `eval` as shown below~~
We will explore the detailed structure of the model in the next ResNet study!

<details>
  <summary>View ResNet Detailed Structure</summary>
```
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)
```
</details>

#### Preprocessing Start!!
```python
# âœ… Extracting dog classes (simple method: names containing 'golden retriever')
dog_classes = [i for i, name in enumerate(imagenet_classes) if 'golden retriever' in name.lower()]

# âœ… Image preprocessing
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
])
input_tensor = transform(img).unsqueeze(0)  # shape: [1, 3, 224, 224]
```
Now all preparations are complete!! Let's put the data into the model and perform inference!!

#### Simple Classification Model!! (Fully Connected Layer + Softmax)
```python
# âœ… Load pre-trained ResNet18 (inference without training)
model = models.resnet18(pretrained=True)
model.eval()

# âœ… Inference
with torch.no_grad():
    output = model(input_tensor)
    pred_class = output.argmax(dim=1).item()

# âœ… Result check
pred_label = imagenet_classes[pred_class]
is_dog = pred_class in dog_classes

print(f"Predicted label: {pred_label}")
print("ğŸ¦´ Is it a dog?", "âœ… Yes" if is_dog else "âŒ No")
```

Through the above code, you can see 'whether it is simply classified as a dog!'
This was the method of the classification model before CAM~~

If you actually examine the output vector `output`,
```python
output[0][205:210]
```
You can see that the value at index 207 is indeed the largest at 13.7348!
(Since 207 came out from `arg.max(dim=1)`, it is indeed the highest value, right!?)

```
tensor([ 9.8655,  6.4875, 13.7348, 11.1263,  8.8567])
```
#### CAM Start!!!!
Let's refer back to the CAM structure we summarized in the previous post.

| Step | Data Shape        | Description                                                  |
|------|-------------------|--------------------------------------------------------------|
| ğŸ“· Input Image        | `[3, 224, 224]`   | RGB Image                                                    |
| ğŸ”„ CNN(resnet) Last conv Output | `[512, 7, 7]`     | 512 7Ã—7 feature maps                                       |
| ğŸ”¥ CAM Calculation: Weighted sum of CNN(resnet) last conv output and **class_weight** | `[7, 7]`        | 7Ã—7 feature map                                          |
| ğŸ”¼ Final CAM Image Creation (Upsample) | `[224, 224]`      | Heatmap overlay possible on the original image           |
| ğŸ“‰ GAP (Global Average Pooling) | `[512]`           | Channel-wise average vector of feature map `[512, 7, 7]` |
| ğŸ§® FC Layer            | `[N_classes]`     | Converts GAP result to class scores                            |
| ğŸ¯ Softmax              | `[N_classes]`     | Outputs predicted class probability values                     |

Load the model again, just like before!
```python
# âœ… Load pretrained ResNet18
model = models.resnet18(pretrained=True)
model.eval()
```

##### feature map Extraction!  

Now, the important part begins! It's as follows!!
```python
features = []

def hook_fn(module, input, output):
    features.append(output)

model.layer4.register_forward_hook(hook_fn)  # Last conv block
```
- `hook_fn`: A function to call data within the model. `module` is the layer object, `input` is the input tuple, and `output` is the output tensor!
- `model.layer4.register_forward_hook(hook_fn)`: Places `hook_fn` at the end of `model.layer4`, so that the output of conv layer4 is stored in the `features` list.

After placing the `hook_fn` function at the end of the model's `layer4`,
execute the model in the same way as the simple classification model.

Now, let's proceed with the [CNN(resnet) Last conv Output]!

| Step | Data Shape        | Description                                                  |
|------|-------------------|--------------------------------------------------------------|
| ğŸ”„ CNN(resnet) Last conv Output | `[512, 7, 7]`     | 512 7Ã—7 feature maps                                       |
```python
# âœ… Get weights from the final linear layer
params = list(model.parameters())
fc_weights = params[-2]  # shape: [1000, 512]
class_weights = fc_weights[pred_class].detach().cpu().numpy()  # [512]

# âœ… Get feature map from hook
feature_map = features[0].squeeze(0).detach().cpu().numpy()  # [512, 7, 7]
```
Through this, we have extracted the `[512, 7, 7]` size feature map!

##### create CAM!!!

Now, this is the process of creating a CAM image from this feature map!

| Step | Data Shape        | Description                                                  |
|------|-------------------|--------------------------------------------------------------|
| ğŸ”¥ CAM Calculation: Weighted sum of CNN(resnet) last conv output and **class_weight** | `[7, 7]`        | 7Ã—7 feature map                                          |
| ğŸ”¼ Final CAM Image Creation (Upsample) | `[224, 224]`      | Heatmap overlay possible on the original image           |
```python
# âœ… Compute CAM
cam = np.zeros((7, 7), dtype=np.float32)
for i in range(len(class_weights)):
    cam += class_weights[i] * feature_map[i]

cam = np.maximum(cam, 0)
cam = cam - np.min(cam)
cam = cam / np.max(cam)
cam = cv2.resize(cam, (224, 224))
heatmap = cv2.applyColorMap(np.uint8(255 *cam), cv2.COLORMAP_JET)
```
In the above process, we obtain a `[7, 7]` size CAM by calculating the weighted sum of the `class_weights` and the `[512, 7, 7]` feature map from the last conv output of ResNet!
Then, we create the final heatmap image through resizing, i.e., Upsampling!

Finally, we overlay the heatmap image on the original image for visualization!
```python
# âœ… Overlay CAM on original image
img_cv = np.array(transforms.Resize((224, 224))(img))[:, :, ::-1]  # PIL â†’ OpenCV BGR
overlay = cv2.addWeighted(img_cv, 0.5, heatmap, 0.5, 0)

# âœ… Show
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title("Original Image")
plt.imshow(img)
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title(f"CAM: {imagenet_classes[pred_class]}")
plt.imshow(overlay[:, :, ::-1])  # Back to RGB
plt.axis('off')
plt.tight_layout()
plt.show()
```
Then!! You will get the image shown directly below, which we saw in the previous post~!

![golden](https://github.com/user-attachments/assets/d3fd65b1-11bd-44dc-9516-3118fd586bf0)

##### image classification using CAM!

In addition to this!!
We can also distinguish within the CAM!!!
By passing through GAP and the FC layer, and calculating the softmax, we can also see the accuracy of the result distinction~!

| Step | Data Shape        | Description                                                  |
|------|-------------------|--------------------------------------------------------------|
| ğŸ“‰ GAP (Global Average Pooling) | `[512]`           | Channel-wise average vector of feature map `[512, 7, 7]` |
| ğŸ§® FC Layer            | `[N_classes]`     | Converts GAP result to class scores                            |
| ğŸ¯ Softmax              | `[N_classes]`     | Outputs predicted class probability values                     |
```python
# âœ… GAP operation: [512, 7, 7] â†’ [512]
gap_vector = feature_map.mean(axis=(1, 2))  # shape: [512]

# âœ… FC operation: [512] Ã— [1000, 512]^T â†’ [1000]
logits = np.dot(fc_weights.detach().cpu().numpy(), gap_vector)  # shape: [1000]

# âœ… Softmax
exp_logits = np.exp(logits - np.max(logits))  # numerical stability
probs = exp_logits / exp_logits.sum()

# âœ… Predicted class
gap_pred_class = np.argmax(probs)
gap_pred_label = imagenet_classes[gap_pred_class]

# âœ… Result comparison
print("\nâœ… GAP â†’ FC â†’ Softmax based prediction result")
print(f"Predicted label: {gap_pred_label}")
print("ğŸ¦´ Is it a dog?", "âœ… Yes" if gap_pred_class in dog_classes else "âŒ No")
```

After going through the above process!?
You can confirm the result:
**Predicted label: golden retriever**

Is it real?
```python
probs[205:210]
```
Looking at this, the value at index 207 is indeed the largest, right!?
However!! You can see that it is different from the original classification vector value of 13.7348!!

```
[1.7553568e-02, 6.1262056e-04, 8.4515899e-01, 6.3063554e-02, 6.3092457e-03]
```

Through today's process, we were able to understand the detailed operation of CAM well!!

<details>
  <summary>View Full Code</summary>
```python
import torch
from torchvision import models, transforms
from PIL import Image
import requests
from io import BytesIO
import numpy as np
import matplotlib.pyplot as plt
import numpy as np
import cv2
import json

imagenet_url = "https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
imagenet_classes = requests.get(imagenet_url).text.splitlines()

# âœ… Loading test image (e.g., from the internet)
img_url = "https://images.unsplash.com/photo-1558788353-f76d92427f16"  # Dog photo
response = requests.get(img_url)
img = Image.open(BytesIO(response.content)).convert("RGB")

# âœ… Loading pre-trained ResNet18 (inference without training)
model = models.resnet18(pretrained=True)
model.eval()

# âœ… Extracting dog classes (simple method: names containing 'golden retriever')
dog_classes = [i for i, name in enumerate(imagenet_classes) if 'golden retriever' in name.lower()]

# âœ… Image preprocessing
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
])
input_tensor = transform(img).unsqueeze(0)  # shape: [1, 3, 224, 224]

# âœ… Inference
with torch.no_grad():
    output = model(input_tensor)
    pred_class = output.argmax(dim=1).item()

# âœ… Result check
pred_label = imagenet_classes[pred_class]
is_dog = pred_class in dog_classes

print(f"Predicted label: {pred_label}")
print("ğŸ¦´ Is it a dog?", "âœ… Yes" if is_dog else "âŒ No")


# âœ… Load pretrained ResNet18
model = models.resnet18(pretrained=True)
model.eval()

# âœ… Hook to get final conv feature map
features = []

def hook_fn(module, input, output):
    features.append(output)

model.layer4.register_forward_hook(hook_fn)  # Last conv block

# âœ… Predict
with torch.no_grad():
    output = model(input_tensor)
    pred_class = output.argmax(dim=1).item()

# âœ… Get weights from the final linear layer
params = list(model.parameters())
fc_weights = params[-2]  # shape: [1000, 512]
class_weights = fc_weights[pred_class].detach().cpu().numpy()  # [512]

# âœ… Get feature map from hook
feature_map = features[0].squeeze(0).detach().cpu().numpy()  # [512, 7, 7]

# âœ… Compute CAM
cam = np.zeros((7, 7), dtype=np.float32)
for i in range(len(class_weights)):
    cam += class_weights[i] * feature_map[i]

# Normalize & resize
cam = np.maximum(cam, 0)
cam = cam - np.min(cam)
cam = cam / np.max(cam)
cam = cv2.resize(cam, (224, 224))
heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)

# âœ… Overlay CAM on original image
img_cv = np.array(transforms.Resize((224, 224))(img))[:, :, ::-1]  # PIL â†’ OpenCV BGR
overlay = cv2.addWeighted(img_cv, 0.5, heatmap, 0.5, 0)

# âœ… Show
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title("Original Image")
plt.imshow(img)
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title(f"CAM: {imagenet_classes[pred_class]}")
plt.imshow(overlay[:, :, ::-1])  # Back to RGB
plt.axis('off')
plt.tight_layout()
plt.show()

# âœ… Result text output
print(f"Predicted label: {imagenet_classes[pred_class]}")
print("ğŸ¦´ Is it a dog?", "âœ… Yes" if pred_class in dog_classes else "âŒ No")
```
</details>

---

## (í•œêµ­ì–´)  íŒŒì´ì¬ìœ¼ë¡œ CAM ê³µë¶€í•˜ê¸° 

ì˜¤ëŠ˜ì€ Python ì½”ë“œë¡œ CAM(Class Activation Map) ì— ëŒ€í•˜ì—¬ ìì„¸íˆ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤!!
ì•Œì•„ë³´ê¸°ì— ì•ì„œ í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤!!
GPU ì—†ì´!! CPU ë¡œë„ ëª¨ë‘ ê°€ëŠ¥í•˜ë‹ˆ ê±±ì •ë§ˆì„¸ìš”~!^^

```python
import torch
from torchvision import models, transforms
from PIL import Image
import requests
from io import BytesIO
import matplotlib.pyplot as plt
import numpy as np
import cv2
```

CAMë„ ê¸°ë³¸ì ìœ¼ë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜ì—ì„œ ì‹œì‘ë©ë‹ˆë‹¤!
ì˜¤ëŠ˜ì€ imagenetìœ¼ë¡œ í•™ìŠµëœ resnetì˜ ë¶„ë¥˜ ëª¨ë¸ì„ ë°”íƒ•ìœ¼ë¡œ CAMì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ ë³´ê³ ìí•©ë‹ˆë‹¤!!!

### imagenetì´ë€!?
!(image_net)[]
- ì•½ 1,400ë§Œ ê°œê°€ ë„˜ëŠ” ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ê³  ìˆìœ¼ë©°, ì•½ 2ë§Œ ê°œ ì´ìƒì˜ ëª…ì‚¬ ê³„ì¸µ êµ¬ì¡° (WordNet ê¸°ë°˜)ë¡œ ì´ë£¨ì–´ì§„ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜  
- ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì˜ ë”¥ëŸ¬ë‹ ë°œì „ì— ì§€ëŒ€í•œ ê³µí—Œì„ í•¨
- resnetë„ ì´ imagenetë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµí•¨!!   

### resnetì´ë€?
!(resnet)[]

- ë¹„ì „ ë¶„ì•¼ í˜ì‹  ëª¨ë¸: ì´ë¯¸ì§€ ì¸ì‹ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¨ ì¤‘ìš”í•œ êµ¬ì¡°!! -2015ë…„ MS researchì—ì„œ ë°œí‘œ!!
- ì”ì°¨ ì—°ê²°(Residual connections)ë¡œ ê¹Šì€ ì‹ ê²½ë§ í•™ìŠµ ì–´ë ¤ì›€ì„ ê·¹ë³µ  
- ì”ì°¨ ì—°ê²°(Residual connections): ì…ë ¥ì— í•™ìŠµëœ ë³€í™”ëŸ‰ì„ ë”í•´ ê¸°ìš¸ê¸° ì†Œì‹¤ì„ ë§‰ìŒ!!
- ì§„ì§œ ê¹Šì€ ë„¤íŠ¸ì›Œí¬(DNN) í˜•ì„± ê°€ëŠ¥: ê¹Šì€ ì¸µì—ì„œë„ íš¨ê³¼ì ì¸ í•™ìŠµì´ ê°€ëŠ¥í•´ì§!

### ì½”ë“œ ì‹œì‘!!  

#### ê´€ë ¨ ë°ì´í„° ë° ëª¨ë¸ ì¤€ë¹„  
```python
# âœ… ImageNet class index ë¡œë”© (ê°•ì•„ì§€ í´ë˜ìŠ¤ êµ¬ë¶„ìš©)
import json
imagenet_url = "https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
imagenet_classes = requests.get(imagenet_url).text.splitlines()

# âœ… í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸° (ì˜ˆ: ì¸í„°ë„· ì´ë¯¸ì§€)
img_url = "https://images.unsplash.com/photo-1558788353-f76d92427f16"  # ê°•ì•„ì§€ ì‚¬ì§„
response = requests.get(img_url)
img = Image.open(BytesIO(response.content)).convert("RGB")

# âœ… ì‚¬ì „ í•™ìŠµëœ ResNet18 ë¡œë“œ (í•™ìŠµ ì—†ì´ inference)
model = models.resnet18(pretrained=True)
model.eval()
```

ìœ„ì˜ ê³¼ì •ì„ í†µí•´ì„œ, imagenetì˜ í´ë˜ìŠ¤ ë°ì´í„°ë„ ë°›ì•„ì˜¤ê³ , ê°•ì•„ì§€ ì‚¬ì§„ë„ ë°›ì•„ì˜¤ê³ ! ë§ˆì§€ë§‰ìœ¼ë¡œ resnet18ì˜ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ë„ ë¶ˆëŸ¬ì˜¤ê²Œë©ë‹ˆë‹¤!  
evalì„ í†µí•´ì„œ ëª¨ë¸ë„ ì•„ë˜ì™€ ê°™ì´ ë³¼ìˆ˜ ìˆì§€ìš”~~  
ëª¨ë¸ì˜ ì„¸ë¶€êµ¬ì¡°ëŠ”!? ë‹¤ìŒ resnet ê³µë¶€ì—ì„œ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤!  

<details>
  <summary>resnet ì„¸ë¶€ êµ¬ì¡° ë³´ê¸°</summary>

```
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)
```
</details>


#### ì „ì²˜ë¦¬ ì‹œì‘!!  

```python
# âœ… ê°•ì•„ì§€ í´ë˜ìŠ¤ë“¤ ì¶”ë ¤ë‚´ê¸° (ê°„ë‹¨í•œ ë°©ë²•: ì´ë¦„ì— 'golden retriever' í¬í•¨ëœ ê²ƒ)
dog_classes = [i for i, name in enumerate(imagenet_classes) if 'golden retriever' in name.lower()]

# âœ… ì´ë¯¸ì§€ ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
])
input_tensor = transform(img).unsqueeze(0)  # shape: [1, 3, 224, 224]
```
ì´ì œ ëª¨ë‘” ì¤€ë¹„ê°€ ëë‚¬ìŠµë‹ˆë‹¤!! ëª¨ë¸ì— ë°ì´í„°ë¥¼ ë„£ì–´ì„œ ì¶”ë¡ í•´ë³´ì•„ìš”!!

#### ë‹¨ìˆœ ë¶„ë¥˜ëª¨ë¸!! (Fully Connected Layer + Softmax)
```python
# âœ… ì‚¬ì „ í•™ìŠµëœ ResNet18 ë¡œë“œ (í•™ìŠµ ì—†ì´ inference)
model = models.resnet18(pretrained=True)
model.eval()

# âœ… ì¶”ë¡ 
with torch.no_grad():
    output = model(input_tensor)
    pred_class = output.argmax(dim=1).item()

# âœ… ê²°ê³¼ í™•ì¸
pred_label = imagenet_classes[pred_class]
is_dog = pred_class in dog_classes

print(f"Predicted label: {pred_label}")
print("ğŸ¦´ Is it a dog?", "âœ… Yes" if is_dog else "âŒ No")
```

ìœ„ ì½”ë“œë¥¼ í†µí•´ì„œ 'ë‹¨ìˆœíˆ ê°•ì•„ì§€ë¡œ ë¶„ë¥˜í•˜ëŠ”ê°€!' ì— ëŒ€í•˜ì—¬ ì•Œì•„ë³¼ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
CAMì´ì „ ë¶„ë¥˜ëª¨ë¸ì˜ ë°©ì‹ì´ì—ˆì§€ìš”~~

ì‹¤ì œë¡œ ê²°ê³¼ê°’ ë²¡í„° outputì„ ì¡°ì‚¬í•´ë³´ë©´!
```python
output[0][205:210]
```
ì •ë§ë¡œ 207ë²ˆì¨°ì˜ ê°’ì´ 13.7348ë¡œ í° ê°’ì„ì„ ì•Œìˆ˜ ìˆìŠµë‹ˆë‹¤!
(arg.max(dim=1)ì—ì„œ 207ì´ ë‚˜ì™”ìœ¼ë‹ˆìµœê³ ê°’ì´ ë§ëŠ”ê²ƒ ì•„ì‹œì£ !?)

```python
tensor([ 9.8655,  6.4875, 13.7348, 11.1263,  8.8567])
```
#### CAM ì‹œì‘!!!! 
ì§€ë‚œ í¬ìŠ¤íŒ…ì—ì„œ ì •ë¦¬í•´ë³¸ CAMì˜ êµ¬ì¡°ë¥¼ ë‹¤ì‹œ ì°¸ê³ í•´ë´…ë‹ˆë‹¤

| ë‹¨ê³„ | ë°ì´í„° í˜•íƒœ        | ì„¤ëª…                                      |
|------|--------------------|-------------------------------------------|
| ğŸ“· ì…ë ¥ ì´ë¯¸ì§€       | `[3, 224, 224]`   | RGB ì´ë¯¸ì§€                                 |
| ğŸ”„ CNN(resnet) ë§ˆì§€ë§‰ conv ì¶œë ¥ | `[512, 7, 7]`     | 512ê°œì˜ 7Ã—7 feature map                     |
| ğŸ”¥ CAM ê³„ì‚° : CNN(resnet) ë§ˆì§€ë§‰ conv ì¶œë ¥ ê³¼  **class_weight**ì˜ weighted sum | `[7, 7]`     | 7Ã—7 feature map                     |
| ğŸ”¼ ìµœì¢… CAM ì´ë¯¸ì§€ ë§Œë“¤ê¸° (Upsample)       | `[224, 224]`      | ì›ë³¸ ì´ë¯¸ì§€ ìœ„ì— íˆíŠ¸ë§µ overlay ê°€ëŠ¥        |
| ğŸ“‰ GAP(Global Average Pooling) | `[512]`             | feature map[512, 7, 7]ì˜ ì±„ë„ë³„ í‰ê·  ë²¡í„°               |
| ğŸ§® FC Layer               | `[N_classes]`       | GAP ê²°ê³¼ë¥¼ í´ë˜ìŠ¤ë³„ scoreë¡œ ë³€í™˜              |
| ğŸ¯ Softmax               | `[N_classes]`       | ì˜ˆì¸¡ í´ë˜ìŠ¤ í™•ë¥ ê°’ ì¶œë ¥                        |


ì „ê³¼ ë™ì¼í•˜ê²Œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ ì¤ë‹ˆë‹¤!

```python
# âœ… Load pretrained ResNet18
model = models.resnet18(pretrained=True)
model.eval()
```

##### feature map ì¶”ì¶œí•˜ê¸°!!
ì§€ê¸ˆë¶€í„° ì¤‘ìš”í•œ ë¶€ë¶„ì´ ì‹œì‘ë©ë‚˜ë‹¤! ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤!! 

```python
features = []

def hook_fn(module, input, output):
    features.append(output)

model.layer4.register_forward_hook(hook_fn)  # ë§ˆì§€ë§‰ conv block
```  

- hook_fn : ëª¨ë¸ ë‚´ì— ë°ì´í„°ë¥¼ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. module ì€ layer ê°ì²´, inputì€  ì…ë ¥íŠœí”Œ, outputì€ ì¶œë ¥ í…ì„œì…ë‹ˆë‹¤!
- model.layer4.register_forward_hook(hook_fn) : ëª¨ë¸ì˜ layer4ì— hook_fnì„ ë°°ì¹˜, convì˜ layer4ì˜ ê²°ê³¼ë¬¼ì„ features ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ë˜ë„ë¡í•©ë‹ˆë‹¤.

ìœ„ì˜ hook_fní•¨ìˆ˜ë¥¼ ëª¨ë¸ layer4 ë’·ë‹¨ì— ì— ë°°ì¹˜ì‹œí‚¨ ë’¤  
ë‹¨ìˆœ ë¶„ë¥˜ëª¨ë¸ê³¼ ë™ì¼í•˜ê²Œ ëª¨ë¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

ì´ì œ [CNN(resnet) ë§ˆì§€ë§‰ conv ì¶œë ¥] ì„ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤!!

| ë‹¨ê³„ | ë°ì´í„° í˜•íƒœ        | ì„¤ëª…                                      |
|------|--------------------|-------------------------------------------|
| ğŸ”„ CNN(resnet) ë§ˆì§€ë§‰ conv ì¶œë ¥ | `[512, 7, 7]`     | 512ê°œì˜ 7Ã—7 feature map                     |

```python
# âœ… Get weights from the final linear layer
params = list(model.parameters())
fc_weights = params[-2]  # shape: [1000, 512]
class_weights = fc_weights[pred_class].detach().cpu().numpy()  # [512]

# âœ… Get feature map from hook
feature_map = features[0].squeeze(0).detach().cpu().numpy()  # [512, 7, 7]
```

ì´ë¥¼ í†µí•´ì„œ [512,7,7] ì‚¬ì´ì¦ˆì˜ feature mapì„ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤!

##### feature mapê³¼ class_weight ê³±í•´ì„œ, CAM ë§Œë“¤ê¸°!!

ì´ì  , ì´ feature mapìœ¼ë¡œ CAMì´ë¯¸ì§€ë¥¼ ë§Œë“œëŠ” ê³¼ì •ì…ë‹ˆë‹¤!  

| ë‹¨ê³„ | ë°ì´í„° í˜•íƒœ        | ì„¤ëª…                                      |
|------|--------------------|-------------------------------------------|
| ğŸ”¥ CAM ê³„ì‚° : CNN(resnet) ë§ˆì§€ë§‰ conv ì¶œë ¥ ê³¼  **class_weight**ì˜ weighted sum | `[7, 7]`     | 7Ã—7 feature map                     |
| ğŸ”¼ ìµœì¢… CAM ì´ë¯¸ì§€ ë§Œë“¤ê¸° (Upsample)       | `[224, 224]`      | ì›ë³¸ ì´ë¯¸ì§€ ìœ„ì— íˆíŠ¸ë§µ overlay ê°€ëŠ¥        |

```python
# âœ… Compute CAM
cam = np.zeros((7, 7), dtype=np.float32)
for i in range(len(class_weights)):
    cam += class_weights[i] * feature_map[i]

cam = np.maximum(cam, 0)
cam = cam - np.min(cam)
cam = cam / np.max(cam)
cam = cv2.resize(cam, (224, 224))
```
ìœ„ ê³¼ì •ì—ì„œ CNN(resnet) ë§ˆì§€ë§‰ conv ì¶œë ¥ class_weightsì™€ [512, 7,7] feature mapê³¼ì˜ weight sumì„ êµ¬í•˜ì—¬ [7,7]ì‚¬ì´ì¦ˆì˜ cam ì„ êµ¬í•©ë‹ˆë‹¤!!
ê·¸ë¦¬ê³  resize, ì¦‰ Upsampleì„ í†µí•´ì„œ ìµœì¢… heatmapì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤!

##### heat map ë§Œë“¤ê¸°!~!

ë§ˆì§€ë§‰ìœ¼ë¡œ heatmapì´ë¯¸ì§€ë¥¼ ê¸°ì¡´ ì´ë¯¸ì§€ì™€ ê²¹ì³ì„œ ì‹œê°í™” í•©ë‹ˆë‹¤!  
```python
# âœ… Overlay CAM on original image
img_cv = np.array(transforms.Resize((224, 224))(img))[:, :, ::-1]  # PIL â†’ OpenCV BGR
overlay = cv2.addWeighted(img_cv, 0.5, heatmap, 0.5, 0)

# âœ… Show
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title("Original Image")
plt.imshow(img)
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title(f"CAM: {imagenet_classes[pred_class]}")
plt.imshow(overlay[:, :, ::-1])  # Back to RGB
plt.axis('off')
plt.tight_layout()
plt.show()
```
ê·¸ëŸ¼!! ì§€ë‚œ í¬ìŠ¤íŒ…ì—ì„œ ë´¤ë˜ ë°”ë¡œ ì•„ë˜ ì´ë¯¸ì§€ê°€ ë‚˜ì˜¤ê²Œë©ë‹ˆë‹¤~!

![golden](https://github.com/user-attachments/assets/d3fd65b1-11bd-44dc-9516-3118fd586bf0)

##### CAMPëª¨ë¸ë¡œ ë¶„ë¥˜í•˜ê¸°!~!

ì´ì— ë”í•´ì„œ!!
CAMì—ì„œë„ êµ¬ë¶„ì„ í•  ìˆ˜ ìˆë‹¤!!!
GAPê³¼ FC layerë¥¼ í†µê³¼, softmaxë¥¼ êµ¬í•˜ê²Œ ë˜ë©´ ê²°ê³¼ êµ¬ë¶„ì˜ ì •í™•ë„ë„ ë³¼ìˆ˜ ìˆëŠ”ë°ìš”~!


| ë‹¨ê³„ | ë°ì´í„° í˜•íƒœ        | ì„¤ëª…                                      |
|------|--------------------|-------------------------------------------|
| ğŸ“‰ GAP(Global Average Pooling) | `[512]`             | feature map[512, 7, 7]ì˜ ì±„ë„ë³„ í‰ê·  ë²¡í„°               |
| ğŸ§® FC Layer               | `[N_classes]`       | GAP ê²°ê³¼ë¥¼ í´ë˜ìŠ¤ë³„ scoreë¡œ ë³€í™˜              |
| ğŸ¯ Softmax               | `[N_classes]`       | ì˜ˆì¸¡ í´ë˜ìŠ¤ í™•ë¥ ê°’ ì¶œë ¥                        |

```python
# âœ… GAP ì—°ì‚°: [512, 7, 7] â†’ [512]
gap_vector = feature_map.mean(axis=(1, 2))  # shape: [512]

# âœ… FC ì—°ì‚°: [512] Ã— [1000, 512]^T â†’ [1000]
logits = np.dot(fc_weights.detach().cpu().numpy(), gap_vector)  # shape: [1000]

# âœ… Softmax
exp_logits = np.exp(logits - np.max(logits))  # numerical stability
probs = exp_logits / exp_logits.sum()

# âœ… ì˜ˆì¸¡ í´ë˜ìŠ¤
gap_pred_class = np.argmax(probs)
gap_pred_label = imagenet_classes[gap_pred_class]

# âœ… ê²°ê³¼ ë¹„êµ
print("\nâœ… GAP â†’ FC â†’ Softmax ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼")
print(f"Predicted label: {gap_pred_label}")
print("ğŸ¦´ Is it a dog?", "âœ… Yes" if gap_pred_class in dog_classes else "âŒ No")

```

ìœ„ì˜ ê³¼ì •ì„ ê±¸ì¹˜ë©´!?
**Predicted label: golden retriever**
ë¼ëŠ” ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!!

ì •ë§ì¼ê¹Œ?
```python
probs[205:210]
```
ë¥¼ ë³´ë©´ ì •ë§ë¡œ 207ë²ˆì¨°ì˜ ê°’ì´ ê°€ì¥ í° ê°’ì´ì£ !?
í•˜ì§€ë§Œ!! ê¸°ì¡´ ë¶„ë¥˜ë²¡í„°ì˜ ê°’ 13.7348ê³¼ëŠ” ë‹¤ë¦„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤!!

```python
[1.7553568e-02, 6.1262056e-04, 8.4515899e-01, 6.3063554e-02, 6.3092457e-03]
```

ì˜¤ëŠ˜ì˜ ê³¼ì •ì„ í†µí•´ì„œ CAMì˜ ì„¸ë¶€ ë™ì‘ì„ ì˜ ì•Œì•„ë³¼ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤!!

<details>
  <summary>ì „ì²´ ì½”ë“œ ë³´ê¸°</summary>

```python
import torch
from torchvision import models, transforms
from PIL import Image
import requests
from io import BytesIO
import numpy as np
import matplotlib.pyplot as plt
import numpy as np
import cv2
import json

imagenet_url = "https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
imagenet_classes = requests.get(imagenet_url).text.splitlines()

# âœ… í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸° (ì˜ˆ: ì¸í„°ë„· ì´ë¯¸ì§€)
img_url = "https://images.unsplash.com/photo-1558788353-f76d92427f16"  # ê°•ì•„ì§€ ì‚¬ì§„
response = requests.get(img_url)
img = Image.open(BytesIO(response.content)).convert("RGB")

# âœ… ì‚¬ì „ í•™ìŠµëœ ResNet18 ë¡œë“œ (í•™ìŠµ ì—†ì´ inference)
model = models.resnet18(pretrained=True)
model.eval()

# âœ… ê°•ì•„ì§€ í´ë˜ìŠ¤ë“¤ ì¶”ë ¤ë‚´ê¸° (ê°„ë‹¨í•œ ë°©ë²•: ì´ë¦„ì— 'golden retriever' í¬í•¨ëœ ê²ƒ)
dog_classes = [i for i, name in enumerate(imagenet_classes) if 'golden retriever' in name.lower()]

# âœ… ì´ë¯¸ì§€ ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
])
input_tensor = transform(img).unsqueeze(0)  # shape: [1, 3, 224, 224]

# âœ… ì¶”ë¡ 
with torch.no_grad():
    output = model(input_tensor)
    pred_class = output.argmax(dim=1).item()

# âœ… ê²°ê³¼ í™•ì¸
pred_label = imagenet_classes[pred_class]
is_dog = pred_class in dog_classes

print(f"Predicted label: {pred_label}")
print("ğŸ¦´ Is it a dog?", "âœ… Yes" if is_dog else "âŒ No")


# âœ… Load pretrained ResNet18
model = models.resnet18(pretrained=True)
model.eval()

# âœ… Hook to get final conv feature map
features = []

def hook_fn(module, input, output):
    features.append(output)

model.layer4.register_forward_hook(hook_fn)  # ë§ˆì§€ë§‰ conv block

# âœ… Predict
with torch.no_grad():
    output = model(input_tensor)
    pred_class = output.argmax(dim=1).item()

# âœ… Get weights from the final linear layer
params = list(model.parameters())
fc_weights = params[-2]  # shape: [1000, 512]
class_weights = fc_weights[pred_class].detach().cpu().numpy()  # [512]

# âœ… Get feature map from hook
feature_map = features[0].squeeze(0).detach().cpu().numpy()  # [512, 7, 7]

# âœ… Compute CAM
cam = np.zeros((7, 7), dtype=np.float32)
for i in range(len(class_weights)):
    cam += class_weights[i] * feature_map[i]

# Normalize & resize
cam = np.maximum(cam, 0)
cam = cam - np.min(cam)
cam = cam / np.max(cam)
cam = cv2.resize(cam, (224, 224))
heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)

# âœ… Overlay CAM on original image
img_cv = np.array(transforms.Resize((224, 224))(img))[:, :, ::-1]  # PIL â†’ OpenCV BGR
overlay = cv2.addWeighted(img_cv, 0.5, heatmap, 0.5, 0)

# âœ… Show
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title("Original Image")
plt.imshow(img)
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title(f"CAM: {imagenet_classes[pred_class]}")
plt.imshow(overlay[:, :, ::-1])  # Back to RGB
plt.axis('off')
plt.tight_layout()
plt.show()

# âœ… ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶œë ¥
print(f"Predicted label: {imagenet_classes[pred_class]}")
print("ğŸ¦´ Is it a dog?", "âœ… Yes" if pred_class in dog_classes else "âŒ No")
```
</details>