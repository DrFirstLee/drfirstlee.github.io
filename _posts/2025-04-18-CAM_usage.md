---
layout: post
title: "Studying CAM with Python! - ÌååÏù¥Ïç¨ÏúºÎ°ú CAM Í≥µÎ∂ÄÌïòÍ∏∞ "
author: [DrFirst]
date: 2025-04-18 09:00:00 +0900
categories: [AI, Experiment]
tags: [CAM, Vision AI, GAP, CVPR, CVPR2016, XAI, Class Activation Map, Python, Ïã§Ïäµ, experiments]
lastmod : 2025-04-18 09:00:00
sitemap :
  changefreq : weekly
  priority : 0.9
---
## (English) Studying CAM with Python

Today, we will delve into CAM (Class Activation Map) in detail using Python code!!
Before we begin, the necessary packages are as follows!!
Don't worry, it's all possible with CPU without a GPU~!^^

```python
import torch
from torchvision import models, transforms
from PIL import Image
import requests
from io import BytesIO
import matplotlib.pyplot as plt
import numpy as np
import cv2
```

CAM basically starts with image classification!
Today, we aim to create a CAM image based on a ResNet classification model trained on ImageNet!!!

### What is ImageNet!?
- Contains over 14 million images and is categorized into approximately 20,000+ noun hierarchical structures (based on WordNet).
- Made a significant contribution to the development of deep learning in the field of computer vision.
- ResNet is also trained based on this ImageNet data!!

### What is ResNet?
- Innovative model in the vision field: An important structure that greatly improved image recognition performance!! - Announced in 2015 by MS Research!!
- Overcame the difficulty of training deep neural networks with residual connections.
- Residual connections: Prevents gradient vanishing by adding the learned changes to the input!!
- Enables the formation of truly deep networks (DNNs): Effective learning is possible even in deep layers!

### Code Start!!

#### Preparing Related Data and Models
```python
# ‚úÖ Loading ImageNet class index (for dog class identification)
import json
imagenet_url = "https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
imagenet_classes = requests.get(imagenet_url).text.splitlines()

# ‚úÖ Loading test image (e.g., from the internet)
img_url = "https://images.unsplash.com/photo-1558788353-f76d92427f16"  # Dog photo
response = requests.get(img_url)
img = Image.open(BytesIO(response.content)).convert("RGB")

# ‚úÖ Loading pre-trained ResNet18 (inference without training)
model = models.resnet18(pretrained=True)
model.eval()
```

Through the above process, we load the ImageNet class data, the dog photo, and finally the pre-trained ResNet18 model!
You can also see the model structure through `eval` as shown below~~
We will explore the detailed structure of the model in the next ResNet study!

<details>
  <summary>View ResNet Detailed Structure</summary>
```
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)
```
</details>

#### Preprocessing Start!!
```python
# ‚úÖ Extracting dog classes (simple method: names containing 'golden retriever')
dog_classes = [i for i, name in enumerate(imagenet_classes) if 'golden retriever' in name.lower()]

# ‚úÖ Image preprocessing
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
])
input_tensor = transform(img).unsqueeze(0)  # shape: [1, 3, 224, 224]
```
Now all preparations are complete!! Let's put the data into the model and perform inference!!

#### Simple Classification Model!! (Fully Connected Layer + Softmax)
```python
# ‚úÖ Load pre-trained ResNet18 (inference without training)
model = models.resnet18(pretrained=True)
model.eval()

# ‚úÖ Inference
with torch.no_grad():
    output = model(input_tensor)
    pred_class = output.argmax(dim=1).item()

# ‚úÖ Result check
pred_label = imagenet_classes[pred_class]
is_dog = pred_class in dog_classes

print(f"Predicted label: {pred_label}")
print("ü¶¥ Is it a dog?", "‚úÖ Yes" if is_dog else "‚ùå No")
```

Through the above code, you can see 'whether it is simply classified as a dog!'
This was the method of the classification model before CAM~~

If you actually examine the output vector `output`,
```python
output[0][205:210]
```
You can see that the value at index 207 is indeed the largest at 13.7348!
(Since 207 came out from `arg.max(dim=1)`, it is indeed the highest value, right!?)

```
tensor([ 9.8655,  6.4875, 13.7348, 11.1263,  8.8567])
```
#### CAM Start!!!!
Let's refer back to the CAM structure we summarized in the previous post.

| Step | Data Shape        | Description                                                  |
|------|-------------------|--------------------------------------------------------------|
| üì∑ Input Image        | `[3, 224, 224]`   | RGB Image                                                    |
| üîÑ CNN(resnet) Last conv Output | `[512, 7, 7]`     | 512 7√ó7 feature maps                                       |
| üî• CAM Calculation: Weighted sum of CNN(resnet) last conv output and **class_weight** | `[7, 7]`        | 7√ó7 feature map                                          |
| üîº Final CAM Image Creation (Upsample) | `[224, 224]`      | Heatmap overlay possible on the original image           |
| üìâ GAP (Global Average Pooling) | `[512]`           | Channel-wise average vector of feature map `[512, 7, 7]` |
| üßÆ FC Layer            | `[N_classes]`     | Converts GAP result to class scores                            |
| üéØ Softmax              | `[N_classes]`     | Outputs predicted class probability values                     |

Load the model again, just like before!
```python
# ‚úÖ Load pretrained ResNet18
model = models.resnet18(pretrained=True)
model.eval()
```

##### feature map Extraction!  

Now, the important part begins! It's as follows!!
```python
features = []

def hook_fn(module, input, output):
    features.append(output)

model.layer4.register_forward_hook(hook_fn)  # Last conv block
```
- `hook_fn`: A function to call data within the model. `module` is the layer object, `input` is the input tuple, and `output` is the output tensor!
- `model.layer4.register_forward_hook(hook_fn)`: Places `hook_fn` at the end of `model.layer4`, so that the output of conv layer4 is stored in the `features` list.

After placing the `hook_fn` function at the end of the model's `layer4`,
execute the model in the same way as the simple classification model.

Now, let's proceed with the [CNN(resnet) Last conv Output]!

| Step | Data Shape        | Description                                                  |
|------|-------------------|--------------------------------------------------------------|
| üîÑ CNN(resnet) Last conv Output | `[512, 7, 7]`     | 512 7√ó7 feature maps                                       |
```python
# ‚úÖ Get weights from the final linear layer
params = list(model.parameters())
fc_weights = params[-2]  # shape: [1000, 512]
class_weights = fc_weights[pred_class].detach().cpu().numpy()  # [512]

# ‚úÖ Get feature map from hook
feature_map = features[0].squeeze(0).detach().cpu().numpy()  # [512, 7, 7]
```
Through this, we have extracted the `[512, 7, 7]` size feature map!

##### create CAM!!!

Now, this is the process of creating a CAM image from this feature map!

| Step | Data Shape        | Description                                                  |
|------|-------------------|--------------------------------------------------------------|
| üî• CAM Calculation: Weighted sum of CNN(resnet) last conv output and **class_weight** | `[7, 7]`        | 7√ó7 feature map                                          |
| üîº Final CAM Image Creation (Upsample) | `[224, 224]`      | Heatmap overlay possible on the original image           |
```python
# ‚úÖ Compute CAM
cam = np.zeros((7, 7), dtype=np.float32)
for i in range(len(class_weights)):
    cam += class_weights[i] * feature_map[i]

cam = np.maximum(cam, 0)
cam = cam - np.min(cam)
cam = cam / np.max(cam)
cam = cv2.resize(cam, (224, 224))
heatmap = cv2.applyColorMap(np.uint8(255 *cam), cv2.COLORMAP_JET)
```
In the above process, we obtain a `[7, 7]` size CAM by calculating the weighted sum of the `class_weights` and the `[512, 7, 7]` feature map from the last conv output of ResNet!
Then, we create the final heatmap image through resizing, i.e., Upsampling!

Finally, we overlay the heatmap image on the original image for visualization!
```python
# ‚úÖ Overlay CAM on original image
img_cv = np.array(transforms.Resize((224, 224))(img))[:, :, ::-1]  # PIL ‚Üí OpenCV BGR
overlay = cv2.addWeighted(img_cv, 0.5, heatmap, 0.5, 0)

# ‚úÖ Show
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title("Original Image")
plt.imshow(img)
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title(f"CAM: {imagenet_classes[pred_class]}")
plt.imshow(overlay[:, :, ::-1])  # Back to RGB
plt.axis('off')
plt.tight_layout()
plt.show()
```
Then!! You will get the image shown directly below, which we saw in the previous post~!

![golden](https://github.com/user-attachments/assets/d3fd65b1-11bd-44dc-9516-3118fd586bf0)

##### image classification using CAM!

In addition to this!!
We can also distinguish within the CAM!!!
By passing through GAP and the FC layer, and calculating the softmax, we can also see the accuracy of the result distinction~!

| Step | Data Shape        | Description                                                  |
|------|-------------------|--------------------------------------------------------------|
| üìâ GAP (Global Average Pooling) | `[512]`           | Channel-wise average vector of feature map `[512, 7, 7]` |
| üßÆ FC Layer            | `[N_classes]`     | Converts GAP result to class scores                            |
| üéØ Softmax              | `[N_classes]`     | Outputs predicted class probability values                     |
```python
# ‚úÖ GAP operation: [512, 7, 7] ‚Üí [512]
gap_vector = feature_map.mean(axis=(1, 2))  # shape: [512]

# ‚úÖ FC operation: [512] √ó [1000, 512]^T ‚Üí [1000]
logits = np.dot(fc_weights.detach().cpu().numpy(), gap_vector)  # shape: [1000]

# ‚úÖ Softmax
exp_logits = np.exp(logits - np.max(logits))  # numerical stability
probs = exp_logits / exp_logits.sum()

# ‚úÖ Predicted class
gap_pred_class = np.argmax(probs)
gap_pred_label = imagenet_classes[gap_pred_class]

# ‚úÖ Result comparison
print("\n‚úÖ GAP ‚Üí FC ‚Üí Softmax based prediction result")
print(f"Predicted label: {gap_pred_label}")
print("ü¶¥ Is it a dog?", "‚úÖ Yes" if gap_pred_class in dog_classes else "‚ùå No")
```

After going through the above process!?
You can confirm the result:
**Predicted label: golden retriever**

Is it real?
```python
probs[205:210]
```
Looking at this, the value at index 207 is indeed the largest, right!?
However!! You can see that it is different from the original classification vector value of 13.7348!!

```
[1.7553568e-02, 6.1262056e-04, 8.4515899e-01, 6.3063554e-02, 6.3092457e-03]
```

Through today's process, we were able to understand the detailed operation of CAM well!!

<details>
  <summary>View Full Code</summary>
```python
import torch
from torchvision import models, transforms
from PIL import Image
import requests
from io import BytesIO
import numpy as np
import matplotlib.pyplot as plt
import numpy as np
import cv2
import json

imagenet_url = "https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
imagenet_classes = requests.get(imagenet_url).text.splitlines()

# ‚úÖ Loading test image (e.g., from the internet)
img_url = "https://images.unsplash.com/photo-1558788353-f76d92427f16"  # Dog photo
response = requests.get(img_url)
img = Image.open(BytesIO(response.content)).convert("RGB")

# ‚úÖ Loading pre-trained ResNet18 (inference without training)
model = models.resnet18(pretrained=True)
model.eval()

# ‚úÖ Extracting dog classes (simple method: names containing 'golden retriever')
dog_classes = [i for i, name in enumerate(imagenet_classes) if 'golden retriever' in name.lower()]

# ‚úÖ Image preprocessing
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
])
input_tensor = transform(img).unsqueeze(0)  # shape: [1, 3, 224, 224]

# ‚úÖ Inference
with torch.no_grad():
    output = model(input_tensor)
    pred_class = output.argmax(dim=1).item()

# ‚úÖ Result check
pred_label = imagenet_classes[pred_class]
is_dog = pred_class in dog_classes

print(f"Predicted label: {pred_label}")
print("ü¶¥ Is it a dog?", "‚úÖ Yes" if is_dog else "‚ùå No")


# ‚úÖ Load pretrained ResNet18
model = models.resnet18(pretrained=True)
model.eval()

# ‚úÖ Hook to get final conv feature map
features = []

def hook_fn(module, input, output):
    features.append(output)

model.layer4.register_forward_hook(hook_fn)  # Last conv block

# ‚úÖ Predict
with torch.no_grad():
    output = model(input_tensor)
    pred_class = output.argmax(dim=1).item()

# ‚úÖ Get weights from the final linear layer
params = list(model.parameters())
fc_weights = params[-2]  # shape: [1000, 512]
class_weights = fc_weights[pred_class].detach().cpu().numpy()  # [512]

# ‚úÖ Get feature map from hook
feature_map = features[0].squeeze(0).detach().cpu().numpy()  # [512, 7, 7]

# ‚úÖ Compute CAM
cam = np.zeros((7, 7), dtype=np.float32)
for i in range(len(class_weights)):
    cam += class_weights[i] * feature_map[i]

# Normalize & resize
cam = np.maximum(cam, 0)
cam = cam - np.min(cam)
cam = cam / np.max(cam)
cam = cv2.resize(cam, (224, 224))
heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)

# ‚úÖ Overlay CAM on original image
img_cv = np.array(transforms.Resize((224, 224))(img))[:, :, ::-1]  # PIL ‚Üí OpenCV BGR
overlay = cv2.addWeighted(img_cv, 0.5, heatmap, 0.5, 0)

# ‚úÖ Show
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title("Original Image")
plt.imshow(img)
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title(f"CAM: {imagenet_classes[pred_class]}")
plt.imshow(overlay[:, :, ::-1])  # Back to RGB
plt.axis('off')
plt.tight_layout()
plt.show()

# ‚úÖ Result text output
print(f"Predicted label: {imagenet_classes[pred_class]}")
print("ü¶¥ Is it a dog?", "‚úÖ Yes" if pred_class in dog_classes else "‚ùå No")
```
</details>

---

## (ÌïúÍµ≠Ïñ¥)  ÌååÏù¥Ïç¨ÏúºÎ°ú CAM Í≥µÎ∂ÄÌïòÍ∏∞ 

Ïò§ÎäòÏùÄ Python ÏΩîÎìúÎ°ú CAM(Class Activation Map) Ïóê ÎåÄÌïòÏó¨ ÏûêÏÑ∏Ìûà ÏïåÏïÑÎ≥¥Í≤†ÏäµÎãàÎã§!!
ÏïåÏïÑÎ≥¥Í∏∞Ïóê ÏïûÏÑú ÌïÑÏöîÌïú Ìå®ÌÇ§ÏßÄÎì§ÏùÄ ÏïÑÎûòÏôÄ Í∞ôÏäµÎãàÎã§!!
GPU ÏóÜÏù¥!! CPU Î°úÎèÑ Î™®Îëê Í∞ÄÎä•ÌïòÎãà Í±±Ï†ïÎßàÏÑ∏Ïöî~!^^

```python
import torch
from torchvision import models, transforms
from PIL import Image
import requests
from io import BytesIO
import matplotlib.pyplot as plt
import numpy as np
import cv2
```

CAMÎèÑ Í∏∞Î≥∏Ï†ÅÏúºÎ°ú Ïù¥ÎØ∏ÏßÄ Î∂ÑÎ•òÏóêÏÑú ÏãúÏûëÎê©ÎãàÎã§!
Ïò§ÎäòÏùÄ imagenetÏúºÎ°ú ÌïôÏäµÎêú resnetÏùò Î∂ÑÎ•ò Î™®Îç∏ÏùÑ Î∞îÌÉïÏúºÎ°ú CAMÏù¥ÎØ∏ÏßÄÎ•º ÎßåÎì§Ïñ¥ Î≥¥Í≥†ÏûêÌï©ÎãàÎã§!!!

### imagenetÏù¥ÎûÄ!?
!(image_net)[]
- ÏïΩ 1,400Îßå Í∞úÍ∞Ä ÎÑòÎäî Ïù¥ÎØ∏ÏßÄÎ•º Ìè¨Ìï®ÌïòÍ≥† ÏûàÏúºÎ©∞, ÏïΩ 2Îßå Í∞ú Ïù¥ÏÉÅÏùò Î™ÖÏÇ¨ Í≥ÑÏ∏µ Íµ¨Ï°∞ (WordNet Í∏∞Î∞ò)Î°ú Ïù¥Î£®Ïñ¥ÏßÑ Ïπ¥ÌÖåÍ≥†Î¶¨Î°ú Î∂ÑÎ•ò  
- Ïª¥Ìì®ÌÑ∞ ÎπÑÏ†Ñ Î∂ÑÏïºÏùò Îî•Îü¨Îãù Î∞úÏ†ÑÏóê ÏßÄÎåÄÌïú Í≥µÌóåÏùÑ Ìï®
- resnetÎèÑ Ïù¥ imagenetÎç∞Ïù¥ÌÑ∞Î•º Î∞îÌÉïÏúºÎ°ú ÌïôÏäµÌï®!!   

### resnetÏù¥ÎûÄ?
!(resnet)[]

- ÎπÑÏ†Ñ Î∂ÑÏïº ÌòÅÏã† Î™®Îç∏: Ïù¥ÎØ∏ÏßÄ Ïù∏Ïãù ÏÑ±Îä•ÏùÑ ÌÅ¨Í≤å Ìñ•ÏÉÅÏãúÌÇ® Ï§ëÏöîÌïú Íµ¨Ï°∞!! -2015ÎÖÑ MS researchÏóêÏÑú Î∞úÌëú!!
- ÏûîÏ∞® Ïó∞Í≤∞(Residual connections)Î°ú ÍπäÏùÄ Ïã†Í≤ΩÎßù ÌïôÏäµ Ïñ¥Î†§ÏõÄÏùÑ Í∑πÎ≥µ  
- ÏûîÏ∞® Ïó∞Í≤∞(Residual connections): ÏûÖÎ†•Ïóê ÌïôÏäµÎêú Î≥ÄÌôîÎüâÏùÑ ÎçîÌï¥ Í∏∞Ïö∏Í∏∞ ÏÜåÏã§ÏùÑ ÎßâÏùå!!
- ÏßÑÏßú ÍπäÏùÄ ÎÑ§Ìä∏ÏõåÌÅ¨(DNN) ÌòïÏÑ± Í∞ÄÎä•: ÍπäÏùÄ Ï∏µÏóêÏÑúÎèÑ Ìö®Í≥ºÏ†ÅÏù∏ ÌïôÏäµÏù¥ Í∞ÄÎä•Ìï¥Ïßê!

### ÏΩîÎìú ÏãúÏûë!!  

#### Í¥ÄÎ†® Îç∞Ïù¥ÌÑ∞ Î∞è Î™®Îç∏ Ï§ÄÎπÑ  
```python
# ‚úÖ ImageNet class index Î°úÎî© (Í∞ïÏïÑÏßÄ ÌÅ¥ÎûòÏä§ Íµ¨Î∂ÑÏö©)
import json
imagenet_url = "https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
imagenet_classes = requests.get(imagenet_url).text.splitlines()

# ‚úÖ ÌÖåÏä§Ìä∏ Ïù¥ÎØ∏ÏßÄ Î∂àÎü¨Ïò§Í∏∞ (Ïòà: Ïù∏ÌÑ∞ÎÑ∑ Ïù¥ÎØ∏ÏßÄ)
img_url = "https://images.unsplash.com/photo-1558788353-f76d92427f16"  # Í∞ïÏïÑÏßÄ ÏÇ¨ÏßÑ
response = requests.get(img_url)
img = Image.open(BytesIO(response.content)).convert("RGB")

# ‚úÖ ÏÇ¨Ï†Ñ ÌïôÏäµÎêú ResNet18 Î°úÎìú (ÌïôÏäµ ÏóÜÏù¥ inference)
model = models.resnet18(pretrained=True)
model.eval()
```

ÏúÑÏùò Í≥ºÏ†ïÏùÑ ÌÜµÌï¥ÏÑú, imagenetÏùò ÌÅ¥ÎûòÏä§ Îç∞Ïù¥ÌÑ∞ÎèÑ Î∞õÏïÑÏò§Í≥†, Í∞ïÏïÑÏßÄ ÏÇ¨ÏßÑÎèÑ Î∞õÏïÑÏò§Í≥†! ÎßàÏßÄÎßâÏúºÎ°ú resnet18Ïùò ÏÇ¨Ï†ÑÌïôÏäµÎêú Î™®Îç∏ÎèÑ Î∂àÎü¨Ïò§Í≤åÎê©ÎãàÎã§!  
evalÏùÑ ÌÜµÌï¥ÏÑú Î™®Îç∏ÎèÑ ÏïÑÎûòÏôÄ Í∞ôÏù¥ Î≥ºÏàò ÏûàÏßÄÏöî~~  
Î™®Îç∏Ïùò ÏÑ∏Î∂ÄÍµ¨Ï°∞Îäî!? Îã§Ïùå resnet Í≥µÎ∂ÄÏóêÏÑú ÏïåÏïÑÎ≥¥Í≤†ÏäµÎãàÎã§!  

<details>
  <summary>resnet ÏÑ∏Î∂Ä Íµ¨Ï°∞ Î≥¥Í∏∞</summary>

```
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)
```
</details>


#### Ï†ÑÏ≤òÎ¶¨ ÏãúÏûë!!  

```python
# ‚úÖ Í∞ïÏïÑÏßÄ ÌÅ¥ÎûòÏä§Îì§ Ï∂îÎ†§ÎÇ¥Í∏∞ (Í∞ÑÎã®Ìïú Î∞©Î≤ï: Ïù¥Î¶ÑÏóê 'golden retriever' Ìè¨Ìï®Îêú Í≤É)
dog_classes = [i for i, name in enumerate(imagenet_classes) if 'golden retriever' in name.lower()]

# ‚úÖ Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
])
input_tensor = transform(img).unsqueeze(0)  # shape: [1, 3, 224, 224]
```
Ïù¥Ï†ú Î™®Îëî Ï§ÄÎπÑÍ∞Ä ÎÅùÎÇ¨ÏäµÎãàÎã§!! Î™®Îç∏Ïóê Îç∞Ïù¥ÌÑ∞Î•º ÎÑ£Ïñ¥ÏÑú Ï∂îÎ°†Ìï¥Î≥¥ÏïÑÏöî!!

#### Îã®Ïàú Î∂ÑÎ•òÎ™®Îç∏!! (Fully Connected Layer + Softmax)
```python
# ‚úÖ ÏÇ¨Ï†Ñ ÌïôÏäµÎêú ResNet18 Î°úÎìú (ÌïôÏäµ ÏóÜÏù¥ inference)
model = models.resnet18(pretrained=True)
model.eval()

# ‚úÖ Ï∂îÎ°†
with torch.no_grad():
    output = model(input_tensor)
    pred_class = output.argmax(dim=1).item()

# ‚úÖ Í≤∞Í≥º ÌôïÏù∏
pred_label = imagenet_classes[pred_class]
is_dog = pred_class in dog_classes

print(f"Predicted label: {pred_label}")
print("ü¶¥ Is it a dog?", "‚úÖ Yes" if is_dog else "‚ùå No")
```

ÏúÑ ÏΩîÎìúÎ•º ÌÜµÌï¥ÏÑú 'Îã®ÏàúÌûà Í∞ïÏïÑÏßÄÎ°ú Î∂ÑÎ•òÌïòÎäîÍ∞Ä!' Ïóê ÎåÄÌïòÏó¨ ÏïåÏïÑÎ≥ºÏàò ÏûàÏäµÎãàÎã§. 
CAMÏù¥Ï†Ñ Î∂ÑÎ•òÎ™®Îç∏Ïùò Î∞©ÏãùÏù¥ÏóàÏßÄÏöî~~

Ïã§Ï†úÎ°ú Í≤∞Í≥ºÍ∞í Î≤°ÌÑ∞ outputÏùÑ Ï°∞ÏÇ¨Ìï¥Î≥¥Î©¥!
```python
output[0][205:210]
```
Ï†ïÎßêÎ°ú 207Î≤àÏ®∞Ïùò Í∞íÏù¥ 13.7348Î°ú ÌÅ∞ Í∞íÏûÑÏùÑ ÏïåÏàò ÏûàÏäµÎãàÎã§!
(arg.max(dim=1)ÏóêÏÑú 207Ïù¥ ÎÇòÏôîÏúºÎãàÏµúÍ≥†Í∞íÏù¥ ÎßûÎäîÍ≤É ÏïÑÏãúÏ£†!?)

```python
tensor([ 9.8655,  6.4875, 13.7348, 11.1263,  8.8567])
```
#### CAM ÏãúÏûë!!!! 
ÏßÄÎÇú Ìè¨Ïä§ÌåÖÏóêÏÑú Ï†ïÎ¶¨Ìï¥Î≥∏ CAMÏùò Íµ¨Ï°∞Î•º Îã§Ïãú Ï∞∏Í≥†Ìï¥Î¥ÖÎãàÎã§

| Îã®Í≥Ñ | Îç∞Ïù¥ÌÑ∞ ÌòïÌÉú        | ÏÑ§Î™Ö                                      |
|------|--------------------|-------------------------------------------|
| üì∑ ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄ       | `[3, 224, 224]`   | RGB Ïù¥ÎØ∏ÏßÄ                                 |
| üîÑ CNN(resnet) ÎßàÏßÄÎßâ conv Ï∂úÎ†• | `[512, 7, 7]`     | 512Í∞úÏùò 7√ó7 feature map                     |
| üî• CAM Í≥ÑÏÇ∞ : CNN(resnet) ÎßàÏßÄÎßâ conv Ï∂úÎ†• Í≥º  **class_weight**Ïùò weighted sum | `[7, 7]`     | 7√ó7 feature map                     |
| üîº ÏµúÏ¢Ö CAM Ïù¥ÎØ∏ÏßÄ ÎßåÎì§Í∏∞ (Upsample)       | `[224, 224]`      | ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ ÏúÑÏóê ÌûàÌä∏Îßµ overlay Í∞ÄÎä•        |
| üìâ GAP(Global Average Pooling) | `[512]`             | feature map[512, 7, 7]Ïùò Ï±ÑÎÑêÎ≥Ñ ÌèâÍ∑† Î≤°ÌÑ∞               |
| üßÆ FC Layer               | `[N_classes]`       | GAP Í≤∞Í≥ºÎ•º ÌÅ¥ÎûòÏä§Î≥Ñ scoreÎ°ú Î≥ÄÌôò              |
| üéØ Softmax               | `[N_classes]`       | ÏòàÏ∏° ÌÅ¥ÎûòÏä§ ÌôïÎ•†Í∞í Ï∂úÎ†•                        |


Ï†ÑÍ≥º ÎèôÏùºÌïòÍ≤å Î™®Îç∏ÏùÑ Î∂àÎü¨ÏôÄ Ï§çÎãàÎã§!

```python
# ‚úÖ Load pretrained ResNet18
model = models.resnet18(pretrained=True)
model.eval()
```

##### feature map Ï∂îÏ∂úÌïòÍ∏∞!!
ÏßÄÍ∏àÎ∂ÄÌÑ∞ Ï§ëÏöîÌïú Î∂ÄÎ∂ÑÏù¥ ÏãúÏûëÎê©ÎÇòÎã§! ÏïÑÎûòÏôÄ Í∞ôÏäµÎãàÎã§!! 

```python
features = []

def hook_fn(module, input, output):
    features.append(output)

model.layer4.register_forward_hook(hook_fn)  # ÎßàÏßÄÎßâ conv block
```  

- hook_fn : Î™®Îç∏ ÎÇ¥Ïóê Îç∞Ïù¥ÌÑ∞Î•º Ìò∏Ï∂úÌïòÎäî Ìï®ÏàòÏûÖÎãàÎã§. module ÏùÄ layer Í∞ùÏ≤¥, inputÏùÄ  ÏûÖÎ†•ÌäúÌîå, outputÏùÄ Ï∂úÎ†• ÌÖêÏÑúÏûÖÎãàÎã§!
- model.layer4.register_forward_hook(hook_fn) : Î™®Îç∏Ïùò layer4Ïóê hook_fnÏùÑ Î∞∞Ïπò, convÏùò layer4Ïùò Í≤∞Í≥ºÎ¨ºÏùÑ features Î¶¨Ïä§Ìä∏Ïóê Ï†ÄÏû•ÎêòÎèÑÎ°ùÌï©ÎãàÎã§.

ÏúÑÏùò hook_fnÌï®ÏàòÎ•º Î™®Îç∏ layer4 Îí∑Îã®Ïóê Ïóê Î∞∞ÏπòÏãúÌÇ® Îí§  
Îã®Ïàú Î∂ÑÎ•òÎ™®Îç∏Í≥º ÎèôÏùºÌïòÍ≤å Î™®Îç∏ÏùÑ Ïã§ÌñâÌï©ÎãàÎã§.

Ïù¥Ï†ú [CNN(resnet) ÎßàÏßÄÎßâ conv Ï∂úÎ†•] ÏùÑ ÏßÑÌñâÌï¥Î≥¥Í≤†ÏäµÎãàÎã§!!

| Îã®Í≥Ñ | Îç∞Ïù¥ÌÑ∞ ÌòïÌÉú        | ÏÑ§Î™Ö                                      |
|------|--------------------|-------------------------------------------|
| üîÑ CNN(resnet) ÎßàÏßÄÎßâ conv Ï∂úÎ†• | `[512, 7, 7]`     | 512Í∞úÏùò 7√ó7 feature map                     |

```python
# ‚úÖ Get weights from the final linear layer
params = list(model.parameters())
fc_weights = params[-2]  # shape: [1000, 512]
class_weights = fc_weights[pred_class].detach().cpu().numpy()  # [512]

# ‚úÖ Get feature map from hook
feature_map = features[0].squeeze(0).detach().cpu().numpy()  # [512, 7, 7]
```

Ïù¥Î•º ÌÜµÌï¥ÏÑú [512,7,7] ÏÇ¨Ïù¥Ï¶àÏùò feature mapÏùÑ Ï∂îÏ∂úÌñàÏäµÎãàÎã§!

##### feature mapÍ≥º class_weight Í≥±Ìï¥ÏÑú, CAM ÎßåÎì§Í∏∞!!

Ïù¥Ï††, Ïù¥ feature mapÏúºÎ°ú CAMÏù¥ÎØ∏ÏßÄÎ•º ÎßåÎìúÎäî Í≥ºÏ†ïÏûÖÎãàÎã§!  

| Îã®Í≥Ñ | Îç∞Ïù¥ÌÑ∞ ÌòïÌÉú        | ÏÑ§Î™Ö                                      |
|------|--------------------|-------------------------------------------|
| üî• CAM Í≥ÑÏÇ∞ : CNN(resnet) ÎßàÏßÄÎßâ conv Ï∂úÎ†• Í≥º  **class_weight**Ïùò weighted sum | `[7, 7]`     | 7√ó7 feature map                     |
| üîº ÏµúÏ¢Ö CAM Ïù¥ÎØ∏ÏßÄ ÎßåÎì§Í∏∞ (Upsample)       | `[224, 224]`      | ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ ÏúÑÏóê ÌûàÌä∏Îßµ overlay Í∞ÄÎä•        |

```python
# ‚úÖ Compute CAM
cam = np.zeros((7, 7), dtype=np.float32)
for i in range(len(class_weights)):
    cam += class_weights[i] * feature_map[i]

cam = np.maximum(cam, 0)
cam = cam - np.min(cam)
cam = cam / np.max(cam)
cam = cv2.resize(cam, (224, 224))
```
ÏúÑ Í≥ºÏ†ïÏóêÏÑú CNN(resnet) ÎßàÏßÄÎßâ conv Ï∂úÎ†• class_weightsÏôÄ [512, 7,7] feature mapÍ≥ºÏùò weight sumÏùÑ Íµ¨ÌïòÏó¨ [7,7]ÏÇ¨Ïù¥Ï¶àÏùò cam ÏùÑ Íµ¨Ìï©ÎãàÎã§!!
Í∑∏Î¶¨Í≥† resize, Ï¶â UpsampleÏùÑ ÌÜµÌï¥ÏÑú ÏµúÏ¢Ö heatmapÏù¥ÎØ∏ÏßÄÎ•º ÎßåÎì§Ïñ¥Ï§çÎãàÎã§!

##### heat map ÎßåÎì§Í∏∞!~!

ÎßàÏßÄÎßâÏúºÎ°ú heatmapÏù¥ÎØ∏ÏßÄÎ•º Í∏∞Ï°¥ Ïù¥ÎØ∏ÏßÄÏôÄ Í≤πÏ≥êÏÑú ÏãúÍ∞ÅÌôî Ìï©ÎãàÎã§!  
```python
# ‚úÖ Overlay CAM on original image
img_cv = np.array(transforms.Resize((224, 224))(img))[:, :, ::-1]  # PIL ‚Üí OpenCV BGR
overlay = cv2.addWeighted(img_cv, 0.5, heatmap, 0.5, 0)

# ‚úÖ Show
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title("Original Image")
plt.imshow(img)
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title(f"CAM: {imagenet_classes[pred_class]}")
plt.imshow(overlay[:, :, ::-1])  # Back to RGB
plt.axis('off')
plt.tight_layout()
plt.show()
```
Í∑∏Îüº!! ÏßÄÎÇú Ìè¨Ïä§ÌåÖÏóêÏÑú Î¥§Îçò Î∞îÎ°ú ÏïÑÎûò Ïù¥ÎØ∏ÏßÄÍ∞Ä ÎÇòÏò§Í≤åÎê©ÎãàÎã§~!

![golden](https://github.com/user-attachments/assets/d3fd65b1-11bd-44dc-9516-3118fd586bf0)

##### CAMPÎ™®Îç∏Î°ú Î∂ÑÎ•òÌïòÍ∏∞!~!

Ïù¥Ïóê ÎçîÌï¥ÏÑú!!
CAMÏóêÏÑúÎèÑ Íµ¨Î∂ÑÏùÑ Ìï† Ïàò ÏûàÎã§!!!
GAPÍ≥º FC layerÎ•º ÌÜµÍ≥º, softmaxÎ•º Íµ¨ÌïòÍ≤å ÎêòÎ©¥ Í≤∞Í≥º Íµ¨Î∂ÑÏùò Ï†ïÌôïÎèÑÎèÑ Î≥ºÏàò ÏûàÎäîÎç∞Ïöî~!


| Îã®Í≥Ñ | Îç∞Ïù¥ÌÑ∞ ÌòïÌÉú        | ÏÑ§Î™Ö                                      |
|------|--------------------|-------------------------------------------|
| üìâ GAP(Global Average Pooling) | `[512]`             | feature map[512, 7, 7]Ïùò Ï±ÑÎÑêÎ≥Ñ ÌèâÍ∑† Î≤°ÌÑ∞               |
| üßÆ FC Layer               | `[N_classes]`       | GAP Í≤∞Í≥ºÎ•º ÌÅ¥ÎûòÏä§Î≥Ñ scoreÎ°ú Î≥ÄÌôò              |
| üéØ Softmax               | `[N_classes]`       | ÏòàÏ∏° ÌÅ¥ÎûòÏä§ ÌôïÎ•†Í∞í Ï∂úÎ†•                        |

```python
# ‚úÖ GAP Ïó∞ÏÇ∞: [512, 7, 7] ‚Üí [512]
gap_vector = feature_map.mean(axis=(1, 2))  # shape: [512]

# ‚úÖ FC Ïó∞ÏÇ∞: [512] √ó [1000, 512]^T ‚Üí [1000]
logits = np.dot(fc_weights.detach().cpu().numpy(), gap_vector)  # shape: [1000]

# ‚úÖ Softmax
exp_logits = np.exp(logits - np.max(logits))  # numerical stability
probs = exp_logits / exp_logits.sum()

# ‚úÖ ÏòàÏ∏° ÌÅ¥ÎûòÏä§
gap_pred_class = np.argmax(probs)
gap_pred_label = imagenet_classes[gap_pred_class]

# ‚úÖ Í≤∞Í≥º ÎπÑÍµê
print("\n‚úÖ GAP ‚Üí FC ‚Üí Softmax Í∏∞Î∞ò ÏòàÏ∏° Í≤∞Í≥º")
print(f"Predicted label: {gap_pred_label}")
print("ü¶¥ Is it a dog?", "‚úÖ Yes" if gap_pred_class in dog_classes else "‚ùå No")

```

ÏúÑÏùò Í≥ºÏ†ïÏùÑ Í±∏ÏπòÎ©¥!?
**Predicted label: golden retriever**
ÎùºÎäî Í≤∞Í≥ºÎ•º ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§!!

Ï†ïÎßêÏùºÍπå?
```python
probs[205:210]
```
Î•º Î≥¥Î©¥ Ï†ïÎßêÎ°ú 207Î≤àÏ®∞Ïùò Í∞íÏù¥ Í∞ÄÏû• ÌÅ∞ Í∞íÏù¥Ï£†!?
ÌïòÏßÄÎßå!! Í∏∞Ï°¥ Î∂ÑÎ•òÎ≤°ÌÑ∞Ïùò Í∞í 13.7348Í≥ºÎäî Îã§Î¶ÑÏùÑ Ïïå Ïàò ÏûàÏäµÎãàÎã§!!

```python
[1.7553568e-02, 6.1262056e-04, 8.4515899e-01, 6.3063554e-02, 6.3092457e-03]
```

Ïò§ÎäòÏùò Í≥ºÏ†ïÏùÑ ÌÜµÌï¥ÏÑú CAMÏùò ÏÑ∏Î∂Ä ÎèôÏûëÏùÑ Ïûò ÏïåÏïÑÎ≥ºÏàò ÏûàÏóàÏäµÎãàÎã§!!

<details>
  <summary>Ï†ÑÏ≤¥ ÏΩîÎìú Î≥¥Í∏∞</summary>

```python
import torch
from torchvision import models, transforms
from PIL import Image
import requests
from io import BytesIO
import numpy as np
import matplotlib.pyplot as plt
import numpy as np
import cv2
import json

imagenet_url = "https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
imagenet_classes = requests.get(imagenet_url).text.splitlines()

# ‚úÖ ÌÖåÏä§Ìä∏ Ïù¥ÎØ∏ÏßÄ Î∂àÎü¨Ïò§Í∏∞ (Ïòà: Ïù∏ÌÑ∞ÎÑ∑ Ïù¥ÎØ∏ÏßÄ)
img_url = "https://images.unsplash.com/photo-1558788353-f76d92427f16"  # Í∞ïÏïÑÏßÄ ÏÇ¨ÏßÑ
response = requests.get(img_url)
img = Image.open(BytesIO(response.content)).convert("RGB")

# ‚úÖ ÏÇ¨Ï†Ñ ÌïôÏäµÎêú ResNet18 Î°úÎìú (ÌïôÏäµ ÏóÜÏù¥ inference)
model = models.resnet18(pretrained=True)
model.eval()

# ‚úÖ Í∞ïÏïÑÏßÄ ÌÅ¥ÎûòÏä§Îì§ Ï∂îÎ†§ÎÇ¥Í∏∞ (Í∞ÑÎã®Ìïú Î∞©Î≤ï: Ïù¥Î¶ÑÏóê 'golden retriever' Ìè¨Ìï®Îêú Í≤É)
dog_classes = [i for i, name in enumerate(imagenet_classes) if 'golden retriever' in name.lower()]

# ‚úÖ Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
])
input_tensor = transform(img).unsqueeze(0)  # shape: [1, 3, 224, 224]

# ‚úÖ Ï∂îÎ°†
with torch.no_grad():
    output = model(input_tensor)
    pred_class = output.argmax(dim=1).item()

# ‚úÖ Í≤∞Í≥º ÌôïÏù∏
pred_label = imagenet_classes[pred_class]
is_dog = pred_class in dog_classes

print(f"Predicted label: {pred_label}")
print("ü¶¥ Is it a dog?", "‚úÖ Yes" if is_dog else "‚ùå No")


# ‚úÖ Load pretrained ResNet18
model = models.resnet18(pretrained=True)
model.eval()

# ‚úÖ Hook to get final conv feature map
features = []

def hook_fn(module, input, output):
    features.append(output)

model.layer4.register_forward_hook(hook_fn)  # ÎßàÏßÄÎßâ conv block

# ‚úÖ Predict
with torch.no_grad():
    output = model(input_tensor)
    pred_class = output.argmax(dim=1).item()

# ‚úÖ Get weights from the final linear layer
params = list(model.parameters())
fc_weights = params[-2]  # shape: [1000, 512]
class_weights = fc_weights[pred_class].detach().cpu().numpy()  # [512]

# ‚úÖ Get feature map from hook
feature_map = features[0].squeeze(0).detach().cpu().numpy()  # [512, 7, 7]

# ‚úÖ Compute CAM
cam = np.zeros((7, 7), dtype=np.float32)
for i in range(len(class_weights)):
    cam += class_weights[i] * feature_map[i]

# Normalize & resize
cam = np.maximum(cam, 0)
cam = cam - np.min(cam)
cam = cam / np.max(cam)
cam = cv2.resize(cam, (224, 224))
heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)

# ‚úÖ Overlay CAM on original image
img_cv = np.array(transforms.Resize((224, 224))(img))[:, :, ::-1]  # PIL ‚Üí OpenCV BGR
overlay = cv2.addWeighted(img_cv, 0.5, heatmap, 0.5, 0)

# ‚úÖ Show
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title("Original Image")
plt.imshow(img)
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title(f"CAM: {imagenet_classes[pred_class]}")
plt.imshow(overlay[:, :, ::-1])  # Back to RGB
plt.axis('off')
plt.tight_layout()
plt.show()

# ‚úÖ Í≤∞Í≥º ÌÖçÏä§Ìä∏ Ï∂úÎ†•
print(f"Predicted label: {imagenet_classes[pred_class]}")
print("ü¶¥ Is it a dog?", "‚úÖ Yes" if pred_class in dog_classes else "‚ùå No")
```
</details>