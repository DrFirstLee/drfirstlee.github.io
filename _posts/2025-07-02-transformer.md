---
layout: post
title: "ğŸš€ Transformer íŒŒì´ì¬ìœ¼ë¡œ ì´í•´í•˜ê¸°!"
author: [DrFirst]
date: 2025-07-02 12:00:00 +0900
categories: [AI, Experiment]
tags: [transformer, attention, python, pytorch, MHCA]
sitemap :
  changefreq : monthly
  priority : 0.8
---


# ğŸš€ **Transformer íŒŒì´ì¬ìœ¼ë¡œ ì™„ì „ì •ë³µ!**

> **"Attention is All You Need"** - 2017ë…„ êµ¬ê¸€ì˜ í˜ëª…ì ì¸ ë…¼ë¬¸ ğŸ¯  
> ì´ì œ ìš°ë¦¬ë„ Transformerë¥¼ ì²˜ìŒë¶€í„° ëê¹Œì§€ íŒŒì´ì¬ìœ¼ë¡œ êµ¬í˜„í•´ë³´ì! ğŸ’ª

---

## ğŸ¯ **ëª©ì°¨**

1. [ğŸ” Transformer ê°œìš”](#transformer-ê°œìš”)
2. [ğŸ§  Self-Attentionì˜ ê¸°ë³¸ ì›ë¦¬](#self-attentionì˜-ê¸°ë³¸-ì›ë¦¬)
3. [ğŸª Single-Head Attention êµ¬í˜„](#single-head-attention-êµ¬í˜„)
4. [ğŸ­ Multi-Head Attention êµ¬í˜„](#multi-head-attention-êµ¬í˜„)
5. [ğŸ—ï¸ Transformer ë¸”ë¡ êµ¬í˜„](#transformer-ë¸”ë¡-êµ¬í˜„)
6. [ğŸ¨ Multi-Head Cross Attention](#multi-head-cross-attention)
7. [ğŸš€ ì‹¤ì „ í™œìš© ì˜ˆì‹œ](#ì‹¤ì „-í™œìš©-ì˜ˆì‹œ)
8. [âš¡ ì„±ëŠ¥ ìµœì í™” íŒ](#ì„±ëŠ¥-ìµœì í™”-íŒ)

---

## ğŸ” **Transformer ê°œìš”**

### **ğŸ¤” Transformerê°€ ë­”ê°€ìš”?**

TransformerëŠ” 2017ë…„ êµ¬ê¸€ì—ì„œ ë°œí‘œí•œ **"Attention is All You Need"** ë…¼ë¬¸ì—ì„œ ì²˜ìŒ ì†Œê°œëœ neural network ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤!

### **ğŸ“š Attentionì˜ ì—­ì‚¬** 

**Attention ê°œë… ìì²´ëŠ” Transformer ë“±ì¥ ì „ë¶€í„° ì´ë¯¸ ìˆì—ˆì–´ìš”!** ğŸ•°ï¸

#### **2015ë…„ ì´ì „: ì´ˆê¸° Attention** ğŸ”
```python
# ì˜ˆì‹œ: Seq2Seq with Attention (2015ë…„)
class OldAttention(nn.Module):
    """ê¸°ì¡´ì˜ RNN + Attention ë°©ì‹"""
    def __init__(self):
        super().__init__()
        self.encoder_rnn = nn.LSTM(input_size, hidden_size)
        self.decoder_rnn = nn.LSTM(input_size, hidden_size) 
        self.attention = nn.Linear(hidden_size * 2, 1)  # ê°„ë‹¨í•œ attention
    
    def forward(self, encoder_outputs, decoder_hidden):
        # ëª¨ë“  encoder stateì™€ í˜„ì¬ decoder state ë¹„êµ
        attention_scores = []
        for encoder_output in encoder_outputs:
            # Concatenate and score
            combined = torch.cat([encoder_output, decoder_hidden], dim=1)
            score = self.attention(combined)
            attention_scores.append(score)
        
        # Softmaxë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°
        attention_weights = F.softmax(torch.stack(attention_scores), dim=0)
        
        # Weighted sum of encoder outputs
        context = torch.sum(attention_weights.unsqueeze(-1) * encoder_outputs, dim=0)
        return context, attention_weights
```

**ì´ˆê¸° Attentionì˜ íŠ¹ì§•**:
- ğŸ”„ **RNNê³¼ í•¨ê»˜ ì‚¬ìš©**: LSTM/GRU encoder-decoderì™€ ê²°í•©
- ğŸ“ **ë‹¨ë°©í–¥**: Decoderê°€ encoderë¥¼ "ë³´ëŠ”" ìš©ë„ (Cross Attention)
- ğŸ¯ **ë²ˆì—­ ë¬¸ì œ í•´ê²°**: ê¸´ ë¬¸ì¥ì—ì„œ ì •ë³´ ì†ì‹¤ ë°©ì§€
- **ğŸª Single-Headë§Œ ì¡´ì¬**: Multi-Head ê°œë…ì€ ì•„ì§ ì—†ì—ˆìŒ!
- ğŸ‘¥ **ëŒ€í‘œì  ì—°êµ¬**: Bahdanau (2015), Luong (2015)

#### **ğŸ’¡ Single-Head Cross Attention (SHCA) ì‹œëŒ€** 
```python
# 2015ë…„ ìŠ¤íƒ€ì¼: Single-Head Cross Attention
class SingleHeadCrossAttention_2015(nn.Module):
    """Transformer ì´ì „ì˜ SHCA ë°©ì‹"""
    def __init__(self, encoder_dim, decoder_dim):
        super().__init__()
        # ë‹¨ í•˜ë‚˜ì˜ attention headë§Œ ì¡´ì¬!
        self.attention_layer = nn.Linear(encoder_dim + decoder_dim, 1)
        
    def forward(self, encoder_outputs, decoder_hidden):
        batch_size, seq_len, encoder_dim = encoder_outputs.size()
        
        # ëª¨ë“  encoder outputê³¼ í˜„ì¬ decoder state ê²°í•©
        attention_scores = []
        for i in range(seq_len):
            # Concatenate encoder output with decoder hidden
            combined = torch.cat([
                encoder_outputs[:, i, :],  # i-th encoder output
                decoder_hidden.squeeze(0)   # current decoder state
            ], dim=1)
            
            # Single attention score ê³„ì‚°
            score = self.attention_layer(combined)  # ì˜¤ì§ í•˜ë‚˜ì˜ ì ìˆ˜!
            attention_scores.append(score)
        
        # Softmax normalization
        attention_weights = F.softmax(torch.stack(attention_scores, dim=1), dim=1)
        
        # Weighted sum (context vector)
        context = torch.sum(
            attention_weights.unsqueeze(-1) * encoder_outputs, 
            dim=1
        )
        
        return context, attention_weights

# ì‹¤ì œ 2015ë…„ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ëœ ë°©ì‹
def bahdanau_attention_2015(encoder_outputs, decoder_hidden):
    """Bahdanau et al. (2015) ë°©ì‹"""
    # íŒŒë¼ë¯¸í„°ë“¤
    W_a = nn.Linear(hidden_size, hidden_size)  # encoder projection
    U_a = nn.Linear(hidden_size, hidden_size)  # decoder projection  
    v_a = nn.Linear(hidden_size, 1)           # final scoring layer
    
    # Attention computation
    seq_len = encoder_outputs.size(1)
    scores = []
    
    for i in range(seq_len):
        # Additive attention (not dot-product!)
        score = v_a(torch.tanh(
            W_a(encoder_outputs[:, i, :]) +  # encoder contribution
            U_a(decoder_hidden)              # decoder contribution
        ))
        scores.append(score)
    
    # Single attention distribution
    attention_weights = F.softmax(torch.cat(scores, dim=1), dim=1)
    
    # Context vector
    context = torch.sum(
        attention_weights.unsqueeze(-1) * encoder_outputs, 
        dim=1
    )
    
    return context, attention_weights
```

**ğŸ¯ SHCAì˜ í•œê³„ë“¤**:
- **ë‹¨ì¼ ê´€ì **: ì˜¤ì§ í•˜ë‚˜ì˜ attention patternë§Œ í•™ìŠµ
- **ì œí•œëœ í‘œí˜„ë ¥**: ë³µì¡í•œ ê´€ê³„ í¬ì°© ì–´ë ¤ì›€  
- **ì •ë³´ ë³‘ëª©**: ëª¨ë“  ì •ë³´ê°€ í•˜ë‚˜ì˜ context vectorë¡œ ì••ì¶•
- **ë‹¤ì–‘ì„± ë¶€ì¡±**: ë¬¸ë²•ì , ì˜ë¯¸ì  ê´€ê³„ë¥¼ ë™ì‹œì— í•™ìŠµí•˜ê¸° ì–´ë ¤ì›€

#### **2017ë…„: Transformerì˜ í˜ì‹ ** âš¡

**ğŸ­ Multi-Head Attention (MHCA) ë“±ì¥!**

```python
# Transformerì˜ Multi-Head Self-Attention (2017ë…„)
class MultiHeadSelfAttention_2017(nn.Module):
    """í˜ì‹ ì ì¸ Multi-Head Self-Attention"""
    def __init__(self, d_model, n_heads=8):
        super().__init__()
        self.n_heads = n_heads
        self.d_k = d_model // n_heads  # ê° headì˜ ì°¨ì›
        
        # ì—¬ëŸ¬ ê°œì˜ attention headë¥¼ ìœ„í•œ projection!
        self.W_q = nn.Linear(d_model, d_model)  # 8ê°œ head ë™ì‹œì—
        self.W_k = nn.Linear(d_model, d_model)  # 8ê°œ head ë™ì‹œì—  
        self.W_v = nn.Linear(d_model, d_model)  # 8ê°œ head ë™ì‹œì—
        self.W_o = nn.Linear(d_model, d_model)  # output projection
        
    def forward(self, x):
        batch_size, seq_len, d_model = x.size()
        
        # Q, K, Vë¥¼ multiple headsë¡œ ë¶„í• 
        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  
        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        
        # ê° headì—ì„œ ë…ë¦½ì ìœ¼ë¡œ attention ê³„ì‚°!
        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        attention_weights = F.softmax(attention_scores, dim=-1)
        
        # Multi-head attention ì ìš©
        context = torch.matmul(attention_weights, V)
        
        # Concatenate heads
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len, d_model
        )
        
        # Final output projection
        output = self.W_o(context)
        
        return output, attention_weights

# ë¹„êµ: Single-Head vs Multi-Head
def compare_attention_mechanisms():
    """SHCA vs MHCA ë¹„êµ"""
    
    # Single-Head (2015ë…„ ë°©ì‹)
    single_head_output = single_attention_head(x)  # 1ê°œ ê´€ì 
    
    # Multi-Head (2017ë…„ ë°©ì‹) 
    multi_head_output = []
    for head in range(8):  # 8ê°œ ë‹¤ë¥¸ ê´€ì !
        head_output = attention_head(x, head_id=head)
        multi_head_output.append(head_output)
    
    # 8ê°œ headì˜ ê²°ê³¼ë¥¼ ê²°í•©
    combined_output = concatenate_and_project(multi_head_output)
    
    return combined_output
```

### **ğŸš€ SHCA â†’ MHCA í˜ëª…ì  ë³€í™”**

| **SHCA (2015ë…„)** | **MHCA (2017ë…„)** |
|-------------------|-------------------|
| ğŸ¯ **1ê°œ ê´€ì ** | ğŸ­ **8ê°œ ê´€ì ** |
| ğŸ“ **Cross Attentionë§Œ** | ğŸ”„ **Self + Cross** |
| ğŸ”„ **RNN ì˜ì¡´** | ğŸš« **RNN ì œê±°** |
| ğŸŒ **ìˆœì°¨ ì²˜ë¦¬** | âš¡ **ë³‘ë ¬ ì²˜ë¦¬** |
| ğŸ“Š **ë‹¨ìˆœ ê°€ì¤‘í•©** | ğŸ§  **ë³µí•© í‘œí˜„** |

#### **ğŸª Multi-Headì˜ ë§ˆë²•ì  íš¨ê³¼**

ê° headê°€ **ì„œë¡œ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ê´€ê³„**ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤:

```python
# ì˜ˆì‹œ: "The cat sat on the mat" ë¶„ì„
sentence = "The cat sat on the mat"

# Head 1: ë¬¸ë²•ì  ê´€ê³„ í•™ìŠµ
head_1_attention = [
    # "cat" â†’ "The" (ê´€ì‚¬-ëª…ì‚¬ ê´€ê³„)
    # "sat" â†’ "cat" (ì£¼ì–´-ë™ì‚¬ ê´€ê³„)  
    # "on" â†’ "sat" (ë™ì‚¬-ì „ì¹˜ì‚¬ ê´€ê³„)
]

# Head 2: ì˜ë¯¸ì  ê´€ê³„ í•™ìŠµ  
head_2_attention = [
    # "cat" â†’ "mat" (ê³ ì–‘ì´ê°€ ë§¤íŠ¸ì™€ ê´€ë ¨)
    # "sat" â†’ "on" (ì•‰ëŠ” ë™ì‘ê³¼ ìœ„ì¹˜)
]

# Head 3: ìœ„ì¹˜ì  ê´€ê³„ í•™ìŠµ
head_3_attention = [
    # ì¸ì ‘í•œ ë‹¨ì–´ë“¤ ê°„ì˜ ê´€ê³„
    # "The" â†’ "cat", "cat" â†’ "sat" ë“±
]

# Head 4-8: ë‹¤ë¥¸ ì¶”ìƒì  ê´€ê³„ë“¤...
```

**ğŸ”¥ Multi-Headì˜ í˜ì‹ ì  ì¥ì **:
- **ğŸ¯ ë‹¤ì–‘í•œ ê´€ì **: ë¬¸ë²•, ì˜ë¯¸, ìœ„ì¹˜ ë“± ë™ì‹œ í•™ìŠµ
- **ğŸ§  í’ë¶€í•œ í‘œí˜„**: ë³µì¡í•œ ì–¸ì–´ íŒ¨í„´ í¬ì°©  
- **âš¡ ë³‘ë ¬ ê³„ì‚°**: ëª¨ë“  headê°€ ë™ì‹œì— ì²˜ë¦¬
- **ğŸš€ ì„±ëŠ¥ í–¥ìƒ**: ì‹¤ì œë¡œ ë²ˆì—­/ì´í•´ ì„±ëŠ¥ ëŒ€í­ ê°œì„ 

**ğŸ’¡ ê²°ê³¼**: Single-Headì˜ í•œê³„ë¥¼ ì™„ì „íˆ ê·¹ë³µ! ğŸ‰

### **ğŸ”¥ Transformerì˜ 3ëŒ€ í˜ì‹ **

| ê¸°ì¡´ Attention (2015) | Transformer Attention (2017) |
|---------------------|----------------------------|
| ğŸ”„ **RNN í•„ìˆ˜** | ğŸš« **RNN ì—†ìŒ** |
| ğŸ“ **Encoderâ†’Decoderë§Œ** | ğŸ”„ **Self-Attention** |
| ğŸ¯ **ë‹¨ì¼ Head** | ğŸ­ **Multi-Head** |
| ğŸŒ **ìˆœì°¨ ì²˜ë¦¬** | âš¡ **ë³‘ë ¬ ì²˜ë¦¬** |


**í˜ì‹  í¬ì¸íŠ¸**:
1. **ğŸ§  Self-Attention**: ê°™ì€ ì‹œí€€ìŠ¤ ë‚´ì—ì„œ ëª¨ë“  ìœ„ì¹˜ê°€ ì„œë¡œ ê´€ê³„ í•™ìŠµ
2. **ğŸ­ Multi-Head**: ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— attention ê³„ì‚°  
3. **âš¡ ë³‘ë ¬í™”**: RNN ì—†ì´ë„ ì‹œí€€ìŠ¤ ì²˜ë¦¬ ê°€ëŠ¥

### **ğŸ“ˆ ë°œì „ ê³¼ì • ìš”ì•½**

```mermaid
timeline
    title Attention ë°œì „ì‚¬
    2014 : Neural Machine Translation
         : Encoder-Decoder ë“±ì¥
    2015 : Bahdanau Attention
         : ì²« ë²ˆì§¸ Attention ë©”ì»¤ë‹ˆì¦˜
         : Luong Attention
    2017 : Transformer
         : "Attention is All You Need"
         : Self-Attention í˜ëª…
    2018+ : BERT, GPT ë“±ì¥
          : Transformer ê¸°ë°˜ ëª¨ë¸ë“¤
```

**ğŸ’¡ ê²°ë¡ **: Attentionì€ ê¸°ì¡´ì— ìˆë˜ ê°œë…ì´ì§€ë§Œ, **Transformerê°€ ì™„ì „íˆ ìƒˆë¡œìš´ ë ˆë²¨ë¡œ ëŒì–´ì˜¬ë ¸ìŠµë‹ˆë‹¤!** ğŸš€

**í•µì‹¬ ì•„ì´ë””ì–´**: 
- ğŸš« **RNN/LSTM ì—†ì´ë„** ì‹œí€€ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥
- âš¡ **ë³‘ë ¬ì²˜ë¦¬** ê°€ëŠ¥ìœ¼ë¡œ í•™ìŠµ ì†ë„ ëŒ€í­ í–¥ìƒ
- ğŸ¯ **Self-Attention** ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì¥ê±°ë¦¬ ì˜ì¡´ì„± í•´ê²°

### **ğŸŒŸ ì™œ í˜ëª…ì ì¸ê°€?**

| ê¸°ì¡´ ë°©ë²• (RNN/LSTM) | Transformer |
|---------------------|-------------|
| ğŸŒ ìˆœì°¨ì  ì²˜ë¦¬ (ëŠë¦¼) | âš¡ ë³‘ë ¬ ì²˜ë¦¬ (ë¹ ë¦„) |
| ğŸ˜µ ì¥ê±°ë¦¬ ì˜ì¡´ì„± ë¬¸ì œ | ğŸ¯ ì§ì ‘ì  ì—°ê²° |
| ğŸ”„ Gradient Vanishing | âœ… ì•ˆì •ì  í•™ìŠµ |

---

## ğŸ§  **Self-Attentionì˜ ê¸°ë³¸ ì›ë¦¬**

### **ğŸ’¡ í•µì‹¬ ê°œë…**

Self-Attentionì€ **"ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ìˆëŠ”ì§€"**ë¥¼ ê³„ì‚°í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤!

```python
# ì˜ˆì‹œ: "The cat sat on the mat"
# "cat"ì´ë¼ëŠ” ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ìˆì„ê¹Œ?
# cat -> The (0.1), cat (1.0), sat (0.8), on (0.2), the (0.1), mat (0.3)
```

### **ğŸ”‘ Query, Key, Value ê°œë…**

Think of it like a **search engine**! ğŸ”

- **Query (Q)**: "ë‚´ê°€ ì°¾ëŠ” ê²ƒ" - í˜„ì¬ ë‹¨ì–´ì˜ ê´€ì‹¬ì‚¬
- **Key (K)**: "ê²€ìƒ‰ í‚¤ì›Œë“œ" - ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì˜ íŠ¹ì„±
- **Value (V)**: "ì‹¤ì œ ë‚´ìš©" - ë‹¨ì–´ì˜ ì‹¤ì œ ì •ë³´

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

def simple_attention_example():
    """ê°„ë‹¨í•œ Attention ì˜ˆì‹œ"""
    
    # ì˜ˆì‹œ ë¬¸ì¥: "I love AI"
    # ê° ë‹¨ì–´ë¥¼ 3ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„
    sentence = torch.tensor([
        [1.0, 0.0, 0.0],  # "I"
        [0.0, 1.0, 0.0],  # "love"  
        [0.0, 0.0, 1.0]   # "AI"
    ])
    
    # Query, Key, Value ê³„ì‚° (ë‹¨ìˆœí™”)
    Q = sentence  # Query: ê° ë‹¨ì–´ê°€ ë¬´ì—‡ì„ ì°¾ê³  ìˆë‚˜?
    K = sentence  # Key: ê° ë‹¨ì–´ì˜ íŠ¹ì„±
    V = sentence  # Value: ê° ë‹¨ì–´ì˜ ì‹¤ì œ ì •ë³´
    
    # Attention Score ê³„ì‚°
    attention_scores = torch.matmul(Q, K.transpose(-2, -1))
    print("Attention Scores:")
    print(attention_scores)
    
    # Softmaxë¡œ í™•ë¥  ë³€í™˜
    attention_weights = F.softmax(attention_scores, dim=-1)
    print("\nAttention Weights:")
    print(attention_weights)
    
    # ìµœì¢… ì¶œë ¥
    output = torch.matmul(attention_weights, V)
    print("\nFinal Output:")
    print(output)

# ì‹¤í–‰
simple_attention_example()
```

---

## ğŸª **Single-Head Attention êµ¬í˜„**

ì´ì œ ì§„ì§œ Self-Attentionì„ êµ¬í˜„í•´ë³´ì! ğŸ‰

```python
class SingleHeadAttention(nn.Module):
    def __init__(self, d_model, d_k=None):
        super().__init__()
        self.d_model = d_model
        self.d_k = d_k or d_model
        
        # Linear layers for Q, K, V
        self.W_q = nn.Linear(d_model, self.d_k, bias=False)
        self.W_k = nn.Linear(d_model, self.d_k, bias=False)
        self.W_v = nn.Linear(d_model, self.d_k, bias=False)
        
        # Output projection
        self.W_o = nn.Linear(self.d_k, d_model, bias=False)
        
        # Scaling factor
        self.scale = math.sqrt(self.d_k)
        
    def forward(self, x, mask=None):
        """
        Args:
            x: Input tensor (batch_size, seq_len, d_model)
            mask: Optional mask tensor
        Returns:
            output: Attention output (batch_size, seq_len, d_model)
            attention_weights: Attention weights (batch_size, seq_len, seq_len)
        """
        batch_size, seq_len, _ = x.size()
        
        # 1. Linear projections
        Q = self.W_q(x)  # (batch_size, seq_len, d_k)
        K = self.W_k(x)  # (batch_size, seq_len, d_k)
        V = self.W_v(x)  # (batch_size, seq_len, d_k)
        
        # 2. Scaled dot-product attention
        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        
        # 3. Apply mask if provided
        if mask is not None:
            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)
        
        # 4. Apply softmax
        attention_weights = F.softmax(attention_scores, dim=-1)
        
        # 5. Apply attention to values
        context = torch.matmul(attention_weights, V)
        
        # 6. Output projection
        output = self.W_o(context)
        
        return output, attention_weights

# í…ŒìŠ¤íŠ¸í•´ë³´ê¸°! ğŸ§ª
def test_single_head_attention():
    """Single-Head Attention í…ŒìŠ¤íŠ¸"""
    
    # ëª¨ë¸ ìƒì„±
    d_model = 512
    attention = SingleHeadAttention(d_model)
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„± (ë°°ì¹˜ í¬ê¸° 2, ì‹œí€€ìŠ¤ ê¸¸ì´ 10)
    batch_size, seq_len = 2, 10
    x = torch.randn(batch_size, seq_len, d_model)
    
    # Forward pass
    output, weights = attention(x)
    
    print(f"Input shape: {x.shape}")
    print(f"Output shape: {output.shape}")
    print(f"Attention weights shape: {weights.shape}")
    
    # Attention íŒ¨í„´ ì‹œê°í™”
    import matplotlib.pyplot as plt
    
    plt.figure(figsize=(10, 8))
    plt.imshow(weights[0].detach().numpy(), cmap='Blues', aspect='auto')
    plt.colorbar()
    plt.title("Single-Head Attention Pattern")
    plt.xlabel("Key Position")
    plt.ylabel("Query Position")
    plt.show()

# ì‹¤í–‰
test_single_head_attention()
```

### **ğŸ” ì½”ë“œ í•´ì„**

1. **Query, Key, Value ìƒì„±**: ì…ë ¥ì„ 3ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ê´€ì ìœ¼ë¡œ ë³€í™˜
2. **Attention Score ê³„ì‚°**: Qì™€ Kì˜ ë‚´ì ìœ¼ë¡œ ìœ ì‚¬ë„ ì¸¡ì •
3. **Scaling**: âˆšd_kë¡œ ë‚˜ëˆ„ì–´ gradient ì•ˆì •í™”
4. **Softmax**: í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜
5. **Context ê³„ì‚°**: Attention ê°€ì¤‘ì¹˜ë¥¼ Valueì— ì ìš©

---

## ğŸ­ **Multi-Head Attention êµ¬í˜„**

ì´ì œ **ì—¬ëŸ¬ ê°œì˜ attention head**ë¥¼ ì‚¬ìš©í•´ë´…ì‹œë‹¤! ğŸª

### **ğŸ¤” ì™œ Multi-Headì¸ê°€?**

- ğŸ¯ **ë‹¤ì–‘í•œ ê´€ì **: ê° headê°€ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ê´€ê³„ë¥¼ í•™ìŠµ
- ğŸ§  **í’ë¶€í•œ í‘œí˜„**: ë” ë³µì¡í•œ íŒ¨í„´ í¬ì°© ê°€ëŠ¥
- ğŸš€ **ì„±ëŠ¥ í–¥ìƒ**: ì‹¤ì œë¡œ ë” ì¢‹ì€ ê²°ê³¼!

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        # Linear layers for Q, K, V
        self.W_q = nn.Linear(d_model, d_model, bias=False)
        self.W_k = nn.Linear(d_model, d_model, bias=False)
        self.W_v = nn.Linear(d_model, d_model, bias=False)
        
        # Output projection
        self.W_o = nn.Linear(d_model, d_model, bias=False)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # Scaling factor
        self.scale = math.sqrt(self.d_k)
        
    def forward(self, query, key, value, mask=None):
        """
        Args:
            query: Query tensor (batch_size, seq_len, d_model)
            key: Key tensor (batch_size, seq_len, d_model)
            value: Value tensor (batch_size, seq_len, d_model)
            mask: Optional mask tensor
        Returns:
            output: Multi-head attention output
            attention_weights: Average attention weights across heads
        """
        batch_size, seq_len = query.size(0), query.size(1)
        
        # 1. Linear projections and reshape for multi-head
        Q = self.W_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        
        # 2. Apply attention for each head
        attention_output, attention_weights = self.scaled_dot_product_attention(
            Q, K, V, mask, self.scale
        )
        
        # 3. Concatenate heads
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model
        )
        
        # 4. Final linear projection
        output = self.W_o(attention_output)
        
        return output, attention_weights
    
    def scaled_dot_product_attention(self, Q, K, V, mask, scale):
        """Scaled Dot-Product Attention"""
        # Attention scores
        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / scale
        
        # Apply mask if provided
        if mask is not None:
            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)
        
        # Softmax
        attention_weights = F.softmax(attention_scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Apply attention to values
        context = torch.matmul(attention_weights, V)
        
        return context, attention_weights

# ì‹¤ì „ ì˜ˆì‹œ! ğŸ¯
def demonstrate_multi_head_attention():
    """Multi-Head Attention ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ"""
    
    # ëª¨ë¸ ì„¤ì •
    d_model = 512
    n_heads = 8
    mha = MultiHeadAttention(d_model, n_heads)
    
    # ì˜ˆì‹œ ë¬¸ì¥ ìƒì„± (ì‹¤ì œë¡œëŠ” embedding layerì—ì„œ ë‚˜ì˜´)
    batch_size, seq_len = 2, 20
    x = torch.randn(batch_size, seq_len, d_model)
    
    # Self-Attention (query, key, value ëª¨ë‘ ë™ì¼)
    output, weights = mha(x, x, x)
    
    print(f"ğŸ¯ Multi-Head Attention Results:")
    print(f"Input shape: {x.shape}")
    print(f"Output shape: {output.shape}")
    print(f"Attention weights shape: {weights.shape}")
    
    # ê° headì˜ attention pattern ì‹œê°í™”
    visualize_attention_heads(weights[0], n_heads)

def visualize_attention_heads(attention_weights, n_heads):
    """Attention headë“¤ì˜ íŒ¨í„´ ì‹œê°í™”"""
    import matplotlib.pyplot as plt
    
    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
    axes = axes.flatten()
    
    for i in range(n_heads):
        im = axes[i].imshow(attention_weights[i].detach().numpy(), 
                           cmap='Blues', aspect='auto')
        axes[i].set_title(f'Head {i+1}')
        axes[i].set_xlabel('Key Position')
        axes[i].set_ylabel('Query Position')
        plt.colorbar(im, ax=axes[i])
    
    plt.tight_layout()
    plt.suptitle('Multi-Head Attention Patterns', fontsize=16, y=1.02)
    plt.show()

# ì‹¤í–‰
demonstrate_multi_head_attention()
```

### **ğŸª Multi-Headì˜ ë§ˆë²•**

ê° headëŠ” ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤:

- **Head 1**: ë¬¸ë²•ì  ê´€ê³„ (ì£¼ì–´-ë™ì‚¬)
- **Head 2**: ì˜ë¯¸ì  ê´€ê³„ (ë™ì˜ì–´, ë°˜ì˜ì–´)
- **Head 3**: ìœ„ì¹˜ì  ê´€ê³„ (ì¸ì ‘í•œ ë‹¨ì–´ë“¤)
- **Head 4**: ì¥ê±°ë¦¬ ì˜ì¡´ì„± (ë¬¸ì¥ ì²˜ìŒ-ë)

ğŸ¯ **ê²°ê³¼**: ë” í’ë¶€í•˜ê³  ì •í™•í•œ í‘œí˜„!

---

## ğŸ—ï¸ **Transformer ë¸”ë¡ êµ¬í˜„**

ì´ì œ ì™„ì „í•œ Transformer ë¸”ë¡ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤! ğŸ‰

```python
class TransformerBlock(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        
        # Multi-Head Attention
        self.attention = MultiHeadAttention(d_model, n_heads, dropout)
        
        # Feed-Forward Network
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        
        # Layer Normalization
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        """
        Args:
            x: Input tensor (batch_size, seq_len, d_model)
            mask: Optional attention mask
        Returns:
            output: Transformer block output
        """
        # 1. Multi-Head Attention + Residual Connection + Layer Norm
        attn_output, attn_weights = self.attention(x, x, x, mask)
        x = self.ln1(x + self.dropout(attn_output))
        
        # 2. Feed-Forward + Residual Connection + Layer Norm
        ff_output = self.feed_forward(x)
        x = self.ln2(x + self.dropout(ff_output))
        
        return x, attn_weights

# ì™„ì „í•œ Transformer ëª¨ë¸! ğŸš€
class SimpleTransformer(nn.Module):
    def __init__(self, vocab_size, d_model=512, n_heads=8, n_layers=6, 
                 d_ff=2048, max_seq_len=5000, dropout=0.1):
        super().__init__()
        
        self.d_model = d_model
        
        # Embedding layers
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_seq_len, d_model)
        
        # Transformer blocks
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])
        
        # Final layer norm
        self.ln_final = nn.LayerNorm(d_model)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        """
        Args:
            x: Input token ids (batch_size, seq_len)
            mask: Optional attention mask
        Returns:
            output: Transformer output (batch_size, seq_len, d_model)
            attention_weights: List of attention weights from each layer
        """
        batch_size, seq_len = x.size()
        
        # Token embeddings
        token_emb = self.token_embedding(x)
        
        # Position embeddings
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)
        pos_emb = self.position_embedding(positions)
        
        # Combine embeddings
        x = self.dropout(token_emb + pos_emb)
        
        # Apply transformer blocks
        attention_weights = []
        for block in self.transformer_blocks:
            x, attn_weights = block(x, mask)
            attention_weights.append(attn_weights)
        
        # Final layer norm
        x = self.ln_final(x)
        
        return x, attention_weights

# ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ! ğŸ¯
def test_transformer():
    """Transformer ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    
    # ëª¨ë¸ ìƒì„±
    vocab_size = 10000
    model = SimpleTransformer(vocab_size)
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„±
    batch_size, seq_len = 2, 50
    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
    
    # Forward pass
    output, attention_weights = model(input_ids)
    
    print(f"ğŸš€ Transformer Results:")
    print(f"Input shape: {input_ids.shape}")
    print(f"Output shape: {output.shape}")
    print(f"Number of layers: {len(attention_weights)}")
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params:,}")
    
    return model, output, attention_weights

# ì‹¤í–‰
model, output, attention_weights = test_transformer()
```

---

## ğŸ¨ **Multi-Head Cross Attention**

ë“œë””ì–´ **Cross Attention**! ğŸ‰ ì´ê±´ **ë‹¤ë¥¸ ì‹œí€€ìŠ¤ ê°„ì˜ ê´€ê³„**ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤!

### **ğŸ¤” ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?**

- ğŸŒ **ë²ˆì—­**: ì˜ì–´ ë¬¸ì¥ â†’ í•œêµ­ì–´ ë¬¸ì¥
- ğŸ–¼ï¸ **ì´ë¯¸ì§€ ìº¡ì…”ë‹**: ì´ë¯¸ì§€ â†’ í…ìŠ¤íŠ¸
- ğŸµ **ìŒì„± ì¸ì‹**: ì˜¤ë””ì˜¤ â†’ í…ìŠ¤íŠ¸

```python
class MultiHeadCrossAttention(nn.Module):
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        self.multihead_attention = MultiHeadAttention(d_model, n_heads, dropout)
        
    def forward(self, query, key, value, mask=None):
        """
        Cross Attention: QueryëŠ” target, Key/ValueëŠ” source
        
        Args:
            query: Target sequence (batch_size, tgt_len, d_model)
            key: Source sequence (batch_size, src_len, d_model)  
            value: Source sequence (batch_size, src_len, d_model)
            mask: Optional cross-attention mask
        Returns:
            output: Cross attention output
            attention_weights: Cross attention weights
        """
        return self.multihead_attention(query, key, value, mask)

# Encoder-Decoder with Cross Attention! ğŸª
class EncoderDecoderTransformer(nn.Module):
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, 
                 n_heads=8, n_layers=6, d_ff=2048, max_seq_len=5000, dropout=0.1):
        super().__init__()
        
        self.d_model = d_model
        
        # Encoder
        self.encoder = SimpleTransformer(src_vocab_size, d_model, n_heads, 
                                       n_layers, d_ff, max_seq_len, dropout)
        
        # Decoder with Cross Attention
        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)
        self.decoder_pos_embedding = nn.Embedding(max_seq_len, d_model)
        
        self.decoder_blocks = nn.ModuleList([
            DecoderBlock(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])
        
        self.ln_final = nn.LayerNorm(d_model)
        self.output_projection = nn.Linear(d_model, tgt_vocab_size)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        """
        Args:
            src: Source sequence (batch_size, src_len)
            tgt: Target sequence (batch_size, tgt_len)
            src_mask: Source attention mask
            tgt_mask: Target attention mask (causal mask)
        Returns:
            output: Decoder output (batch_size, tgt_len, tgt_vocab_size)
        """
        # Encode source
        encoder_output, _ = self.encoder(src, src_mask)
        
        # Decode target
        batch_size, tgt_len = tgt.size()
        
        # Target embeddings
        tgt_emb = self.decoder_embedding(tgt)
        positions = torch.arange(tgt_len, device=tgt.device).unsqueeze(0).expand(batch_size, -1)
        pos_emb = self.decoder_pos_embedding(positions)
        
        tgt_emb = self.dropout(tgt_emb + pos_emb)
        
        # Apply decoder blocks
        for block in self.decoder_blocks:
            tgt_emb = block(tgt_emb, encoder_output, tgt_mask, src_mask)
        
        # Final projection
        output = self.output_projection(self.ln_final(tgt_emb))
        
        return output

class DecoderBlock(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        
        # Self-Attention
        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)
        
        # Cross-Attention
        self.cross_attention = MultiHeadCrossAttention(d_model, n_heads, dropout)
        
        # Feed-Forward
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        
        # Layer Normalizations
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        self.ln3 = nn.LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, tgt, encoder_output, tgt_mask=None, src_mask=None):
        """
        Args:
            tgt: Target sequence embeddings
            encoder_output: Encoder output (source representation)
            tgt_mask: Target self-attention mask (causal)
            src_mask: Source attention mask
        """
        # 1. Self-Attention on target
        self_attn_output, _ = self.self_attention(tgt, tgt, tgt, tgt_mask)
        tgt = self.ln1(tgt + self.dropout(self_attn_output))
        
        # 2. Cross-Attention (target attends to source)
        cross_attn_output, _ = self.cross_attention(tgt, encoder_output, encoder_output, src_mask)
        tgt = self.ln2(tgt + self.dropout(cross_attn_output))
        
        # 3. Feed-Forward
        ff_output = self.feed_forward(tgt)
        tgt = self.ln3(tgt + self.dropout(ff_output))
        
        return tgt

# ì‹¤ì œ ë²ˆì—­ ì˜ˆì‹œ! ğŸŒ
def translation_example():
    """ë²ˆì—­ ëª¨ë¸ ì˜ˆì‹œ"""
    
    # ëª¨ë¸ ìƒì„±
    src_vocab_size = 10000  # ì˜ì–´ ì–´íœ˜
    tgt_vocab_size = 8000   # í•œêµ­ì–´ ì–´íœ˜
    
    model = EncoderDecoderTransformer(src_vocab_size, tgt_vocab_size)
    
    # ì˜ˆì‹œ ë°ì´í„°
    batch_size = 2
    src_len, tgt_len = 20, 25
    
    src = torch.randint(0, src_vocab_size, (batch_size, src_len))  # ì˜ì–´ ë¬¸ì¥
    tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_len))  # í•œêµ­ì–´ ë¬¸ì¥
    
    # Causal mask ìƒì„± (decoderê°€ ë¯¸ë˜ í† í°ì„ ë³´ì§€ ëª»í•˜ë„ë¡)
    tgt_mask = torch.tril(torch.ones(tgt_len, tgt_len)).unsqueeze(0).unsqueeze(0)
    
    # Forward pass
    output = model(src, tgt, tgt_mask=tgt_mask)
    
    print(f"ğŸŒ Translation Model Results:")
    print(f"Source shape: {src.shape}")
    print(f"Target shape: {tgt.shape}")
    print(f"Output shape: {output.shape}")
    
    # ë²ˆì—­ í™•ë¥  ê³„ì‚°
    translation_probs = F.softmax(output, dim=-1)
    predicted_tokens = torch.argmax(translation_probs, dim=-1)
    
    print(f"Predicted tokens shape: {predicted_tokens.shape}")
    
    return model, output

# ì‹¤í–‰
model, output = translation_example()
```

---

## ğŸš€ **ì‹¤ì „ í™œìš© ì˜ˆì‹œ**

### **1ï¸âƒ£ ê°ì • ë¶„ì„ ëª¨ë¸** ğŸ˜ŠğŸ˜¢

```python
class SentimentAnalyzer(nn.Module):
    def __init__(self, vocab_size, d_model=256, n_heads=8, n_layers=4):
        super().__init__()
        
        self.transformer = SimpleTransformer(vocab_size, d_model, n_heads, n_layers)
        self.classifier = nn.Sequential(
            nn.Linear(d_model, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 3)  # Positive, Negative, Neutral
        )
        
    def forward(self, x):
        # Transformer encoding
        transformer_output, _ = self.transformer(x)
        
        # Global average pooling
        sentence_embedding = transformer_output.mean(dim=1)
        
        # Classification
        sentiment_scores = self.classifier(sentence_embedding)
        
        return sentiment_scores

# ì‚¬ìš© ì˜ˆì‹œ
sentiment_model = SentimentAnalyzer(vocab_size=10000)
text_ids = torch.randint(0, 10000, (32, 50))  # ë°°ì¹˜ í¬ê¸° 32, ì‹œí€€ìŠ¤ ê¸¸ì´ 50
sentiment_scores = sentiment_model(text_ids)
print(f"Sentiment scores shape: {sentiment_scores.shape}")  # [32, 3]
```

### **2ï¸âƒ£ ì´ë¯¸ì§€ ìº¡ì…”ë‹ ëª¨ë¸** ğŸ–¼ï¸â†’ğŸ“

```python
class ImageCaptioning(nn.Module):
    def __init__(self, vocab_size, d_model=512, n_heads=8, n_layers=6):
        super().__init__()
        
        # ì´ë¯¸ì§€ ì¸ì½”ë” (ì˜ˆ: CNN features)
        self.image_encoder = nn.Linear(2048, d_model)  # CNN features â†’ d_model
        
        # í…ìŠ¤íŠ¸ ë””ì½”ë”
        self.text_decoder = SimpleTransformer(vocab_size, d_model, n_heads, n_layers)
        
        # Cross-attention for image-text alignment
        self.cross_attention = MultiHeadCrossAttention(d_model, n_heads)
        
        # Output projection
        self.output_projection = nn.Linear(d_model, vocab_size)
        
    def forward(self, image_features, text_ids):
        """
        Args:
            image_features: CNN features (batch_size, num_regions, 2048)
            text_ids: Text token ids (batch_size, seq_len)
        """
        # Encode image features
        image_emb = self.image_encoder(image_features)
        
        # Decode text
        text_emb, _ = self.text_decoder(text_ids)
        
        # Cross-attention: text attends to image
        cross_attn_output, _ = self.cross_attention(text_emb, image_emb, image_emb)
        
        # Combine and project
        combined = text_emb + cross_attn_output
        output = self.output_projection(combined)
        
        return output

# ì‚¬ìš© ì˜ˆì‹œ
captioning_model = ImageCaptioning(vocab_size=10000)
image_features = torch.randn(16, 36, 2048)  # 16 images, 36 regions each
text_ids = torch.randint(0, 10000, (16, 20))  # 16 captions, 20 tokens each
captions = captioning_model(image_features, text_ids)
print(f"Generated captions shape: {captions.shape}")  # [16, 20, 10000]
```

---

## âš¡ **ì„±ëŠ¥ ìµœì í™” íŒ**

### **1ï¸âƒ£ Flash Attention** âš¡

```python
try:
    # Flash Attention ì‚¬ìš© (PyTorch 2.0+)
    from torch.nn.functional import scaled_dot_product_attention
    
    def optimized_attention(Q, K, V, mask=None):
        # Flash Attention ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ëŒ€í­ í–¥ìƒ!
        return scaled_dot_product_attention(Q, K, V, attn_mask=mask)
        
except ImportError:
    # Fallback to standard attention
    def optimized_attention(Q, K, V, mask=None):
        # ê¸°ì¡´ ë°©ì‹
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Q.size(-1))
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        weights = F.softmax(scores, dim=-1)
        return torch.matmul(weights, V)
```

### **2ï¸âƒ£ Gradient Checkpointing** ğŸ’¾

```python
import torch.utils.checkpoint as checkpoint

class OptimizedTransformerBlock(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
    def forward(self, x, mask=None):
        # Gradient checkpointingìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¤„ì´ê¸°
        return checkpoint.checkpoint(self._forward, x, mask)
    
    def _forward(self, x, mask=None):
        # ì‹¤ì œ forward ë¡œì§
        attn_output, _ = self.attention(x, x, x, mask)
        x = self.ln1(x + self.dropout(attn_output))
        
        ff_output = self.feed_forward(x)
        x = self.ln2(x + self.dropout(ff_output))
        
        return x
```

### **3ï¸âƒ£ Mixed Precision Training** ğŸ¯

```python
from torch.cuda.amp import autocast, GradScaler

# í•™ìŠµ ë£¨í”„ ìµœì í™”
scaler = GradScaler()

for batch in dataloader:
    optimizer.zero_grad()
    
    with autocast():  # Mixed precision
        output = model(batch)
        loss = criterion(output, targets)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

---

## ğŸ¯ **ìš”ì•½ ë° ë§ˆë¬´ë¦¬**

### **ğŸŒŸ ìš°ë¦¬ê°€ ë°°ìš´ ê²ƒë“¤**

1. **ğŸ§  Self-Attention**: ê° í† í°ì´ ë‹¤ë¥¸ í† í°ë“¤ê³¼ì˜ ê´€ê³„ë¥¼ í•™ìŠµ
2. **ğŸª Multi-Head Attention**: ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— attention ê³„ì‚°
3. **ğŸ—ï¸ Transformer Block**: Attention + Feed-Forward + Residual + LayerNorm
4. **ğŸ¨ Cross Attention**: ì„œë¡œ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ ê°„ì˜ ê´€ê³„ í•™ìŠµ
5. **âš¡ ìµœì í™” ê¸°ë²•**: Flash Attention, Gradient Checkpointing, Mixed Precision

### **ğŸš€ ë‹¤ìŒ ë‹¨ê³„**

- **GPT**: Decoder-only ì•„í‚¤í…ì²˜ êµ¬í˜„
- **BERT**: Encoder-only ì•„í‚¤í…ì²˜ êµ¬í˜„  
- **T5**: Encoder-Decoder ì•„í‚¤í…ì²˜ êµ¬í˜„
- **Vision Transformer**: ì´ë¯¸ì§€ì— Transformer ì ìš©

### **ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸**

> **"Attention is All You Need"** - ì •ë§ë¡œ Attentionë§Œìœ¼ë¡œë„ ì¶©ë¶„í–ˆìŠµë‹ˆë‹¤! ğŸ¯
> 
> RNN/LSTM ì—†ì´ë„ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì´ ê°€ëŠ¥í•˜ë©°, ë³‘ë ¬ì²˜ë¦¬ë¡œ í›¨ì”¬ ë¹ ë¥´ê³  íš¨ìœ¨ì ì…ë‹ˆë‹¤.

**ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!** ì´ì œ ì—¬ëŸ¬ë¶„ì€ Transformerë¥¼ ì™„ì „íˆ ì´í•´í•˜ê³  êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ’ª

---

## ğŸ“š **ì°¸ê³  ìë£Œ**

- [Attention Is All You Need (ì›ë…¼ë¬¸)](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [PyTorch Official Tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)

---

*ì´ í¬ìŠ¤íŠ¸ê°€ ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´ â­ ìŠ¤íƒ€ì™€ ëŒ“ê¸€ ë¶€íƒë“œë¦½ë‹ˆë‹¤! ğŸ™* 