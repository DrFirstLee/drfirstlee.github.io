---
layout: post
title: "ğŸš€ Transformer íŒŒì´ì¬ìœ¼ë¡œ ì´í•´í•˜ê¸°!"
author: [DrFirst]
date: 2025-07-02 12:00:00 +0900
categories: [AI, Experiment]
tags: [transformer, attention, python, pytorch, MHCA]
sitemap :
  changefreq : monthly
  priority : 0.8
---


# ğŸš€ **Transformer íŒŒì´ì¬ìœ¼ë¡œ ì™„ì „ì •ë³µ!**

> **"Attention is All You Need"** - 2017ë…„ êµ¬ê¸€ì˜ í˜ëª…ì ì¸ ë…¼ë¬¸ ğŸ¯  
> ì´ì œ ìš°ë¦¬ë„ Transformerë¥¼ ì²˜ìŒë¶€í„° ëê¹Œì§€ íŒŒì´ì¬ìœ¼ë¡œ êµ¬í˜„í•´ë³´ì! ğŸ’ª

---

## ğŸ¯ **ëª©ì°¨**

1. [ğŸ” Transformer ê°œìš”](#overview)
2. [ğŸ§  Self-Attentionì˜ ê¸°ë³¸ ì›ë¦¬](#self-attention-basics)
3. [ğŸª Single-Head Attention êµ¬í˜„](#single-head-attention)
4. [ğŸ­ Multi-Head Attention êµ¬í˜„](#multi-head-attention)
5. [ğŸ—ï¸ Transformer ë¸”ë¡ êµ¬í˜„](#transformer-block)
6. [ğŸ¨ Multi-Head Cross Attention](#multi-head-cross-attention)
7. [ğŸš€ ì‹¤ì „ í™œìš© ì˜ˆì‹œ](#practical-examples)
8. [âš¡ ì„±ëŠ¥ ìµœì í™” íŒ](#optimization-tips)
9. [ğŸ¯ ìš”ì•½ ë° ë§ˆë¬´ë¦¬](#summary)
10. [ğŸ“š ì°¸ê³  ìë£Œ](#references)

---

## ğŸ” **Transformer ê°œìš”** {#overview}

### **ğŸ¤” Transformerê°€ ë­”ê°€ìš”?**

TransformerëŠ” 2017ë…„ êµ¬ê¸€ì—ì„œ ë°œí‘œí•œ **"Attention is All You Need"** ë…¼ë¬¸ì—ì„œ ì²˜ìŒ ì†Œê°œëœ neural network ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤!

### **ğŸ“š Attentionì˜ ì—­ì‚¬** 

**Attention ê°œë… ìì²´ëŠ” Transformer ë“±ì¥ ì „ë¶€í„° ì´ë¯¸ ìˆì—ˆì–´ìš”!** ğŸ•°ï¸
> ë‹¤ë§Œ ìš°ë¦¬ê°€ ì•Œê³ ìˆëŠ” Q / K / V ê°€ ì—†ì—ˆë‹¤!!  
> Q/K/VëŠ” Transformer ì´í›„ ëª…í™•íˆ ì •ë¦¬ëœ í‘œí˜„ì´ì•¼!!  

#### ** ì´ˆê¸° Attention(SHCA: Single-head Cross Attention)** ğŸ”

- LSTMì˜ ì¸ì½”ë”© : ê¸°ì¡´ì˜ LSTMì€ ë¬¸ì¥ì„ ìˆœì„œëŒ€ë¡œ ì½ê³  `hidden_state`ë¥¼ ìƒì„±í•œë‹¤(ë’¤ì—ì„œ ë³´ë ¤ê³ )!!  
- LSTMì˜ ë””ì½”ë”© : `hidden_state`ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²°ê³¼ ë‹¨ì–´ë¥¼ ì¶œë ¥í•œë‹¤!  
- ì—¬ê¸°ì„œ!! ë¬¸ì¥ì´ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì•ì˜ ë‚´ìš©ì„ ê¹Œë¨¹ëŠ” ë¬¸ì œê°€ ë§ìƒ!!  
- ê·¸ë˜ì„œ Attentionì„ ì œì‹œí–ˆë‹¤!! â†’ LSTMì—ì„œ ì¶œë ¥ì„ ìƒì„±í• ë–„ hidden_stateê°€ ì•„ë‹ˆë¼ ê³¼ê±° ë¬¸ì¥ì˜ ì „ì²´ë¥¼ ë³´ê³  ì–´ë””ë¥¼ ì§‘ì¤‘í• ì§€ ë³¸ë‹¤!!  


**ê·¸ë˜ì„œ!! AI ê°€ ì •ë¦¬í•œ ì´ˆê¸° Attentionì˜ íŠ¹ì§•**:
- ğŸ”„ **RNNê³¼ í•¨ê»˜ ì‚¬ìš©**: LSTM/GRU encoder-decoderì™€ ê²°í•©
- ğŸ“ **ë‹¨ë°©í–¥**: Decoderê°€ encoderë¥¼ "ë³´ëŠ”" ìš©ë„ (Cross Attention)
- ğŸ¯ **ë²ˆì—­ ë¬¸ì œ í•´ê²°**: ê¸´ ë¬¸ì¥ì—ì„œ ì •ë³´ ì†ì‹¤ ë°©ì§€
- **ğŸª Single-Headë§Œ ì¡´ì¬**: Multi-Head ê°œë…ì€ ì•„ì§ ì—†ì—ˆìŒ!
- ğŸ‘¥ **ëŒ€í‘œì  ì—°êµ¬**: Bahdanau (2015), Luong (2015)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class OldAttention(nn.Module):
    """ğŸ” 2015ë…„ Seq2Seq + Attention êµ¬ì¡° ì˜ˆì‹œ (Bahdanau-style)"""
    def __init__(self):
        super().__init__()
        # ğŸ”¹ ì¸ì½”ë”: ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ëŠ” LSTM (hidden_sizeëŠ” ì¶œë ¥ ì°¨ì›)
        self.encoder_rnn = nn.LSTM(input_size, hidden_size)

        # ğŸ”¹ ë””ì½”ë”: ì¶œë ¥ì„ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” LSTM
        self.decoder_rnn = nn.LSTM(input_size, hidden_size)

        # ğŸ”¸ ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°ìš© ì„ í˜• ë ˆì´ì–´
        # ì¸ì½”ë”ì™€ ë””ì½”ë” hidden stateë¥¼ ì—°ê²°í•´ ì ìˆ˜í™” (concat â†’ score)
        self.attention = nn.Linear(hidden_size * 2, 1)

    def forward(self, encoder_outputs, decoder_hidden):
        """
        Args:
            encoder_outputs: (seq_len, hidden_size) - ì¸ì½”ë”ì˜ ì „ì²´ ì¶œë ¥ ì‹œí€€ìŠ¤
            decoder_hidden: (1, hidden_size) - í˜„ì¬ ë””ì½”ë”ì˜ hidden state
        Returns:
            context: (1, hidden_size) - ì¸ì½”ë” ì¶œë ¥ë“¤ì˜ weighted sum
            attention_weights: (seq_len,) - softmax attention weights
        """

        attention_scores = []

        # ğŸ” ê° ì¸ì½”ë” ì¶œë ¥ ë²¡í„°ì™€ í˜„ì¬ ë””ì½”ë” ìƒíƒœë¥¼ ë¹„êµí•˜ì—¬ score ê³„ì‚°
        for encoder_output in encoder_outputs:
            # ğŸ§© ì¸ì½”ë” ì¶œë ¥ê³¼ ë””ì½”ë” ìƒíƒœë¥¼ ì—°ê²° (concat)
            # decoder_hidden ê°€ Qì—­í• , encoder_outputê°€ K V ì—­í• ì„í•œë‹¤!! 
            ## ì™œëƒí•˜ë©´ decoder_hiddenëŠ” ë¬´ì—‡ì„ ë³´ê³ ì‹¶ì€ê°€! Query í•˜ëŠ”ê±°ê³ ,
            ## encoder_outputê°€ ì¸ì½”ë” ìœ„ì¹˜ì˜ íŠ¹ì„±(K), ì‹¤ì œ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” ë²¡í„°(V) ì—­í• ì„ í•œë‹¤!
            combined = torch.cat([encoder_output, decoder_hidden], dim=1)

            # ğŸ“ ì„ í˜• ë ˆì´ì–´ë¥¼ í†µê³¼ì‹œì¼œ scalar score ì¶œë ¥
            score = self.attention(combined)  # (1, 1)
            attention_scores.append(score)

        # ğŸ”ƒ attention_scores: [(1,1), (1,1), ...] â†’ (seq_len, 1)
        attention_scores = torch.stack(attention_scores, dim=0)

        # ğŸ“Š softmaxë¡œ í™•ë¥ í™”í•˜ì—¬ attention weight ê³„ì‚°
        attention_weights = F.softmax(attention_scores, dim=0)  # (seq_len, 1)

        # ğŸ§® ê° ì¸ì½”ë” ì¶œë ¥ì— attention weight ê³±í•´ì„œ í•©ì‚° (ê°€ì¤‘í•©)
        # encoder_outputs: (seq_len, hidden_size)
        # attention_weights: (seq_len, 1) â†’ broadcasting
        context = torch.sum(attention_weights * encoder_outputs, dim=0)  # (1, hidden_size)

        return context, attention_weights  # contextëŠ” ë””ì½”ë”ì— ì „ë‹¬ë˜ëŠ” "ìš”ì•½ ì •ë³´"

```



#### **2017ë…„: Transformerì˜ í˜ì‹ ** âš¡

**ğŸ­ Multi-Head Attention (MHCA) ë“±ì¥!**

> ìš°ë¦¬ê°€ ì•„ë‹Œ 'Attention is all you Need!'  
> RNN ì—†ì— Attention ë§Œìœ¼ë¡œ, 
> ì—¬ê¸°ì„œ Q,K,V  ê°œë… ì •ë¦¬ ë°  Q=K=V  self attention ê°œë…ê³¼    
> ë‹¤ê°ë„ì—ì„œ ë³´ëŠ” Multi-Head ê°œë…ì„ ë„ì…í•¨!

```python
# Transformerì˜ Multi-Head Self-Attention (2017ë…„)
class MultiHeadSelfAttention_2017(nn.Module):
    """í˜ì‹ ì ì¸ Multi-Head Self-Attention"""
    def __init__(self, d_model, n_heads=8):
        super().__init__()
        self.n_heads = n_heads
        self.d_k = d_model // n_heads  # ê° headì˜ ì°¨ì›
        
        # ì—¬ëŸ¬ ê°œì˜ attention headë¥¼ ìœ„í•œ projection!
        self.W_q = nn.Linear(d_model, d_model)  # 8ê°œ head ë™ì‹œì—
        self.W_k = nn.Linear(d_model, d_model)  # 8ê°œ head ë™ì‹œì—  
        self.W_v = nn.Linear(d_model, d_model)  # 8ê°œ head ë™ì‹œì—
        self.W_o = nn.Linear(d_model, d_model)  # output projection
        
    def forward(self, x):
        batch_size, seq_len, d_model = x.size()
        
        # Q, K, Vë¥¼ multiple headsë¡œ ë¶„í• 
        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  
        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        
        # ê° headì—ì„œ ë…ë¦½ì ìœ¼ë¡œ attention ê³„ì‚°!
        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        attention_weights = F.softmax(attention_scores, dim=-1)
        
        # Multi-head attention ì ìš©
        context = torch.matmul(attention_weights, V)
        
        # Concatenate heads
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len, d_model
        )
        
        # Final output projection
        output = self.W_o(context)
        
        return output, attention_weights

# ë¹„êµ: Single-Head vs Multi-Head
def compare_attention_mechanisms():
    """SHCA vs MHCA ë¹„êµ"""
    
    # Single-Head (2015ë…„ ë°©ì‹)
    single_head_output = single_attention_head(x)  # 1ê°œ ê´€ì 
    
    # Multi-Head (2017ë…„ ë°©ì‹) 
    multi_head_output = []
    for head in range(8):  # 8ê°œ ë‹¤ë¥¸ ê´€ì !
        head_output = attention_head(x, head_id=head)
        multi_head_output.append(head_output)
    
    # 8ê°œ headì˜ ê²°ê³¼ë¥¼ ê²°í•©
    combined_output = concatenate_and_project(multi_head_output)
    
    return combined_output
```


### ğŸ§  **Self-Attentionì˜ ê¸°ë³¸ ì›ë¦¬** {#self-attention-basics}

Self-Attentionì€ **"ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ìˆëŠ”ì§€"**ë¥¼ ê³„ì‚°í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤!

```python
# ì˜ˆì‹œ: "The cat sat on the mat"
# "cat"ì´ë¼ëŠ” ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ìˆì„ê¹Œ?
# cat -> The (0.1), cat (1.0), sat (0.8), on (0.2), the (0.1), mat (0.3)
```

#### **ğŸ”‘ Query, Key, Value ê°œë…**

Think of it like a **search engine**! ğŸ”

- **Query (Q)**: "ë‚´ê°€ ì°¾ëŠ” ê²ƒ" - í˜„ì¬ ë‹¨ì–´ì˜ ê´€ì‹¬ì‚¬
- **Key (K)**: "ê²€ìƒ‰ í‚¤ì›Œë“œ" - ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì˜ íŠ¹ì„±
- **Value (V)**: "ì‹¤ì œ ë‚´ìš©" - ë‹¨ì–´ì˜ ì‹¤ì œ ì •ë³´

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

def simple_attention_example():
    """ê°„ë‹¨í•œ Attention ì˜ˆì‹œ"""
    
    # ì˜ˆì‹œ ë¬¸ì¥: "I love AI"
    # ê° ë‹¨ì–´ë¥¼ 3ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„
    sentence = torch.tensor([
        [1.0, 0.0, 0.0],  # "I"
        [0.0, 1.0, 0.0],  # "love"  
        [0.0, 0.0, 1.0]   # "AI"
    ])
    
    # Query, Key, Value ê³„ì‚° (ë‹¨ìˆœí™”)
    Q = sentence  # Query: ê° ë‹¨ì–´ê°€ ë¬´ì—‡ì„ ì°¾ê³  ìˆë‚˜?
    K = sentence  # Key: ê° ë‹¨ì–´ì˜ íŠ¹ì„±
    V = sentence  # Value: ê° ë‹¨ì–´ì˜ ì‹¤ì œ ì •ë³´
    
    # Attention Score ê³„ì‚°
    attention_scores = torch.matmul(Q, K.transpose(-2, -1))
    print("Attention Scores:")
    print(attention_scores)
    
    # Softmaxë¡œ í™•ë¥  ë³€í™˜
    attention_weights = F.softmax(attention_scores, dim=-1)
    print("\nAttention Weights:")
    print(attention_weights)
    
    # ìµœì¢… ì¶œë ¥
    output = torch.matmul(attention_weights, V)
    print("\nFinal Output:")
    print(output)

# ì‹¤í–‰
simple_attention_example()
```


### **ğŸš€ SHCA â†’ MHCA í˜ëª…ì  ë³€í™”**

| **SHCA (2015ë…„)** | **MHCA (2017ë…„)** |
|-------------------|-------------------|
| ğŸ¯ **1ê°œ ê´€ì ** | ğŸ­ **8ê°œ ê´€ì ** |
| ğŸ“ **Cross Attentionë§Œ** | ğŸ”„ **Self + Cross** |
| ğŸ”„ **RNN ì˜ì¡´** | ğŸš« **RNN ì œê±°** |
| ğŸŒ **ìˆœì°¨ ì²˜ë¦¬** | âš¡ **ë³‘ë ¬ ì²˜ë¦¬** |
| ğŸ“Š **ë‹¨ìˆœ ê°€ì¤‘í•©** | ğŸ§  **ë³µí•© í‘œí˜„** |

#### **ğŸª Multi-Headì˜ íš¨ê³¼**

ê° headê°€ **ì„œë¡œ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ê´€ê³„**ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤:

```python
# ì˜ˆì‹œ: "The cat sat on the mat" ë¶„ì„
sentence = "The cat sat on the mat"

# Head 1: ë¬¸ë²•ì  ê´€ê³„ í•™ìŠµ
head_1_attention = [
    # "cat" â†’ "The" (ê´€ì‚¬-ëª…ì‚¬ ê´€ê³„)
    # "sat" â†’ "cat" (ì£¼ì–´-ë™ì‚¬ ê´€ê³„)  
    # "on" â†’ "sat" (ë™ì‚¬-ì „ì¹˜ì‚¬ ê´€ê³„)
]

# Head 2: ì˜ë¯¸ì  ê´€ê³„ í•™ìŠµ  
head_2_attention = [
    # "cat" â†’ "mat" (ê³ ì–‘ì´ê°€ ë§¤íŠ¸ì™€ ê´€ë ¨)
    # "sat" â†’ "on" (ì•‰ëŠ” ë™ì‘ê³¼ ìœ„ì¹˜)
]

# Head 3: ìœ„ì¹˜ì  ê´€ê³„ í•™ìŠµ
head_3_attention = [
    # ì¸ì ‘í•œ ë‹¨ì–´ë“¤ ê°„ì˜ ê´€ê³„
    # "The" â†’ "cat", "cat" â†’ "sat" ë“±
]

# Head 4-8: ë‹¤ë¥¸ ì¶”ìƒì  ê´€ê³„ë“¤...
```

**ğŸ”¥ Multi-Headì˜ í˜ì‹ ì  ì¥ì **:
- **ğŸ¯ ë‹¤ì–‘í•œ ê´€ì **: ë¬¸ë²•, ì˜ë¯¸, ìœ„ì¹˜ ë“± ë™ì‹œ í•™ìŠµ
- **ğŸ§  í’ë¶€í•œ í‘œí˜„**: ë³µì¡í•œ ì–¸ì–´ íŒ¨í„´ í¬ì°©  
- **âš¡ ë³‘ë ¬ ê³„ì‚°**: ëª¨ë“  headê°€ ë™ì‹œì— ì²˜ë¦¬
- **ğŸš€ ì„±ëŠ¥ í–¥ìƒ**: ì‹¤ì œë¡œ ë²ˆì—­/ì´í•´ ì„±ëŠ¥ ëŒ€í­ ê°œì„ 

**ğŸ’¡ ê²°ê³¼**: Single-Headì˜ í•œê³„ë¥¼ ì™„ì „íˆ ê·¹ë³µ! ğŸ‰

### **ğŸ”¥ Transformerì˜ 3ëŒ€ í˜ì‹ **

| ê¸°ì¡´ Attention (2015) | Transformer Attention (2017) |
|---------------------|----------------------------|
| ğŸ”„ **RNN í•„ìˆ˜** | ğŸš« **RNN ì—†ìŒ** |
| ğŸ“ **Encoderâ†’Decoderë§Œ** | ğŸ”„ **Self-Attention** |
| ğŸ¯ **ë‹¨ì¼ Head** | ğŸ­ **Multi-Head** |
| ğŸŒ **ìˆœì°¨ ì²˜ë¦¬** | âš¡ **ë³‘ë ¬ ì²˜ë¦¬** |


**í˜ì‹  í¬ì¸íŠ¸**:
1. **ğŸ§  Self-Attention**: ê°™ì€ ì‹œí€€ìŠ¤ ë‚´ì—ì„œ ëª¨ë“  ìœ„ì¹˜ê°€ ì„œë¡œ ê´€ê³„ í•™ìŠµ
2. **ğŸ­ Multi-Head**: ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— attention ê³„ì‚°  
3. **âš¡ ë³‘ë ¬í™”**: RNN ì—†ì´ë„ ì‹œí€€ìŠ¤ ì²˜ë¦¬ ê°€ëŠ¥

### **ğŸ“ˆ ë°œì „ ê³¼ì • ìš”ì•½**

```mermaid
timeline
    title Attention ë°œì „ì‚¬
    2014 : Neural Machine Translation
         : Encoder-Decoder ë“±ì¥
    2015 : Bahdanau Attention
         : ì²« ë²ˆì§¸ Attention ë©”ì»¤ë‹ˆì¦˜
         : Luong Attention
    2017 : Transformer
         : "Attention is All You Need"
         : Self-Attention í˜ëª…
    2018+ : BERT, GPT ë“±ì¥
          : Transformer ê¸°ë°˜ ëª¨ë¸ë“¤
```

**ğŸ’¡ ê²°ë¡ **: Attentionì€ ê¸°ì¡´ì— ìˆë˜ ê°œë…ì´ì§€ë§Œ, **Transformerê°€ ì™„ì „íˆ ìƒˆë¡œìš´ ë ˆë²¨ë¡œ ëŒì–´ì˜¬ë ¸ìŠµë‹ˆë‹¤!** ğŸš€

**í•µì‹¬ ì•„ì´ë””ì–´**: 
- ğŸš« **RNN/LSTM ì—†ì´ë„** ì‹œí€€ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥
- âš¡ **ë³‘ë ¬ì²˜ë¦¬** ê°€ëŠ¥ìœ¼ë¡œ í•™ìŠµ ì†ë„ ëŒ€í­ í–¥ìƒ
- ğŸ¯ **Self-Attention** ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì¥ê±°ë¦¬ ì˜ì¡´ì„± í•´ê²°

---

## ğŸ—ï¸ **Transformer ë¸”ë¡ êµ¬í˜„** {#transformer-block}

ì´ì œ ì™„ì „í•œ Transformer ë¸”ë¡ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤! ğŸ‰

```python
class TransformerBlock(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        
        # Multi-Head Attention
        self.attention = MultiHeadAttention(d_model, n_heads, dropout)
        
        # Feed-Forward Network
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        
        # Layer Normalization
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        """
        Args:
            x: Input tensor (batch_size, seq_len, d_model)
            mask: Optional attention mask
        Returns:
            output: Transformer block output
        """
        # 1. Multi-Head Attention + Residual Connection + Layer Norm
        attn_output, attn_weights = self.attention(x, x, x, mask)
        x = self.ln1(x + self.dropout(attn_output))
        
        # 2. Feed-Forward + Residual Connection + Layer Norm
        ff_output = self.feed_forward(x)
        x = self.ln2(x + self.dropout(ff_output))
        
        return x, attn_weights

# ì™„ì „í•œ Transformer ëª¨ë¸! ğŸš€
class SimpleTransformer(nn.Module):
    def __init__(self, vocab_size, d_model=512, n_heads=8, n_layers=6, 
                 d_ff=2048, max_seq_len=5000, dropout=0.1):
        super().__init__()
        
        self.d_model = d_model
        
        # Embedding layers
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_seq_len, d_model)
        
        # Transformer blocks
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])
        
        # Final layer norm
        self.ln_final = nn.LayerNorm(d_model)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        """
        Args:
            x: Input token ids (batch_size, seq_len)
            mask: Optional attention mask
        Returns:
            output: Transformer output (batch_size, seq_len, d_model)
            attention_weights: List of attention weights from each layer
        """
        batch_size, seq_len = x.size()
        
        # Token embeddings
        token_emb = self.token_embedding(x)
        
        # Position embeddings
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)
        pos_emb = self.position_embedding(positions)
        
        # Combine embeddings
        x = self.dropout(token_emb + pos_emb)
        
        # Apply transformer blocks
        attention_weights = []
        for block in self.transformer_blocks:
            x, attn_weights = block(x, mask)
            attention_weights.append(attn_weights)
        
        # Final layer norm
        x = self.ln_final(x)
        
        return x, attention_weights

# ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ! ğŸ¯
def test_transformer():
    """Transformer ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    
    # ëª¨ë¸ ìƒì„±
    vocab_size = 10000
    model = SimpleTransformer(vocab_size)
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„±
    batch_size, seq_len = 2, 50
    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
    
    # Forward pass
    output, attention_weights = model(input_ids)
    
    print(f"ğŸš€ Transformer Results:")
    print(f"Input shape: {input_ids.shape}")
    print(f"Output shape: {output.shape}")
    print(f"Number of layers: {len(attention_weights)}")
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params:,}")
    
    return model, output, attention_weights

# ì‹¤í–‰
model, output, attention_weights = test_transformer()
```


---

## ğŸ¯ **ìš”ì•½ ë° ë§ˆë¬´ë¦¬** {#summary}

### **ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸**

> **"Attention is All You Need"** - ì •ë§ë¡œ Attentionë§Œìœ¼ë¡œë„ ì¶©ë¶„í–ˆìŠµë‹ˆë‹¤! ğŸ¯
> 
> RNN/LSTM ì—†ì´ë„ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì´ ê°€ëŠ¥í•˜ë©°, ë³‘ë ¬ì²˜ë¦¬ë¡œ í›¨ì”¬ ë¹ ë¥´ê³  íš¨ìœ¨ì ì…ë‹ˆë‹¤.

**ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!** ì´ì œ ì—¬ëŸ¬ë¶„ì€ Transformerë¥¼ ì™„ì „íˆ ì´í•´í•˜ê³  êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ’ª

---

## ğŸ“š **ì°¸ê³  ìë£Œ** {#references}

- [Attention Is All You Need (ì›ë…¼ë¬¸)](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [PyTorch Official Tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)

---

