---
layout: post
title: "ğŸ§  SAM2 Hands-On Practice!! : SAM2 ì‹¤ìŠµ!! with Python"
author: [DrFirst]
date: 2025-09-02 09:00:00 +0900
categories: [AI, Experiment]
tags: [SAM2, Segment Anything, Meta AI, Video segmentation, Python, Vision]
sitemap:
  changefreq: weekly
  priority: 0.9
---
---
### ğŸ§¬ (English) EfficientSAM Practice!!

Today, letâ€™s revisit the practice of the new SAM model `SAM2`, which we studied [theoretically in a previous post](https://drfirstlee.github.io/posts/SAM2/)  
and also [experimented with before](https://drfirstlee.github.io/posts/SAM2_usage/)!  

This time, instead of using `ultralytics`, weâ€™ll fetch the model directly from HuggingFace!!  

> Howeverâ€¦ compared to EfficientSAM, the results of this SAM2 arenâ€™t really satisfying to meâ€¦

---

### ğŸ”§ 1. Installation & Setup

Clone directly from GitHub and install.  
I created a virtual environment called `sam2` beforehand!  

```bash
conda create --name sam2 python=3.12
git clone https://github.com/facebookresearch/sam2.git && cd sam2

pip install -e .
```

Additionally, although the SAM2 GitHub readme suggests downloading the model via `./download_ckpts.sh`,  
the actual code has a `hf_hub_download` function to fetch weights â€” so we can skip that!!

---

### ğŸ–¼ï¸ 2. Image Segmentation    

Just like in the [EfficientSAM practice](https://drfirstlee.github.io/posts/efficientSAM_usage/),  
Iâ€™ll use a dog photo with only two prompt points!!  

```python
import torch
import numpy as np
from PIL import Image, ImageDraw
import os
from sam2.sam2_image_predictor import SAM2ImagePredictor

# 1. Device setup
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# 2. Load SAM2 model
predictor = SAM2ImagePredictor.from_pretrained("facebook/sam2-hiera-large")

# 3. Load image
image_path = "./EfficientSAM_gdino/figs/examples/dogs.jpg"
output_image_path = "output_masked_image.png"

image_pil = Image.open(image_path).convert("RGB")
image_np = np.array(image_pil)

# -----------------------------------------------------
# 4. Prepare prompt 
# -----------------------------------------------------
input_points = torch.tensor([[[580, 350], [650, 350]]], device=device)
input_labels = torch.tensor([[1, 1]], device=device)

# 5. Run prediction
with torch.inference_mode(), torch.autocast(device_type=device.split(":")[0], dtype=torch.bfloat16):
    predictor.set_image(image_np)
    masks, scores, _ = predictor.predict(
        point_coords=input_points,
        point_labels=input_labels,
    )
mask = masks[0]
print(f"mask shape :{mask.shape}")

# Create black canvas
segmented_image_np = np.zeros_like(image_np)

# Convert mask
binary_mask = (mask > 0.5)

# Apply mask
segmented_image_np[binary_mask] = image_np[binary_mask]

# Convert back to image
result_image = Image.fromarray(segmented_image_np)

# Draw prompt points
draw_result = ImageDraw.Draw(result_image)
points_np = input_points[0].cpu().numpy()
labels_np = input_labels[0].cpu().numpy()

for i, (x, y) in enumerate(points_np):
    label = labels_np[i]
    fill_color = "green" if label == 1 else "red"
    outline_color = "white"
    radius = 5

    if label == 1:
        draw_result.ellipse((x - radius, y - radius, x + radius, y + radius),
                            fill=fill_color, outline=outline_color, width=1)
    else:
        draw_result.line((x - radius, y - radius, x + radius, y + radius),
                         fill=fill_color, width=2)
        draw_result.line((x + radius, y - radius, x - radius, y + radius),
                         fill=fill_color, width=2)

result_image.save(output_image_path)
print(f"Result saved to '{output_image_path}'")
```

Result image is!!!??

![Image](https://github.com/user-attachments/assets/a175eae3-a5e8-463b-bb96-15dcd5c94c51)

Totally disappointingâ€¦  
So, I tried with 4 prompt points and switched the model to `sam2.1_hiera_large.pt`!!

![Image](https://github.com/user-attachments/assets/83a1eb1c-3d3a-4b9a-812b-eb951fdd7ee4)

Still disappointingâ€¦  
GPT explained itâ€™s due to **prompt interpretation differences**.  
But honestly, I still prefer EfficientSAM!!

```text
EfficientSAM didnâ€™t actually â€œperform better,â€ but rather interpreted your imperfect prompts more â€œforgivingly.â€

SAM2, on the other hand, is much more powerful and precise, so to fully leverage it, you must provide more accurate prompts.  
As suggested before, if you place several points on the dogsâ€™ body and head, youâ€™ll see that SAM2 produces masks that are far more refined and higher quality than EfficientSAM.
```

---

### ğŸ§ª 3. Video Segmentation    

![Image](https://github.com/user-attachments/assets/0ef426cb-262a-42ab-b714-d109844500b0)

Starting with the result!!! This one looks good~~  

Code is below:

```python
import torch
import numpy as np
import cv2
import os
from sam2.sam2_video_predictor import SAM2VideoPredictor
from moviepy.editor import VideoFileClip

# 1. Device setup
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Paths
output_video_path = "output_segmented_video_50_55s.mp4"
clipped_video_path = "temp_clip.mp4"

# 3. Load video
cap = cv2.VideoCapture(clipped_video_path)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

video_frames = []
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    video_frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
cap.release()
print(f"Loaded clip: {width}x{height}, {total_frames} frames, {fps:.2f} FPS")

# 4. Load SAM2 model
print("Loading SAM2 video predictor...")
predictor = SAM2VideoPredictor.from_pretrained("facebook/sam2-hiera-large")

# -----------------------------------------------------
# 5. Prompt setup
# -----------------------------------------------------
prompt_frame_idx = 0  # frame index to add prompt
prompt_obj_id = 1     # unique object ID

# Coordinates and labels
points = np.array([[width // 2, height // 2]], dtype=np.float32)
labels = np.array([1], dtype=np.int32)

# -----------------------------------------------------
# 6. Initialize and predict
# -----------------------------------------------------
with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
    print("Initializing predictor state...")
    state = predictor.init_state(clipped_video_path) 
    
    print("Adding prompt on first frame...")
    _, _, masks = predictor.add_new_points_or_box(
        inference_state=state,
        frame_idx=prompt_frame_idx,
        obj_id=prompt_obj_id,
        points=points,
        labels=labels,
    )

    # 7. Propagate in video
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))
    
    print("Propagating masks across video...")
    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):
        original_frame = video_frames[frame_idx]
        segmented_image_np = np.full_like(original_frame, 255)

        if prompt_obj_id in object_ids:
            mask_logits = masks[0][0].cpu().numpy()
            binary_mask_before_resize = (mask_logits > 0.0).astype(np.uint8)
            resized_mask = cv2.resize(binary_mask_before_resize, (width, height), interpolation=cv2.INTER_NEAREST)
            boolean_mask = (resized_mask == 1)
            segmented_image_np[boolean_mask] = original_frame[boolean_mask]

        # Draw red point
        for x, y in points:
            cv2.circle(segmented_image_np, (int(x), int(y)), radius=5, color=(255, 0, 0), thickness=-1)

        output_frame = cv2.cvtColor(segmented_image_np, cv2.COLOR_RGB2BGR)
        out_writer.write(output_frame)
        
        print(f"\r- Processing: frame {frame_idx + 1}/{total_frames}", end="")

    out_writer.release()
    print(f"\nVideo segmentation complete! Saved to '{output_video_path}'")
```

SAM2â€™s video tracking is really impressive!!!

---

### ğŸ§¬ (í•œêµ­ì–´) EfficientSAM ì‹¤ìŠµ!!

ì˜¤ëŠ˜ì€ [ì˜ˆì „í¬ìŠ¤íŒ…](https://drfirstlee.github.io/posts/SAM2/)ì—ì„œ ì´ë¡ ì— ëŒ€í•˜ì—¬ ê³µë¶€í•´ë³´ì•˜ë˜!  
ê·¸ë¦¬ê³  [ì‹¤ìŠµë„ í•´ë³´ì•˜ë˜](https://drfirstlee.github.io/posts/SAM2_usage/)
ìƒˆë¡œìš´ SAM ëª¨ë¸! `SAM2` ì˜ ì‹¤ìŠµì„ ë‹¤ì‹œ í•œ ë²ˆ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤!!  
ì´ë²ˆì—” ì§€ë‚œë²ˆê³¼ ë‹¬ë¦¬ `ultralytics` ê°€ ì•„ë‹ˆë¼ huggingfaceì—ì„œ ëª¨ë¸ì„ ë°›ì•„ì„œê³ ê³ !!  

> ê·¸ëŸ°ë°,, ì´ SAM2, EfficientSAMë³´ë‹¤ëŠ” ê²°ê³¼ë¬¼ì´ ë§˜ì—ë“¤ì§€ ì•Šêµ¬ë§Œìœ ,,

---

### ğŸ”§ 1. ì„¤ì¹˜ ë° ì…‹ì—…

GitHubì—ì„œ ì§ì ‘ í´ë¡ í•˜ì—¬ ì„¤ì¹˜í•©ë‹ˆë‹¤.  
ì €ëŠ” ê·¸ì „ì— sam2ë¼ëŠ” ê°€ìƒí™˜ê²½ì„ ë§Œë“¤ê³  ì§„í–‰í–ˆìŠµë‹ˆë‹¤!!  

```bash
conda create --name sam2 python=3.12
git clone https://github.com/facebookresearch/sam2.git && cd sam2

pip install -e .
```

ì¶”ê°€ë¡œ SAM2 gitì˜ readmeì—ì„œëŠ” `./download_ckpts.sh`ë¡œ ëª¨ë¸ì„ ë°›ìœ¼ë¼ê³ í–ˆì§€ë§Œ,  
ì‹¤ì œ ì½”ë“œëŠ” `hf_hub_download`ë¥¼ í†µí•´ weightë¥¼ ë°›ì•„ì˜¤ëŠ” ê¸°ëŠ¥ì´ ìˆê¸°ì— ìŠ¤í‚µ!!

---

### ğŸ–¼ï¸ 2. ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜    

ì§€ë‚œ [EfficientSAM ì‹¤ìŠµ](https://drfirstlee.github.io/posts/efficientSAM_usage/)ê³¼ ë™ì¼í•˜ê²Œ!! ë©ë©ì´ ì‚¬ì§„ìœ¼ë¡œ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤!!  
í”„ë¡¬í¬íŠ¸ë„~ ì ì„ 2ê°œë§Œ!!  

```python
import torch
import numpy as np
from PIL import Image, ImageDraw
import os
from sam2.sam2_image_predictor import SAM2ImagePredictor

# 1. ê¸°ë³¸ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# 2. SAM2 ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
predictor = SAM2ImagePredictor.from_pretrained("facebook/sam2-hiera-large")

# 3. ì´ë¯¸ì§€ ì¤€ë¹„
image_path = "./EfficientSAM_gdino/figs/examples/dogs.jpg"
output_image_path = "output_masked_image.png"


image_pil = Image.open(image_path).convert("RGB")
image_np = np.array(image_pil)


# -----------------------------------------------------
# 4. í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ 
# -----------------------------------------------------
# input_pointsë¥¼ (batch, num_points, 2) í˜•íƒœì˜ 3D í…ì„œë¡œ ë§Œë“­ë‹ˆë‹¤.
input_points = torch.tensor([[[580, 350], [650, 350]]], device=device)
input_labels = torch.tensor([[1, 1]], device=device)

# input_labels = torch.tensor([[1, 1, 1, 1]], device=device) # ëª¨ë“  ì ì´ ì „ê²½
# 5. ì˜ˆì¸¡ ì‹¤í–‰
with torch.inference_mode(), torch.autocast(device_type=device.split(":")[0], dtype=torch.bfloat16):
    predictor.set_image(image_np)
    masks, scores, _ = predictor.predict(
        point_coords=input_points,
        point_labels=input_labels,
    )
mask = masks[0]
print(f"mask shape :{mask.shape}")

# ì›ë³¸ ì´ë¯¸ì§€ì™€ ê°™ì€ í¬ê¸°ì˜ ì™„ì „íˆ ê²€ì •ìƒ‰ NumPy ë°°ì—´ ìƒì„±
segmented_image_np = np.zeros_like(image_np)

# ë§ˆìŠ¤í¬ê°€ Trueì¸ ì˜ì—­ì—ë§Œ ì›ë³¸ ì´ë¯¸ì§€ í”½ì…€ì„ ë³µì‚¬
# maskëŠ” boolean ë°°ì—´ (True/False)ì´ê±°ë‚˜ 0~1 ì‚¬ì´ì˜ float ë°°ì—´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# float ë°°ì—´ì´ë¼ë©´ ì„ê³„ê°’ì„ ì ìš©í•˜ì—¬ boolean ë§ˆìŠ¤í¬ë¡œ ë§Œë“­ë‹ˆë‹¤.
binary_mask = (mask > 0.5) # 0.5ë¥¼ ê¸°ì¤€ìœ¼ë¡œ True/False ë§ˆìŠ¤í¬ ìƒì„±

# ë§ˆìŠ¤í¬ê°€ Trueì¸ ìœ„ì¹˜ì— ì›ë³¸ ì´ë¯¸ì§€ í”½ì…€ì„ í• ë‹¹
segmented_image_np[binary_mask] = image_np[binary_mask]

# NumPy ë°°ì—´ì„ PIL Imageë¡œ ë³€í™˜
result_image = Image.fromarray(segmented_image_np)

# í”„ë¡¬í”„íŠ¸ ì ë„ ì´ë¯¸ì§€ ìœ„ì— ê·¸ë¦¬ê¸° (ì„ íƒ ì‚¬í•­)
draw_result = ImageDraw.Draw(result_image)
points_np = input_points[0].cpu().numpy()
labels_np = input_labels[0].cpu().numpy()

for i, (x, y) in enumerate(points_np):
    label = labels_np[i]

    fill_color = "green" if label == 1 else "red"
    outline_color = "white"
    radius = 5

    if label == 1:
        draw_result.ellipse((x - radius, y - radius, x + radius, y + radius), fill=fill_color, outline=outline_color, width=1)
    else:
        draw_result.line((x - radius, y - radius, x + radius, y + radius), fill=fill_color, width=2)
        draw_result.line((x + radius, y - radius, x - radius, y + radius), fill=fill_color, width=2)

result_image.save(output_image_path)
print(f"ê²°ê³¼ ì´ë¯¸ì§€ê°€ '{output_image_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
```

ê²°ê³¼ ì´ë¯¸ì§€ëŠ”!!!??

![Image](https://github.com/user-attachments/assets/a175eae3-a5e8-463b-bb96-15dcd5c94c51)

ì™„ì „ ì‹¤ë§ìŠ¤ëŸ¬ìš´ë°,,??  
ê·¸ë˜ì„œ, í”„ë¡¬í¬íŠ¸ ì ì„ 4ê°œë¡œ, ëª¨ë¸ë„ `sam2.1_hiera_large.pt`ë¡œ ë°”ê¿”ì„œ í•´ë³´ì•˜ëŠ”ë°!!

![Image](https://github.com/user-attachments/assets/83a1eb1c-3d3a-4b9a-812b-eb951fdd7ee4)

 ê·¸ë˜ë„ ê²°ê³¼ê°€ ì‹¤ë§ìŠ¤ëŸ½ë„¤ìš”,,
GPTì— ë¬¼ì–´ë³´ë‹ˆ í”„ë¡¬í¬íŠ¸ì˜ í•´ì„ì°¨ì´ë¼ê³ í•˜ëŠ”ë°,,  
ì €ëŠ” ê·¸ë˜ë„ EfficientSAMì´ ì¢‹ë„¤ìš”!!

```text
EfficientSAMì´ ë” "ì˜" í–ˆë‹¤ê¸°ë³´ë‹¤ëŠ”, ì‚¬ìš©ìì˜ ë¶€ì •í™•í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ë” "ê´€ëŒ€í•˜ê²Œ" í•´ì„í•´ ì¤€ ê²ƒì…ë‹ˆë‹¤.

ë°˜ë©´ì— SAM2ëŠ” í›¨ì”¬ ê°•ë ¥í•˜ê³  ì •ë°€í•œ ë„êµ¬ì´ê¸° ë•Œë¬¸ì—, ê·¸ ì„±ëŠ¥ì„ ì œëŒ€ë¡œ í™œìš©í•˜ë ¤ë©´ ì‚¬ìš©ìë„ ë” ì •í™•í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. ì´ì „ ë‹µë³€ì—ì„œ ì œì•ˆí•´ ë“œë¦° ê²ƒì²˜ëŸ¼ ê°•ì•„ì§€ë“¤ì˜ ëª¸í†µê³¼ ë¨¸ë¦¬ì— ì—¬ëŸ¬ ê°œì˜ ì ì„ ì°ì–´ì£¼ì‹œë©´, SAM2ê°€ EfficientSAMë³´ë‹¤ í›¨ì”¬ ë” ê³ í’ˆì§ˆì˜ ì •êµí•œ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆì„ ê²ë‹ˆë‹¤.
```
 

---

### ğŸ§ª 3. ì˜ìƒ Segmentation    

![Image](https://github.com/user-attachments/assets/0ef426cb-262a-42ab-b714-d109844500b0)

ê²°ê³¼ë¶€í„°!!! ë§˜ì—ë“œëŠ”ë°ìš”~~  

ì½”ë“œëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤!

```python
import torch
import numpy as np
import cv2
import os
from sam2.sam2_video_predictor import SAM2VideoPredictor
from moviepy.editor import VideoFileClip

# 1. ê¸°ë³¸ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# ì›ë³¸ ë° ì¶œë ¥ ê²½ë¡œ ì„¤ì •
output_video_path = "output_segmented_video_50_55s.mp4"
clipped_video_path = "temp_clip.mp4"


# 3. ë¹„ë””ì˜¤ ë¡œë”©
cap = cv2.VideoCapture(clipped_video_path)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

video_frames = []
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    video_frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
cap.release()
print(f"í´ë¦½ ë¡œë“œ ì™„ë£Œ: {width}x{height}, {total_frames} í”„ë ˆì„, {fps:.2f} FPS")

# 4. SAM2 ëª¨ë¸ ë¡œë“œ
print("SAM2 ë¹„ë””ì˜¤ ì˜ˆì¸¡ê¸° ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
predictor = SAM2VideoPredictor.from_pretrained("facebook/sam2-hiera-large")

# -----------------------------------------------------
# 5. í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (â˜…â˜…â˜…â˜…â˜… ì´ ë¶€ë¶„ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤ â˜…â˜…â˜…â˜…â˜…)
# -----------------------------------------------------
prompt_frame_idx = 0  # í”„ë¡¬í”„íŠ¸ë¥¼ ì ìš©í•  í”„ë ˆì„ ì¸ë±ìŠ¤ (0ì€ ì²« ë²ˆì§¸ í”„ë ˆì„)
prompt_obj_id = 1     # ì¶”ì í•  ê°ì²´ì˜ ê³ ìœ  ID (ì²« ë²ˆì§¸ ê°ì²´ì´ë¯€ë¡œ 1)

# ì  ì¢Œí‘œ: (NumPoints, 2) í˜•íƒœì˜ NumPy ë°°ì—´
points = np.array([[width // 2, height // 2]], dtype=np.float32)
# ë ˆì´ë¸”: (NumPoints,) í˜•íƒœì˜ NumPy ë°°ì—´
labels = np.array([1], dtype=np.int32)

# -----------------------------------------------------
# 6. ëª¨ë¸ ì´ˆê¸°í™” ë° ì²« í”„ë ˆì„ ì˜ˆì¸¡
# -----------------------------------------------------
with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
    print("ì˜ˆì¸¡ê¸° ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
    state = predictor.init_state(clipped_video_path) 
    
    print("ì²« ë²ˆì§¸ í”„ë ˆì„ì— í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤...")
    # ì˜ˆì œ ì½”ë“œì— ë§ì¶° í•¨ìˆ˜ ì´ë¦„ê³¼ ì¸ìë¥¼ ëª¨ë‘ ë³€ê²½
    _, _, masks = predictor.add_new_points_or_box(
        inference_state=state,
        frame_idx=prompt_frame_idx,
        obj_id=prompt_obj_id,
        points=points,
        labels=labels,
    )
    # 7. ë¹„ë””ì˜¤ ì „íŒŒ ë° ê²°ê³¼ ì €ì¥
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))
    
    print("í´ë¦½ ì „ì²´ì— ë§ˆìŠ¤í¬ë¥¼ ì „íŒŒí•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...")
    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):
        original_frame = video_frames[frame_idx]
        # segmented_image_np = np.zeros_like(original_frame)
        # segmented_image_np = np.zeros_like(original_frame)
        segmented_image_np = np.full_like(original_frame, 255)

        # prompt_obj_id (1)ê°€ ì¶”ì ë˜ê³  ìˆëŠ”ì§€ í™•ì¸
        if prompt_obj_id in object_ids:
            mask_logits = masks[0][0].cpu().numpy()
            binary_mask_before_resize = (mask_logits > 0.0).astype(np.uint8)
            resized_mask = cv2.resize(binary_mask_before_resize, (width, height), interpolation=cv2.INTER_NEAREST)

            # 4. ë¦¬ì‚¬ì´ì¦ˆëœ ë§ˆìŠ¤í¬ë¥¼ boolean íƒ€ì…ìœ¼ë¡œ ìµœì¢… ë³€í™˜
            boolean_mask = (resized_mask == 1)

            # 5. ì˜¬ë°”ë¥¸ ë§ˆìŠ¤í¬ë¡œ ì¸ë±ì‹±
            segmented_image_np[boolean_mask] = original_frame[boolean_mask]
            # # ì´ì œ shapeì´ ì¼ì¹˜í•˜ë¯€ë¡œ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.
            # segmented_image_np[mask] = original_frame[mask]   
        # --- ì¶”ê°€ëœ ë¶€ë¶„: ëª¨ë“  í”„ë ˆì„ì— í”„ë¡¬í”„íŠ¸ ì  ê·¸ë¦¬ê¸° ---
        for x, y in points:
            # cv2.circleì„ ì‚¬ìš©í•˜ì—¬ ë¹¨ê°„ìƒ‰ ì ì„ ê·¸ë¦½ë‹ˆë‹¤.
            # segmented_image_npëŠ” RGB ìƒíƒœì´ë¯€ë¡œ, ë¹¨ê°„ìƒ‰ì€ (255, 0, 0)ì…ë‹ˆë‹¤.
            # thickness=-1ì€ ì±„ì›Œì§„ ì›ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
            cv2.circle(segmented_image_np, (int(x), int(y)), radius=5, color=(255, 0, 0), thickness=-1)

        output_frame = cv2.cvtColor(segmented_image_np, cv2.COLOR_RGB2BGR)
        out_writer.write(output_frame)
        
        print(f"\r- ì²˜ë¦¬ ì¤‘: í”„ë ˆì„ {frame_idx + 1}/{total_frames}", end="")

    out_writer.release()
    print(f"\në¹„ë””ì˜¤ ë¶„í•  ì™„ë£Œ! ê²°ê³¼ê°€ '{output_video_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

```

SAM2, ì˜ìƒì—ì„œ íŠ¸ë˜ì´ì‹±ì€ ì •ë§ ë§˜ì—ë“œëŠ”êµ°ìš”!^^