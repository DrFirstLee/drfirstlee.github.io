---
layout: post
title: "üß† [CIReVL] VISION-BY-LANGUAGE FOR TRAINING-FREE COMPOSITIONAL IMAGE RETRIEVAL : First Training Free on CIRR"
author: [DrFirst]
date: 2025-07-30 09:00:00 +0900
categories: [AI, Research]
tags: [VLM, CIR, Training-Free, CIReVL, ICLR, ICLR 2024, Image Retrieval, Compositionality]
sitemap :
  changefreq : monthly
  priority : 0.8
---

---

### üß† (ÌïúÍµ≠Ïñ¥) Training-Free Ï°∞Ìï© Ïù¥ÎØ∏ÏßÄ Í≤ÄÏÉâ!! **CIReVL (ICLR 2024)**  
> CIRCO(ICCV 2023)Ïùò Îí§Î•º Ïù¥Ïñ¥, CIR Ïó∞Íµ¨Í∞Ä ÌïôÏäµ Í∏∞Î∞òÏóêÏÑú **ÏôÑÏ†ÑÌïú Training-Free** Î∞©ÏãùÏúºÎ°ú ÌôïÏû•!!  

![Image](https://github.com/user-attachments/assets/3985a62d-91b1-4048-a5a4-016e6053c92f)

- **Ï†úÎ™©**: [Vision-by-Language for Training-Free Compositional Image Retrieval](https://arxiv.org/abs/2310.09291)  
- **ÌïôÌöå**: ICLR 2024 (Karthik et al.)  
- **ÏΩîÎìú**: [Vision-by-Language (GitHub)](https://github.com/ExplainableML/Vision_by_Language)  
- **ÌïµÏã¨ ÌÇ§ÏõåÎìú**: `Composed Image Retrieval`, `Training-Free`, `Vision-by-Language`, `TIFA`, `ICLR 2024`, `ZS-CIR`  

---

### üîç Ïó∞Íµ¨ Î∞∞Í≤Ω  

CIRR(ICCV 2021) ‚Üí CIRCO(ICCV 2023)ÍπåÏßÄ, CIR Ïó∞Íµ¨Îäî ÌÅ¨Í≤å ÏßÑÌôîÌï¥ÏôîÎã§.  
ÌïòÏßÄÎßå Í∏∞Ï°¥ Ï†ëÍ∑ºÏóêÎäî Ïó¨Ï†ÑÌûà ÌïúÍ≥ÑÍ∞Ä ÏûàÏóàÎã§:  

- CIRR, FashionIQ, CIRCO Îì±Ïùò Îç∞Ïù¥ÌÑ∞ÏÖãÏóê ÎßûÏ∂ò **ÌïôÏäµ Í∏∞Î∞ò Ï†ëÍ∑º**Ïù¥ Ï£ºÎ•ò  
- Ï†úÎ°úÏÉ∑(Zero-Shot) Ï†ëÍ∑º(ZS-CIR)Ïù¥ Îì±Ïû•ÌñàÏßÄÎßå, Textual Inversion Í∞ôÏùÄ ÏÇ¨Ï†Ñ ÌïôÏäµÎêú Ïª¥Ìè¨ÎÑåÌä∏Ïóê ÏùòÏ°¥  

> Îî∞ÎùºÏÑú Ïó∞Íµ¨ÏûêÎì§ÏùÄ ÏôÑÏ†ÑÌûà **Training-Free**Ìïú CIR ÌîÑÎ†àÏûÑÏõåÌÅ¨Î•º Ï†úÏïàÌïòÍ≥†Ïûê ÌñàÎã§.  
> ÌïôÏäµ ÏóÜÏù¥, Ïò§ÏßÅ VLM(Visual-Language Model)Í≥º Ïñ∏Ïñ¥Ï†Å Ï°∞Ìï©ÏùÑ ÌÜµÌï¥ Ïù¥ÎØ∏ÏßÄ Í≤ÄÏÉâÏùÑ ÏàòÌñâÌïòÎäî Î∞©ÏãùÏù¥ Î∞îÎ°ú **Vision-by-Language**Ïù¥Îã§.  

---

### üß† Ï£ºÏöî Í∏∞Ïó¨  

1. #### CIReVL: Training-Free Zero-Shot CIR ÌîÑÎ†àÏûÑÏõåÌÅ¨ Ï†úÏïà  
   - Î≥ÑÎèÑÏùò ÌïôÏäµ(Pretraining/Fine-tuning, Textual Inversion Îì±) ÏóÜÏù¥ **Í∏∞Ï°¥ ÏÇ¨Ï†ÑÌïôÏäµÎêú Î™®Îç∏(off-the-shelf VLM)**Îßå ÌôúÏö©  
   - Ï∞∏Ï°∞ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÖçÏä§Ìä∏Î•º **Ïñ∏Ïñ¥ Í≥µÍ∞ÑÏóêÏÑú ÏßÅÏ†ë Ï°∞Ìï©**ÌïòÏó¨ ÏøºÎ¶¨Î•º Íµ¨ÏÑ±  
   - CIRR, FashionIQ, CIRCO, CIRR-Extended Îì± **ÎÑ§ Í∞ÄÏßÄ CIR Î≤§ÏπòÎßàÌÅ¨ÏóêÏÑú Í∏∞Ï°¥ ÌïôÏäµ Í∏∞Î∞ò Î∞©Î≤ïÍ≥º ÎπÑÏä∑ÌïòÍ±∞ÎÇò Îçî Ï¢ãÏùÄ ÏÑ±Îä•**ÏùÑ Îã¨ÏÑ±  

2. #### Î™®ÎìàÏÑ±(Modularity) & Ïñ∏Ïñ¥ ÏàòÏ§Ä Ï∂îÎ°†(Language-level Reasoning)  
> Ïù¥Î∂ÄÎ∂ÑÏù¥ ÎßòÏóê Îì†Îã§!! Ïù¥Ìï¥Ìï†Ïàò ÏóÜÎäî Î≤°ÌÑ∞Ï∞®ÏõêÏù¥ ÏïÑÎãàÎùº Ïñ∏Ïñ¥ ÎèÑÎ©îÏù∏Ïù¥Ïñ¥ÏÑú Ï§ëÍ∞ÑÍ≥ºÏ†ïÏùÑ Îî∞ÎùºÍ∞àÏàò ÏûàÏñ¥!  
   - CIReVLÏùÄ Ïñ∏Ïñ¥ ÎèÑÎ©îÏù∏ÏóêÏÑú ÏøºÎ¶¨Î•º Ï≤òÎ¶¨ÌïòÍ∏∞ ÎïåÎ¨∏Ïóê **Í≤ÄÏÉâ Í≥ºÏ†ïÏùò Ìï¥ÏÑù Í∞ÄÎä•ÏÑ±**ÏùÑ ÎÜíÏûÑ  
   - Ïù∏Í∞ÑÏù¥ Ïù¥Ìï¥ Í∞ÄÎä•Ìïú ÏàòÏ§ÄÏóêÏÑú Ï°∞Ìï©Ï†Å Í≤ÄÏÉâ Í≥ºÏ†ïÏùÑ ÏÑ§Î™ÖÌï† Ïàò ÏûàÍ≥†  
     ÏÇ¨Ïö©ÏûêÍ∞Ä ÏßÅÏ†ë ÏßàÏùò Ï°∞Ìï©ÏùÑ ÏàòÏ†ï¬∑Í∞úÏûÖÌïòÎäî Í≤ÉÎèÑ Í∞ÄÎä•  

3. #### Ï∂îÍ∞Ä Ïã§Ìóò Î∞è Î∂ÑÏÑù (Ablation Studies)  
   - ÌååÏù¥ÌîÑÎùºÏù∏ Íµ¨ÏÑ± ÏöîÏÜåÏóê ÎåÄÌïú Îã§ÏñëÌïú ablation Ïó∞Íµ¨ ÏàòÌñâ  
   - **Ïñ∏Ïñ¥ Í∏∞Î∞ò reasoningÏùò Ï§ëÏöîÏÑ±**ÏùÑ Ïã§Ï¶ùÏ†ÅÏúºÎ°ú Î≥¥Ïó¨Ï£ºÍ≥†  
     Î™®ÎìàÌôîÎêú Íµ¨Ï°∞ ÎçïÎ∂ÑÏóê CIReVLÏù¥ **Í∞ÑÎã®ÌïòÍ≤å ÌôïÏû• Í∞ÄÎä•(scalable)**Ìï®ÏùÑ Í∞ïÏ°∞  

---

### üß† CIReVL ÏÉÅÏÑ∏ Íµ¨Ï°∞  

![Image](https://github.com/user-attachments/assets/905b8c23-5537-4b75-9791-d2f208d36ddf)

- **ÌïµÏã¨ ÏïÑÏù¥ÎîîÏñ¥**  
  - Image Captioning : Ïù¥ÎØ∏ÏßÄÎ•º VLMÏù¥ Ïù¥Ìï¥Ìï† Ïàò ÏûàÎäî ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôò  
  - ÏàòÏ†ï ÌÖçÏä§Ìä∏ÏôÄ Í≤∞Ìï©ÌïòÏó¨ **ÏµúÏ¢Ö ÏßàÏùò Î¨∏Ïû•(query sentence)**ÏùÑ ÏÉùÏÑ±  
  - Ïù¥ Î¨∏Ïû•ÏùÑ Îã§Ïãú VLM ÏûÑÎ≤†Îî©ÏúºÎ°ú Î≥ÄÌôò ÌõÑ, ÌõÑÎ≥¥ Ïù¥ÎØ∏ÏßÄÏôÄ Îß§Ïπ≠  

1. From text embeddings to captions.  
  - BLIP, BLIP-2, CoCa Î•º ÌôúÏö© Ïù¥ÎØ∏ÏßÄÎ•º ÌÖçÏä§Ìä∏Î°ú ÏÑ§Î™Ö    

2. From templates to reasoning targets.  
  - LLMÏùÑ ÌôúÏö©Ìï¥ÏÑú, ÏàòÏ†ïÏöîÏ≤≠ ÌîÑÎ°¨Ìè¨Ìä∏Îûë Ïù¥ÎØ∏ÏßÄ captionÏùÑ Í≤∞Ìï©!  
  - LLMÏùÄ GPT3.5/4 Llama Îì±ÏùÑ ÌôúÏö©
  - ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú Ïö∞Î¶¨Í∞Ä Ï∞æÏùÑ Ïù¥ÎØ∏ÏßÄÏùò ÏÑ§Î™ÖÏù¥ÎùºÍ≥†ÏÉùÍ∞ÅÌïòÎ©¥Îê®!  

  ```text
  "I have an image. Given an instruction to edit the image,  carefully generate a description of the edited image. 

  I will put my image content beginning with ‚ÄôImage Content:{Ïù¥ÎØ∏ÏßÄcaption}‚Äô.
  The instruction I provide will begin with ‚ÄôInstruction:{ÏàòÏ†ïÌîÑÎ°¨Ìè¨Ìä∏}‚Äô.

  The edited description you generate should begin with ‚ÄôEdited Description:‚Äô. Each time generate one instruction and one edited description only."
  ```

3. Compositional image retrieval.  
![Image](https://github.com/user-attachments/assets/6a191f6d-8ce1-49bf-925b-8c0d20e76e56)  
  - 2Ïùò Í≤∞Í≥ºÎ¨º ÏûÑÎ≤†Îî©Í≥º Ïù¥ÎØ∏ÏßÄÎì§Ïùò ÏûÑÎ≤†Îî©ÏùÑ ÎπÑÍµê!!  


### üß† ÏÑ±Îä• ÎπÑÍµêÔºàEXPERIMENTSÔºâ  

![Image](https://github.com/user-attachments/assets/dfb3a51b-5047-4222-bd37-5aa8c36ca180)

- Training FreeÏûÑÏóêÎèÑ Î∂àÍµ¨ÌïòÍ≥† Ïó¨Îü¨ ÏßÄÌëúÏóêÏÑú Ïö∞ÏàòÌïú ÏÑ±Îä•!!  
  - **FashionIQ**  
    - Vision-by-Language Ï†ëÍ∑ºÎßåÏúºÎ°úÎèÑ R@10 Í∏∞Ï§Ä 20%ÎåÄ ÏÑ±Îä• ÌôïÎ≥¥  
    - SEARLEÎ≥¥Îã§ ÏïΩÍ∞Ñ ÎÇÆÏßÄÎßå, ÌïôÏäµ Î∂àÌïÑÏöîÌïòÎã§Îäî Ï†êÏóêÏÑú ÌÅ∞ Ïû•Ï†ê  
  - **CIRR / CIRCO**  
    - Training-Free Ï†ëÍ∑ºÏù¥ Í∏∞Ï°¥ ÌïôÏäµ Î™®Îç∏Í≥º ÎπÑÏä∑Ìïú ÏàòÏ§ÄÏùò Recall Îã¨ÏÑ±  
    - ÌäπÌûà TIFA ÌèâÍ∞ÄÏóêÏÑúÎäî **ÌïôÏäµ Í∏∞Î∞ò ÎåÄÎπÑ Í≤ΩÏüÅÎ†• ÏûàÎäî Ï†ïÌï©ÏÑ± ÌôïÎ≥¥**  

![Image](https://github.com/user-attachments/assets/06672961-05c5-41b5-96ae-eb4a67e330cc)

- ÎòêÌïú Ablation StudyÎ•º ÌÜµÌï¥ LLM, VLM Îì± ÏÑ±Îä•Ïù¥ Ï¢ãÏïÑÏßÄÎ©¥, Î™®Îç∏ Í≤∞Í≥ºÎèÑ Ï¢ãÏïÑÏßêÏùÑ ÌôïÏù∏!  

---

### üß© Í≤∞Î°†  

> **CIReVL (ICLR 2024)**Îäî CIR Ïó∞Íµ¨Ïùò Ìå®Îü¨Îã§ÏûÑÏùÑ **‚ÄúÌïôÏäµ Í∏∞Î∞ò ‚Üí Ï†úÎ°úÏÉ∑ ‚Üí Training-Free‚Äù**Î°ú ÌôïÏû•ÏãúÏº∞Îã§!!  

- **ÏôÑÏ†ÑÌïú Training-Free ÌîÑÎ†àÏûÑÏõåÌÅ¨**Î•º Ï†úÏïàÌïòÏó¨, ÌïôÏäµ Í∏∞Î∞ò Ï†ëÍ∑º ÏóÜÏù¥ÎèÑ CIRR, FashionIQ, CIRCO Îì± Ï£ºÏöî Î≤§ÏπòÎßàÌÅ¨ÏóêÏÑú Í≤ΩÏüÅÎ†• ÏûàÎäî ÏÑ±Îä• Îã¨ÏÑ±  
- **Ïñ∏Ïñ¥ Í∏∞Î∞ò Ï°∞Ìï©(Language-level reasoning)**ÏùÑ ÌÜµÌï¥ Í≤ÄÏÉâ Í≥ºÏ†ïÏùÑ **Ìï¥ÏÑù Í∞ÄÎä•**ÌïòÍ≤å ÎßåÎì§Í≥†, ÏÇ¨Ïö©ÏûêÍ∞Ä ÏßÅÏ†ë Í∞úÏûÖÌï† Ïàò ÏûàÎäî Ìà¨Î™ÖÏÑ±Í≥º Ïú†Ïó∞ÏÑ± Ï†úÍ≥µ  
- **Î™®ÎìàÏÑ±(Modularity)** ÎçïÎ∂ÑÏóê Í≤ÄÏÉâ ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ ÏâΩÍ≤å ÍµêÏ≤¥¬∑ÌôïÏû•Ìï† Ïàò ÏûàÏúºÎ©∞, Ìñ•ÌõÑ Îçî Í∞ïÎ†•Ìïú VLM¬∑LLMÏù¥ Îì±Ïû•Ìï†ÏàòÎ°ù ÏÑ±Îä• Ìñ•ÏÉÅÏùò Ïû†Ïû¨Î†• Î≥¥Ïú†  

Îî∞ÎùºÏÑú, CIReVLÏùÄ **Zero-Shot CIR ‚Üí Training-Free CIR**Î°ú Ïù¥Ïñ¥ÏßÄÎäî Î∞úÏ†Ñ ÌùêÎ¶ÑÏóêÏÑú ÌïµÏã¨Ï†ÅÏù∏ ÏúÑÏπòÎ•º Ï∞®ÏßÄÌïòÎ©∞,  
ÏïûÏúºÎ°úÏùò Ï°∞Ìï© Ïù¥ÎØ∏ÏßÄ Í≤ÄÏÉâ Ïó∞Íµ¨Ïóê Ï§ëÏöîÌïú Í∏∞Î∞òÏùÑ Ï†úÍ≥µÌïúÎã§.  



### üß† Training-Free Compositional Image Retrieval!! **CIReVL (ICLR 2024)**  
> Following CIRCO (ICCV 2023), CIR research evolves from training-based methods to a **completely Training-Free** approach!!  

![Image](https://github.com/user-attachments/assets/3985a62d-91b1-4048-a5a4-016e6053c92f)

- **Title**: [Vision-by-Language for Training-Free Compositional Image Retrieval](https://arxiv.org/abs/2310.09291)  
- **Conference**: ICLR 2024 (Karthik et al.)  
- **Code**: [Vision-by-Language (GitHub)](https://github.com/ExplainableML/Vision_by_Language)  
- **Keywords**: `Composed Image Retrieval`, `Training-Free`, `Vision-by-Language`, `TIFA`, `ICLR 2024`, `ZS-CIR`  

---

### üîç Research Background  

From **CIRR (ICCV 2021)** ‚Üí **CIRCO (ICCV 2023)**, CIR research has significantly evolved.  
However, there were still limitations:  

- Most approaches relied on **training-based pipelines** adapted to CIRR, FashionIQ, or CIRCO datasets.  
- Zero-Shot approaches (ZS-CIR) emerged, but they still depended on pre-trained components like Textual Inversion.  

> Therefore, the authors proposed a fully **Training-Free** CIR framework.  
> Without additional training, they perform image retrieval using only a VLM (Visual-Language Model) and **language-level composition** ‚Äî this is exactly **Vision-by-Language**.  

---

### üß† Main Contributions  

1. #### CIReVL: A Training-Free Zero-Shot CIR Framework  
   - Utilizes **only off-the-shelf pre-trained models** (no Pretraining, Fine-tuning, or Textual Inversion).  
   - Combines reference images and modification texts directly in the **language space** to form queries.  
   - Achieves performance **comparable or superior** to training-based methods on **CIRR, FashionIQ, CIRCO, and CIRR-Extended** benchmarks.  

2. #### Modularity & Language-Level Reasoning  
> This is the exciting part!! Instead of incomprehensible vector space operations, the reasoning happens in the **language domain**, making the process easier to follow!  
   - Since CIReVL processes queries in natural language, the **retrieval process is interpretable**.  
   - Human users can understand intermediate reasoning steps and even **intervene or edit** query composition directly.  

3. #### Additional Studies (Ablation Analysis)  
   - Extensive ablation studies on pipeline components.  
   - Showed the **importance of language-based reasoning** for effective CIR.  
   - Highlighted how the modular design makes CIReVL **easily scalable and extendable**.  

---

### üß† CIReVL Framework Details  

![Image](https://github.com/user-attachments/assets/905b8c23-5537-4b75-9791-d2f208d36ddf)

- **Core Idea**  
  - **Image Captioning**: Convert an image into a textual description understandable by a VLM.  
  - Combine this caption with the modification instruction to form a **final query sentence**.  
  - Encode this query with a VLM and compare it to image embeddings to retrieve results.  

1. **From text embeddings to captions**  
   - Use BLIP, BLIP-2, or CoCa to caption the reference image.  

2. **From templates to reasoning targets**  
   - An LLM (e.g., GPT-3.5/4, LLaMA) fuses the image caption and user modification instruction.  
   - Produces a coherent **edited description** ‚Äî essentially a textual specification of the target image.  

   ```text
   "I have an image. Given an instruction to edit the image, carefully generate a description of the edited image. 

   I will put my image content beginning with ‚ÄôImage Content:{image caption}‚Äô.
   The instruction I provide will begin with ‚ÄôInstruction:{modification prompt}‚Äô.

   The edited description you generate should begin with ‚ÄôEdited Description:‚Äô. 
   Each time generate one instruction and one edited description only."
  ```


3. Compositional image retrieval.  
![Image](https://github.com/user-attachments/assets/6a191f6d-8ce1-49bf-925b-8c0d20e76e56)  
  - Compare the embedding of the result from step 2 with the embeddings of the images!!  

---

### üß† Experimental Results  

![Image](https://github.com/user-attachments/assets/dfb3a51b-5047-4222-bd37-5aa8c36ca180)

- Despite being **Training-Free**, CIReVL achieves strong performance across benchmarks!!  
  - **FashionIQ**  
    - Achieves ~20% R@10 purely with Vision-by-Language composition.  
    - Slightly lower than SEARLE, but with the major advantage of requiring **no training**.  
  - **CIRR / CIRCO**  
    - Training-Free approach reaches Recall levels comparable to training-based models.  
    - Particularly on **TIFA evaluation**, CIReVL demonstrates **competitive compositional faithfulness** compared to training-based methods.  

![Image](https://github.com/user-attachments/assets/06672961-05c5-41b5-96ae-eb4a67e330cc)

- Ablation studies further confirm that as **LLMs and VLMs improve, CIReVL‚Äôs performance also improves accordingly!**  

---

### üß© Conclusion  

> **CIReVL (ICLR 2024)** extends the paradigm of CIR research from **‚ÄúTraining-based ‚Üí Zero-Shot ‚Üí Training-Free.‚Äù**  

- Proposes a **completely Training-Free framework**, achieving competitive results on CIRR, FashionIQ, and CIRCO without any training.  
- Enables **interpretable retrieval** through **language-level reasoning**, providing transparency and flexibility that allows direct human intervention.  
- Thanks to its **modular design**, the retrieval pipeline can be easily replaced or extended, and with the advancement of stronger VLMs/LLMs, CIReVL has clear potential for further improvements.  

Therefore, CIReVL occupies a **key position in the evolution from Zero-Shot CIR to Training-Free CIR**,  
and offers an important foundation for the future of compositional image retrieval research.  
