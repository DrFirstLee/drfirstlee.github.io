---
layout: post
title: "📝Understanding EZ-HOI - EZ-HOI 알아보기!!"
author: [DrFirst]
date: 2025-06-18 07:00:00 +0900
categories: [AI, Research]
tags: [HOI, NeurIPS, NeurIPS 2024]
sitemap :
  changefreq : monthly
  priority : 0.8
---


---

### 🧠 (한국어) EZ-HOI 알아보기?!!  
_🔍 !!_  

![manhwa]()

> 논문: [EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection](https://arxiv.org/pdf/2410.23904)  
> 발표: NeurIPS 2024 (Lei, Wang, et al.)  
> 코드: [ChelsieLei/EZ-HOI](https://github.com/ChelsieLei/EZ-HOI)  

---


#### 📌 배경: HOI와 VLM 결합 연구의 한계!?

**Human-Object Interaction (HOI)**란!!  
이미지 또는 비디오에서 사람(Human)과 객체(Object)의 쌍을 찾아고, 이들 사이의 상호작용(Interaction)을 분류하는 작업입니다.  

##### ❓ 문제 1: VLM과 연계하는 HOI 연구!  
> 너무 모델이 크고 세부적인것은 못한다는 단점!!  

최근의 HOT연구들은 **Vision-Language Models (VLMs)**을 많이 활용했는데,  
대표적인 것이 HOI 검출기와 VLM의 특징 벡터를 정렬(alignment)시켜 **행동(action)**과 같은 개념을 양쪽 모델이 유사하게 이해할 수 있도록 만드는 방법이었음!!  
이를 통해 정렬된 특징은 제로샷(zero-shot) 상황에서도 모델이 본 적 없는 상호작용도 이해할수 있었지만!!  
아래와 같은 단점들이 있었음  

- 💸 **고비용의 정렬 학습 과정**: VLM과의 정렬은 대개 트랜스포머 구조 기반으로, 연산 비용/학습 시간 등이 큰 문제!  
- 🔒 **제로샷 일반화의 어려움**:  VLM 정렬은 학습된 클래스(Seen classes)에만 최적화되어, **보지 못한 클래스(Unseen classes)**에 대한 예측 성능이 낮음!  
- 🧠 **지식 전이의 한계**: VLM은 넓은 개념은 잘 이해하지만, HOI처럼 사람의 미세한 행동 차이를 구분해야 하는 과제에는 약점이 있음!  

##### ❗ 문제 2: 프롬포트만을 튜닝해서 가벼운 학습!!  
> 다만, 프롬포트 튜닝은 Seen위주로만 진행되어 Unseen에서는 성능이 좋지않음!  

이에, 최근에는 정렬 과정을 생략하고, VLM의 표현력을 그대로 활용하는 **프롬프트 튜닝(prompt tuning)** 기반 접근 방식이 대안으로 주목받고 있지만,  
이 또한 제로샷 문제에서는 아직 충분한 성과를 보여주지 못했음!! 

> 참고 : VLM의 표현력을 그대로 활용하는 **프롬프트 튜닝(prompt tuning)** 기반 접근 방식 이란!?  
> "A photo of a cat" 를 "[P1] [P2] [P3] cat"  와 같이 넣고 P1 P2 P3을 학습시킴!  
> 논문에서 예를든 MaPLe의 프롬포트 튜닝은 이미지와 텍스트를 함꼐 튜닝함!!  

- 결과적으로, **HOI와 VLM의 결합은 유망하지만**, **가벼운 모델&일반화 능력 확보**라는 한계가 있었습니다!  

---

#### 💡 EZ-HOI 요약!!!   

##### 🧩 Inference (추론)
> 사전 Fine Tuning 된 learnable prompt가 기존 foundation 모델과 결합되어 쓰임!!
> 그래서 기존 foundation 모델은 학습된것 없는 Zero shot!!

```text
[Input] 단일 이미지
    ↓
Stage 1: Human-Object Detection
    - 사람과 모든 객체 bbox 추출
    - 가능한 모든 (human, object) pair 생성

Stage 2: HOI 인식
    - 각 human-object pair → CLIP의 visual encoder → 이미지 임베딩 (f_vis)
    - 모든 HOI 클래스의 (object-action pair) → CLIP의 text encoder + learnable prompt → 텍스트 임베딩 (f_txt)
    - cosine similarity(f_vis, f_txt) 기반으로 가장 유사한 HOI class 선택
    → 최종 HOI 예측
```

##### 🛠️ Training (학습)

1. LLM 기반 HOI 클래스 설명 생성  
- 모든 object-interaction (HOI class) 쌍에 대해 LLM으로 풍부한 문장 생성  
  `"Swinging a baseball bat describes a person..."`

2. Seen 클래스 학습  
> 이때 Seen Class의 learnable prompt와 MHCA weight가 정해짐!!  

→ Cross-Attention (MHCA)  
  - Q: learnable prompt (초기화됨)  
  - K/V: LLM 설명의 토큰 임베딩  
→ Attention 결과물을 이미지 임베딩과 유사하도록 학습 (cosine similarity 기반)  

3. Unseen Class 학습 : 3단게!!
> 이때 Seen Class의 learnable prompt와 MHCA weight를 바탕으로 Unseen Class의 learnable prompt가 정해짐!!  

1단계: Cross-Attention (MHCA) - Seen 에서 정해진 MHCA weight  
    - Q: learnable prompt (가장 유사한 Seen Class의 최종 learnable prompt로 시작)  
    - K/V: Unseen class의 LLM 설명의 토큰 임베딩  
  → Attention 결과물을 유사 seen class의 prompt 결과와 유사하도록 학습 (cosine similarity 기반)  

2단계: Class-relation 학습 - Seen과 Unseen의 LLM 설명 임베딩끼리의 유사도 만큼 learnable prompt가 유사하게 되도록 학습!  

3단계: Negative 학습 - Seen class의 이미지 인코딩과 Unclass의 learnable prompt가 멀어지도록 학습  


!!learnable prompt가 처음에 한번 들어가는게 아니라 layer별로 나눠서 들어간다!!  


ez-HOI inference

이미지가 주어짐!!
> stage1 : 이미지에서 human과 모든 object bbox 추출 > 모든 human–object pair
> stage2 
 - pair들의 이미지를 learnable promt와 함꼐 CLIP으로 인코딩
 - 존재하는 class (object-interation)를 각각의 class별 learnable prompt 랑 함께 인코딩
 - 이미지와 가장 유사한 인코딩 class결과 추출해서 HOI 산출

ez-HOI 학습

> 우선 모든 object-interaction pair(Class) 에 대하여 llm을 통한 장문의 caption 생성
> SEEN에 대해서,
 - MHCA(q = learnable prompt_class, k/v = llm caption) >> 결과물을 이미지 encoding이랑 sim이 작아지도록 MHCA, learnable prompt 학습
> Unseen에 대해서
 - MHCA(q = learnable prompt_class(가장 유사한 class의 learnable prompt 그대로 가져옴), k/v = llm caption(Unseen도 미리 준비됨)) 
 - attention의 결과물이 가장 유사한 class의 learnable prompt 결과 인코딩이랑 비슷하게 되도록함 : 이때 unseen의 learnable prompt 학습됨
 - 또한 Seen 이미지의 CLIP 벡터랑 unseen의 learnable prompt 의 sim이 달라지도록 Unseen의 learnable prompt 학습
 


 을 이미지 encoding이랑 sim이 작아지도록 MHCA, learnable prompt 학습



![structure](https://github.com/user-attachments/assets/8b56436c-8e37-494a-9232-5fa84ae2e9a1)  

 CLIP-Adapter는 **이미지와 텍스트의 feature level에서 직접 조정**을 수행합니다.

```text
            ┌────────────────────┐                ┌────────────────────┐
            │   Image Input (x)  │                │   Text Input (t)   │
            └────────┬───────────┘                └────────┬───────────┘
                     ↓                                     ↓
       ┌────────────────────────────┐         ┌────────────────────────────┐
       │   CLIP Image Encoder       │         │   CLIP Text Encoder        │
       │       (frozen)             │         │        (frozen)            │
       └─────────┬──────────────────┘         └────────┬───────────────────┘
                 ↓                                     ↓
     ┌─────────────────────┐               ┌─────────────────────┐
     │  Image Adapter MLP  │               │  Text Adapter MLP   │
     │     (trainable)     │               │     (trainable)     │
     └────────┬────────────┘               └──────────┬──────────┘
              ↓                                       ↓
     ┌────────────────────────────┐       ┌────────────────────────────┐
     │ Residual: image + adapted  │       │ Residual: text + adapted   │
     └────────────────────────────┘       └────────────────────────────┘
              ↓                                       ↓
     ┌────────────────────┐              ┌────────────────────┐
     │  Image Embedding   │              │  Text Embedding    │
     └────────────────────┘              └────────────────────┘
                     └───────────────┬───────────────┘
                                     ↓
                      Cosine Similarity / Classification

```

결국 위의 구조에서 `Adapter MLP` 와 `Residual Connection` 부분이 이번 연구의 핵심인데요!!  

##### 🔧 Adapter MLP (Image, Text에 각각!!)

![adapter](https://github.com/user-attachments/assets/8af17eed-1b5a-4069-9836-d974b27f7bea)  

Adapter 부분의 MLP는!! 
- 두 개의 선형 계층 + ReLU 비선형 함수구조로서,  
- 구조: `Linear → ReLU → Linear`
- Bottleneck 구조로 중간 차원으로 축소했다가 다시 확장하게 됩니다!! 


##### 🖇️ Residual Connection

![residual](https://github.com/user-attachments/assets/884a49f8-f76c-4dea-850e-394d93599fee)

few-shot 으로 학습하게 된다면!!  
학습 데이터가 극히 적기 때문에, 모델이 데이터에 지나치게 맞춰지는(overfitting) 경향이 있습니다!  
이런 오버피팅에 대한 해결 방법으로   Residual Connection (잔차 연결)을 적용했습니다!  

핵심 아이디어는 `"새롭게 학습한 표현과, 기존에 잘 학습된 CLIP 표현을 비율을 조절해 섞자."` 로서  

1. (이미지와 텍스트의 CLIP 임베딩 결과를 adapter 에 통과시킨 결과) X α  
2. (이미지와 텍스트의 기존 CLIP 임베딩 결과) X (1-α)  

로 하여 학습 결과및 CLIP의 기존 결과를 알맞게 섞어 줍니다!  


#### 🔬 성능 실험!!   

##### CLIP-Adapter 실험 세팅!  

1. 📊 사용한 데이터셋

  CLIP-Adapter는 총 11개의 이미지 분류 데이터셋에서 성능을 평가했습니다:

  - **ImageNet**
  - **StanfordCars**
  - **UCF101**
  - **Caltech101**
  - **Flowers102**
  - **SUN397**
  - **DTD**
  - **EuroSAT**
  - **FGVCAircraft**
  - **OxfordPets**
  - **Food101**

  각 데이터셋에 대해 **1, 2, 4, 8, 16-shot** 설정으로 fine-tuning을 수행하고,  
  **전체 테스트 세트**에서 성능을 측정합니다.  
  모든 실험은 **NVIDIA A100 GPU 단일 장비**에서 수행되며,  
  **각 실험은 3회 반복하여 평균 정확도**를 산출합니다!!  

2. ⚙️ 구현 세부 설정

- **기본 구조**: 이미지 특성만 fine-tune (visual adapter), 텍스트(branch)는 고정  

- **하이퍼파라미터**:
  - 배치 사이즈: `32`
  - 학습률: `1e-5`
  - **잔차 비율 α, β**는 각 데이터셋마다 탐색을 통해 선택 (grid search)

- **백본(backbone)**:
  - Visual encoder: `ResNet-50`
  - Text encoder: `12-layer Transformer`

- **어댑터 hidden embedding**: 시각/텍스트 어댑터 모두 `256` (기존 임베딩의 1/4)

- **프롬프트 입력**:
  - CoOp과 달리, **고정 텍스트 프롬프트** 사용  
    예: `"a photo of a {class}"`
  - 세밀한 분류에는 도메인 키워드를 포함  
    예: `"a centered satellite photo of {class}"`


##### CLIP-Adapter 실험 결과 분석!!  

1. 기본 실험
 - CLIP-Adapter는 성능을 비교하기 위해 다음 3가지 주요 베이스라인과 비교 실험을 진행했습니다!  

- Zero-shot CLIP : CLIP 모델 그대로, `a photo of {class}` 로 프롬포트사용
- Linear probe CLIP : CLIP의 이미지 인코더는 고정시키고, 그 위에 **얕은 선형 분류기(linear classifier)**만 학습.
- CoOp (Context Optimization) : 텍스트 프롬포트에 대하여 V1 V2를 추가하여 학습  

![res_compare](https://github.com/user-attachments/assets/7418df5c-fb3e-42f7-aa99-1127700bd362)

CLIP-Adapter 결곡 좋은 성능을 보여주었습니다!!  
위 이미지에서 보듯, 짧은 학습, 적은 parameter및 GPU메모리 빠른 속도에 높은 정확도를 보여줬는데요!  
뿐만아니라 적은 데이터셋 학습 (few shot) 에서도 좋았어요!!

2. 어뎁터는 어디에!?  

추가로 어뎁터를 `이미지만`, `텍스트만`, `이미지랑 텍스트 모두` 에 붙이는 비교도 해보았고!!

![adaptersto](https://github.com/user-attachments/assets/c58fa9d4-9704-46fa-8f97-574c20601cd9)

결국 이미지만 하는게 제일 좋았다고합니다!!  

![where](https://github.com/user-attachments/assets/7930d693-3340-4df0-bfbf-a6af0399dd97)

또한 12개 Transformer레이어로 구성된 CLIP 의 앞부분, 중간부분 등에 붙이는것도 테스트해보았고,  
지금까지 이해한것 처럼 CLIP의 맨 뒷부분,  
즉 12번쨰 레이어(CLIP이 12개 Layer로 구성) 뒤에 붙이는 것이 가장 효율이 좋았습니다!!


3. 잔차 학습의 계수는?!  
- 오버피팅을 막기위한 `Residual Connection`의 계수 평가를 진행했고!!

  a. 세밀한 도메인의 fine-grained 데이터셋의 경우는 최적의 α 값이 보통 0.6 ~ 0.8 수준에!,  

  b. Caltech-101이나 ImageNet처럼 포괄적이고 일반적인 이미지 데이터셋에서는 최적 α 값이 약 0.2 수준이었다고 합니다!  


---

#### 🧠 마무리 생각

LORA에 이어 두번째로 공부해본 PEFT (Parameter Efficient Fine Tuning) 기법!!  
시도도 참신할 뿐만아니라 성능도 인상적이서!  
앞으로 이 방식을 기억해서 여러곳에 사용해봐야겠습니다!!  

\+ 어뎁터하면 전기콘센트 어뎁터만 떠올랐는데, 앞으로는 이 CLIP-Adapter가 기억에 남을것 같네요! :)

---
