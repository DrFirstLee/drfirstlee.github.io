---
layout: post
title: "📝Understanding EZ-HOI - EZ-HOI 알아보기!!"
author: [DrFirst]
date: 2025-06-18 07:00:00 +0900
categories: [AI, Research]
tags: [HOI, NeurIPS, NeurIPS 2024]
sitemap :
  changefreq : monthly
  priority : 0.8
---


---

### 🧠 (한국어) EZ-HOI 알아보기?!!  
_🔍 Zero shot, Unseen을 위한 완벽한 프롬포트 만들기!!_  

![manhwa](https://github.com/user-attachments/assets/b1a9d05f-8601-4af6-8ba2-09d76271ecda)

> 논문: [EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection](https://arxiv.org/pdf/2410.23904)  
> 발표: NeurIPS 2024 (Lei, Wang, et al.)  
> 코드: [ChelsieLei/EZ-HOI](https://github.com/ChelsieLei/EZ-HOI)  

---


#### 📌 배경: HOI와 VLM 결합 연구의 한계!?

**Human-Object Interaction (HOI)**란!!  
이미지 또는 비디오에서 사람(Human)과 객체(Object)의 쌍을 찾아고, 이들 사이의 상호작용(Interaction)을 분류하는 작업입니다.  

![existings](https://github.com/user-attachments/assets/146faa77-b777-4d67-834c-f07652af5016)

##### ❓ 문제 a: VLM과 연계하는 HOI 연구!  
> 너무 모델이 크고 세세한 부분까지 파악은 어렵다는 단점!!  

최근의 HOI 연구들은 **Vision-Language Models (VLMs)**을 많이 활용했는데,  
대표적인 것이 HOI 검출기와 VLM의 특징 벡터를 정렬(alignment)시켜 **행동(action)**과 같은 개념을 양쪽 모델이 유사하게 이해할 수 있도록 만드는 방법이었음!!  
이를 통해 정렬된 특징은 제로샷(zero-shot) 상황에서도 모델이 본 적 없는 상호작용도 이해할수 있었지만!!  
아래와 같은 단점들이 있었음  

- 💸 **고비용의 정렬 학습 과정**: VLM과의 정렬은 대개 트랜스포머 구조 기반으로, 연산 비용/학습 시간 등이 큰 문제!  
- 🔒 **제로샷 일반화의 어려움**:  VLM 정렬은 학습된 클래스(Seen classes)에만 최적화되어, **보지 못한 클래스(Unseen classes)**에 대한 예측 성능이 낮음!  
- 🧠 **지식 전이의 한계**: VLM은 넓은 개념은 잘 이해하지만, HOI처럼 사람의 미세한 행동 차이를 구분해야 하는 과제에는 약점이 있음!  

##### ❗ 문제 b: 프롬포트만을 튜닝해서 가벼운 학습!!  
> 다만, 프롬포트 튜닝은 Seen위주로만 진행되어 Unseen에서는 성능이 좋지않음!  

이에, 최근에는 정렬 과정을 생략하고, VLM의 표현력을 그대로 활용하는 **프롬프트 튜닝(prompt tuning)** 기반 접근 방식이 대안으로 주목받고 있지만,  
이 또한 제로샷 문제에서는 아직 충분한 성과를 보여주지 못했음!! 

> 참고 : VLM의 표현력을 그대로 활용하는 **프롬프트 튜닝(prompt tuning)** 기반 접근 방식 이란!?  
> "A photo of a cat" 를 "[P1] [P2] [P3] cat"  와 같이 넣고 P1 P2 P3을 학습시킴!  
> 논문에서 예를든 MaPLe의 프롬포트 튜닝은 이미지와 텍스트를 함꼐 튜닝함!!  

- 결과적으로, **HOI와 VLM의 결합은 유망하지만**, **가벼운 모델&일반화 능력 확보**라는 한계가 있었습니다!  

---

#### 💡 EZ-HOI 등장!!!   

##### 🧩 Inference (추론)
> 사전 Fine Tuning 된 learnable prompt가 기존 foundation 모델과 결합되어 쓰임!!  
> 그래서 기존 foundation 모델은 학습된것 없는, 프롬포트 튜닝 기반의 Zero shot!!

![ezHOI_structure](https://github.com/user-attachments/assets/1fc6c8d8-6705-4878-a55c-e222700218de)  

```text
[Input] 단일 이미지
    ↓
Stage 1: Human-Object Detection
    - 사람과 모든 객체 bbox 추출
    - 가능한 모든 (human, object) pair 생성

Stage 2: HOI 인식
    - 각 human-object pair → CLIP의 visual encoder + vision learnable prompt  → 이미지 임베딩 (f_vis)
    - 모든 HOI 클래스의 (object-action pair) → CLIP의 text encoder + test learnable prompt → 텍스트 임베딩 (f_txt)
    - cosine similarity(f_vis, f_txt) 기반으로 가장 유사한 HOI class 선택
    → 최종 HOI 예측
```

---

##### 🛠️ Training (학습)

1. LLM 기반 HOI 클래스 설명 생성  
- 모든 object-interaction (HOI class) 쌍에 대해 LLM으로 풍부한 문장 생성  
  `"Swinging a baseball bat describes a person..."`

2. VLM 기반의 이미지 프롬포트 (VLM Guidance)  

```
→ Cross-Attention (이미지 MHCA, Multi-Head Cross Attention, 초기회되어 시작 후 학슴됨)  
  - Q: vision learnable prompt (초기화되어 시작 후 학습됨)  
  - K/V: CLIP(VLM)으로 인코딩된 벡터 (Unseen의 경우 llm으로 생성된 설명을 인코딩)
→ Attention 결과물이 CLIP(VLM)으로 인코딩결과과 유사해지도록 MHCA 및 Learnable prompt 학습
```
 -  vision MHCA는 vision프롬포트+Vision MHCA의 결과가 LLM이 만든 unseen의 설명 임베딩과 유사하도록!!  

3. Seen 클래스 학습  
> 이때 Seen Class의 learnable prompt와 MHCA weight가 정해짐!!  

```
→ Cross-Attention (텍스트 MHCA, Multi-Head Cross Attention, 초기회되어 시작 후 학슴됨)  
  - Q: text learnable prompt (초기화되어 시작 후 학습됨)  
  - K/V: LLM 설명의 토큰 임베딩  
→ Attention 결과물을 이미지 임베딩과 유사하도록 학습 (cosine similarity 기반)  
```

 -  text MHCA는 텍스트프롬포트+MHCA의 결과가 이미지 임베딩과 유사하도록 되는것 (SEEN)위주!!  


4. Unseen Class 학습 : 3단게!! (UPTL: Unseen Text Prompt Learning)
> 이때 Seen Class의 learnable prompt와 MHCA weight를 바탕으로 Unseen Class의 learnable prompt가 정해짐!!  

1단계: Cross-Attention (MHCA) - Seen 에서 정해진 MHCA weight  
    - Q: learnable prompt (가장 유사한 Seen Class의 최종 learnable prompt로 시작)  
    - K/V: Unseen class의 LLM 설명의 토큰 임베딩  
  → Attention 결과물을 유사 seen class의 prompt 결과와 유사하도록 학습 (cosine similarity 기반)  

2단계: Class-relation 학습 - Seen과 Unseen의 LLM 설명 임베딩끼리의 유사도 만큼 learnable prompt가 유사하게 되도록 학습!  

3단계: Negative 학습 - Seen class의 이미지 인코딩과 Unclass의 learnable prompt가 멀어지도록 학습  

이떄! learnable prompt가 처음에 한번 들어가는게 아니라 layer별로 나눠서 들어간다!!  

#####  Deep Visual-Text Prompt Learning

- 기존에는 단순히 입력되는 프롬포트를 튜닝했다면(인코더 입력 앞단에 고정된 토큰 추가),  
- 이번 연구는 text 및 Vision 인코더의 Transformer 레이어에 각각의 learnable 프롬포트를 삽입!  

✅ **기본 Prompt Tuning vs. Deep Visual-Text Prompt Learning**

| 항목         | 기본 Prompt Tuning | Deep Visual-Text Prompt Learning |
|--------------|--------------------|----------------------------------|
| **적용 위치**   | 인코더 입력 앞단에 고정된 토큰 추가 | 인코더의 모든 Transformer 레이어에 학습 가능한 프롬프트 삽입 |
| **학습 대상**   | 보통 몇 개의 학습 가능한 프롬프트 벡터만 튜닝 | 레이어별로 텍스트/비주얼 프롬프트 전체 시퀀스 학습 |
| **표현력**     | 제한적 (shallow), 상류 정보만 조절 | 깊은 위치의 표현까지 조절 가능 (deep) |
| **유연성**     | 단순 구조로 빠르게 튜닝 | 문맥/관계/복합 정보 반영 가능 (e.g. HOI) |
| **예: CLIP**  | 텍스트에만 토큰 삽입 → 단일 문장 의미 조절 | 텍스트 & 비주얼 둘 다 조정하며 시각-언어 정렬 자체를 재설계 |


---

🎯 **왜 Deep Visual-Text Prompt Learning 방식이 더 좋을까??**

---

###### 1. 레이어별 의미/기능 분화 고려

Transformer의 각 레이어는 서로 다른 수준의 의미를 담당합니다:

- **초기 레이어**: 저수준 (local) 특징  
- **중간 레이어**: 관계 (contextual) 정보  
- **마지막 레이어**: 개념적 추상 (high-level semantics)

➡️ 단순히 입력 앞에만 프롬프트를 붙이면, 이 모든 레이어에 정보를 전달하거나 조작하기 어렵습니다.

🔹 반면 **Deep Prompt Learning**은 각 레이어마다 **적절한 프롬프트를 삽입**함으로써, **계층별 의미 흐름을 미세하게 조절**할 수 있습니다.

---

###### 2. 시각/텍스트 모두에 적용 → Modal Alignment 개선

기존 Prompt Tuning은 주로 **텍스트 쪽**에만 프롬프트를 삽입합니다. 하지만:

- **HOI (Human-Object Interaction)**  
- **VQA (Visual Question Answering)**

처럼 **텍스트와 비주얼의 조합이 핵심인 작업**에서는,  
**시각 표현도 동시에 정렬/조절**해야 성능이 향상됩니다.

🔹 **Deep Visual-Text Prompt**는 **텍스트와 이미지 인코더 모두에 병렬적으로 프롬프트를 삽입**하여,  
**두 모달리티 간의 정렬 품질 (alignment)**을 높입니다.

---

###### 3. Fine-grained Control & Context Adaptation

프롬프트가 **각 레이어에 독립적으로 존재**하므로 다음이 가능합니다:

- 특정 작업 / 클래스 / 문맥에 맞는 **세부적 조정**  
- 프롬프트를 **HOI 클래스별로 다르게 학습**시켜 **세밀한 표현 제어**  
- 단순히 "이건 고양이야"가 아니라 → **"사람이 고양이를 안고 있다"** 같은 **복합 관계 표현**에 유리



---

#### 🔬 EZ-HOI의 성능 실험!!   

---

##### 1. 📘 Zero-Shot HOI 설정의 정의

- 기존 zero-shot HOI 방식과 동일하게, **unseen HOI 클래스의 이름을 훈련 중에 활용**  
- 기존 연구들:
  - **VCL, FCL, ATL**: unseen HOI 클래스 이름을 조합하여 새로운 샘플 구성
  - **EoID**: CLIP을 사전 정의된 HOI 프롬프트로 디스틸 (seen + unseen 클래스)
  - **HOICLIP**: verb class representation 도입 (seen/unseen 포함)

---

##### 2. ⚙️ 구현 세부 설정

- **기본 구조**:  
  - DETR + ResNet-50 백본  
  - CLIP 기반 dual encoder 구조 (텍스트/비주얼 모두 프롬프트 삽입)  

- **하이퍼파라미터**:
  - 배치 사이즈: `16`
  - 학습률: `1e-3`
  - 옵티마이저: `AdamW`
  - GPU: `4 × Nvidia A5000`

- **백본(backbone)**:
  - Visual encoder: `DETR (ResNet-50)`
  - Text encoder: `LLaVA-v1.5-7b`로 설명 기반 프롬프트 생성

- **프롬프트 설계**:
  - 레이어 수: `N = 9`, 프롬프트 길이: `p = 2`
  - 학습 가능한 text & visual prompts를 Transformer 레이어마다 삽입

- **추가 기법**:
  - Intra-HOI fusion: 사람-객체 쌍의 feature 융합
  - Inter-HOI fusion: 이미지 내 여러 HOI 쌍 간 문맥 주입
  - LLM 기반 세밀 프롬프트 (텍스트 설명 포함)
  - Visual Adapter (참고: [27])
  - UTPL 모듈 (Unseen Text Prompt Learning)

---

##### 3. 실험결과 분석 및 Ablation Study  

![results](https://github.com/user-attachments/assets/9d9e6d9c-9c40-490e-8d03-e30ed0a3bf6b)  

- **Unseen-Verb Setting**  
  - 기존 대비 학습 가능한 파라미터 수 `최대 87.9% 감소`  
  - CLIP4HOI보다 성능은 약간 낮지만, 효율성 극대화  
  - UniHOI 대비 `2.77 mAP` 향상, 파라미터 수는 `26.9%` 수준

- **Unseen-Composition Setting (RF-UC / NF-UC)**  
  - CLIP4HOI 대비 모든 설정에서 성능 우수  
  - UniHOI 대비 RF-UC에서 `+5.56 mAP`, NF-UC에서 `+7.88 mAP`

- **Unseen-Object Setting**  
  - CLIP4HOI보다 `+1.49 mAP`, 파라미터 수는 `12.08%`  
  - UniHOI 대비 unseen 클래스에서 `+13.36 mAP`

- 🔬 Ablation Study 해석

![ablation](https://github.com/user-attachments/assets/8eda36ad-3552-4e75-a2c4-2fb44037f813)  

  | 항목 | 기능 설명 | 성능 변화 | 해석 |
  |------|-----------|------------|------|
  | **Intra-HOI Fusion** | 한 이미지 내에서 단일 사람-객체(H-O) 쌍 내부의 정보 결합 | seen `+7.41 mAP` | 한 쌍 내 사람/객체 관계를 더 정확히 포착함으로써, 학습된 클래스(seen)에 대한 인식 정밀도 크게 향상 |
  | **Visual Adapter** | 시각 인코더 각 레이어에 외부 정보(예: 위치, 클래스)를 삽입하는 모듈 | seen ↑ / unseen ↓ | seen 클래스에서는 이 정보가 도움이 되지만, unseen 클래스에는 과적합 가능성 → 일반화에 방해 요인 |
  | **LLM Guidance** | LLaVA 기반 언어모델이 생성한 정교한 텍스트 설명을 프롬프트에 사용 | unseen `+1.52 mAP` | 단순 클래스 이름보다 의미 기반의 묘사를 활용함으로써, 처음 보는 클래스에 대한 이해력 증가 |
  | **UTPL (Unseen Text Prompt Learning)** | unseen 전용 학습 프롬프트를 따로 훈련하는 구조 | unseen `+2.42 mAP` | 프롬프트가 seen에 편중되지 않게 하고, unseen을 위한 표현력을 직접 학습하게 하여 성능 강화 |
  | **Inter-HOI Fusion** | 여러 사람-객체 쌍 간 문맥을 공유하여 정보 보강 | seen/unseen 모두 향상 | 한 이미지 내의 다양한 관계들이 서로 간에 문맥적 도움을 주어, 전반적인 인식력과 분류 정확도 상승 |
  | **VLM Guidance** | CLIP 등 사전학습된 시각언어모델의 특성을 유도(align)하는 전략 | unseen `+1.33 mAP` | VLM의 일반화 성질을 프롬프트에 반영함으로써, 처음 보는 클래스에도 의미 유추 가능 |


---

#### 🧠 마무리 생각

Unseen 케이스에 대한 대비를 위한 연구!  
즉 Zero shot으로 처음 보는 상황에도 잘 적응 할수 있도록,  
미리 학습된 프롬포트를 넣는 연구!!  

1) Seen 케이스에 대하여 LLM으로 설명하고 이를 이미지 임베딩과 매칭될 수 있도록 텍스트 프롬포트를 학습시킨 다음,  
2) Unseen 케이스의 프롬포트는 유사한 Seen 프롬포트에서 시작해서, LLM의 Unseen 설명 및 유사한 Seen 이미지로 추가학습!  
3) LLM으로 Unseen 케이스에 대하여 설명하고, Seen case기반의 프럼포트가 Unseen의 VLM 임베딩과 매칭되도록 학습! 
4) 이를 통해서! Unseen 의 Zero shot을 위한 완벽한 프롬포트를 만든다!



---
