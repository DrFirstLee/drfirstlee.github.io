---
layout: post
title: "üìùUnderstanding EZ-HOI - EZ-HOI ÏïåÏïÑÎ≥¥Í∏∞!!"
author: [DrFirst]
date: 2025-06-18 07:00:00 +0900
categories: [AI, Research]
tags: [HOI, NeurIPS, NeurIPS 2024, prompt learning]
sitemap :
  changefreq : monthly
  priority : 0.8
---


---

### üß† (English) Understanding EZ-HOI?!!  
_üîç Creating Perfect Prompts for Zero-shot and Unseen Cases!!_  

![manhwa](https://github.com/user-attachments/assets/b1a9d05f-8601-4af6-8ba2-09d76271ecda)

> Paper: [EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection](https://arxiv.org/pdf/2410.23904)  
> Conference: NeurIPS 2024 (Lei, Wang, et al.)  
> Code: [ChelsieLei/EZ-HOI](https://github.com/ChelsieLei/EZ-HOI)  

---


#### üìå Background: Limitations of HOI and VLM Integration Research!?

**Human-Object Interaction (HOI)** refers to the task of finding pairs of humans and objects in images or videos and classifying the interactions between them.

![existings](https://github.com/user-attachments/assets/146faa77-b777-4d67-834c-f07652af5016)

##### ‚ùì Problem a: HOI Research with VLM Integration!  
> Models are too large and have difficulty capturing fine-grained details!!  

Recent HOI research has extensively utilized **Vision-Language Models (VLMs)**, with a representative approach being the alignment of feature vectors between HOI detectors and VLMs so that both models can similarly understand concepts like **actions**.  
Through this alignment, the features could understand previously unseen interactions even in zero-shot situations, but there were the following drawbacks:  

- üí∏ **High-cost alignment learning process**: VLM alignment is typically based on transformer structures, causing significant computational cost and training time issues!  
- üîí **Difficulty in zero-shot generalization**: VLM alignment is optimized only for trained classes (Seen classes), resulting in poor prediction performance for **unseen classes**!  
- üß† **Limitations in knowledge transfer**: While VLMs understand broad concepts well, they have weaknesses in tasks like HOI that require distinguishing subtle differences in human actions!  

##### ‚ùó Problem b: Lightweight learning by tuning only prompts!!  
> However, prompt tuning is mainly focused on Seen classes, resulting in poor performance on Unseen classes!  

Recently, **prompt tuning** based approaches that skip the alignment process and directly utilize VLM's representational power have gained attention as alternatives, but they still haven't shown sufficient results in zero-shot problems!! 

> Note: What is the **prompt tuning** based approach that directly utilizes VLM's representational power!?  
> It changes "A photo of a cat" to "[P1] [P2] [P3] cat" and trains P1 P2 P3!  
> The MaPLe prompt tuning mentioned in the paper tunes both image and text together!!  

- Consequently, while **the combination of HOI and VLM is promising**, there were limitations in **achieving lightweight models & generalization capabilities**!  

---

#### üí° EZ-HOI Emerges!!!   

##### üß© Inference
> Pre-fine-tuned learnable prompts are combined with existing foundation models!!  
> So the existing foundation models remain untrained, achieving zero-shot through prompt tuning!!

![ezHOI_structure](https://github.com/user-attachments/assets/1fc6c8d8-6705-4878-a55c-e222700218de)  

```text
[Input] Single image
    ‚Üì
Stage 1: Human-Object Detection
    - Extract bounding boxes for humans and all objects
    - Generate all possible (human, object) pairs

Stage 2: HOI Recognition
    - Each human-object pair ‚Üí CLIP's visual encoder + vision learnable prompt ‚Üí image embedding (f_vis)
    - All HOI classes (object-action pairs) ‚Üí CLIP's text encoder + text learnable prompt ‚Üí text embedding (f_txt)
    - Select the most similar HOI class based on cosine similarity(f_vis, f_txt)
    ‚Üí Final HOI prediction
```

---

##### üõ†Ô∏è Training

.1. LLM-based HOI Class Description Generation  
- Generate rich sentences using LLM for all object-interaction (HOI class) pairs  
  `"Swinging a baseball bat describes a person..."`

---

.2. VLM-based Image Prompts (VLM Guidance)  

```
‚Üí Cross-Attention (Image MHCA, Multi-Head Cross Attention, initialized and then trained)  
  - Q: vision learnable prompt (initialized and then trained)  
  - K/V: Vectors encoded by CLIP(VLM) (For unseen cases, descriptions generated by LLM are encoded)
‚Üí Train MHCA and learnable prompt so that attention results become similar to CLIP(VLM) encoding results
```
 - Vision MHCA ensures that the results of vision prompt + Vision MHCA become similar to unseen description embeddings created by LLM!!  

---

.3. Seen Class Training  
> At this point, learnable prompts and MHCA weights for Seen Classes are determined!!  

```
‚Üí Cross-Attention (Text MHCA, Multi-Head Cross Attention, initialized and then trained)  
  - Q: text learnable prompt (initialized and then trained)  
  - K/V: Token embeddings of LLM descriptions  
‚Üí Train to make attention results similar to image embeddings (based on cosine similarity)  
```

 - Text MHCA makes the results of text prompt + MHCA similar to image embeddings (mainly for SEEN)!!  

---

.4. Unseen Class Training: 3 stages!! (UPTL: Unseen Text Prompt Learning)
> At this point, learnable prompts for Unseen Classes are determined based on the learnable prompts and MHCA weights of Seen Classes!!  

Stage 1: Cross-Attention (MHCA) - MHCA weights determined from Seen classes  
    - Q: learnable prompt (starts with the final learnable prompt of the most similar Seen Class)  
    - K/V: Token embeddings of Unseen class LLM descriptions  
  ‚Üí Train to make attention results similar to similar seen class prompt results (based on cosine similarity)  

Stage 2: Class-relation learning - Train learnable prompts to be similar according to the similarity between Seen and Unseen LLM description embeddings!  

Stage 3: Negative learning - Train so that Seen class image encodings and Unseen class learnable prompts become distant  

Note! Learnable prompts are not inserted just once at the beginning, but are divided and inserted by layer!!  

---

##### Deep Visual-Text Prompt Learning

- While previous approaches simply tuned input prompts (adding fixed tokens at the front of encoder input),  
- This research inserts individual learnable prompts into each Transformer layer of text and vision encoders!  

‚úÖ **Basic Prompt Tuning vs. Deep Visual-Text Prompt Learning**

| Item         | Basic Prompt Tuning | Deep Visual-Text Prompt Learning |
|--------------|--------------------|----------------------------------|
| **Application Location**   | Add fixed tokens at the front of encoder input | Insert learnable prompts into all Transformer layers of the encoder |
| **Learning Target**   | Usually tune only a few learnable prompt vectors | Learn entire sequences of text/visual prompts layer by layer |
| **Expressiveness**     | Limited (shallow), only controls upstream information | Can control representations at deep positions (deep) |
| **Flexibility**     | Fast tuning with simple structure | Can reflect context/relationship/complex information (e.g. HOI) |
| **Example: CLIP**  | Token insertion only in text ‚Üí control single sentence meaning | Adjust both text & visual, redesigning vision-language alignment itself |


---

üéØ **Why is Deep Visual-Text Prompt Learning Better??**

---

###### 1. Considering Layer-wise Semantic/Functional Differentiation

Each layer of Transformer handles different levels of meaning:

- **Early layers**: Low-level (local) features  
- **Middle layers**: Relational (contextual) information  
- **Final layers**: Conceptual abstraction (high-level semantics)

‚û°Ô∏è Simply attaching prompts only at the input makes it difficult to convey or manipulate information to all these layers.

üîπ In contrast, **Deep Prompt Learning** can **finely control hierarchical semantic flow** by **inserting appropriate prompts at each layer**.

---

###### 2. Application to Both Visual/Text ‚Üí Improved Modal Alignment

Existing Prompt Tuning mainly inserts prompts only on the **text side**. However:

- **HOI (Human-Object Interaction)**  
- **VQA (Visual Question Answering)**

In tasks where **the combination of text and visual is key**,  
**visual representations must also be simultaneously aligned/controlled** to improve performance.

üîπ **Deep Visual-Text Prompt** **inserts prompts in parallel to both text and image encoders**,  
improving **alignment quality between the two modalities**.

---

###### 3. Fine-grained Control & Context Adaptation

Since prompts **exist independently at each layer**, the following becomes possible:

- **Detailed adjustments** for specific tasks / classes / contexts  
- **Learning prompts differently for each HOI class** to achieve **fine-grained expression control**  
- Advantageous for **complex relational expressions** like **"a person holding a cat"** rather than simply "this is a cat"



---

#### üî¨ EZ-HOI Performance Experiments!!   

---

##### 1. üìò Definition of Zero-Shot HOI Setting

- Similar to existing zero-shot HOI methods, **utilize names of unseen HOI classes during training**  
- Previous studies:
  - **VCL, FCL, ATL**: Compose new samples by combining unseen HOI class names
  - **EoID**: Distill CLIP with predefined HOI prompts (seen + unseen classes)
  - **HOICLIP**: Introduce verb class representation (including seen/unseen)

---

##### 2. ‚öôÔ∏è Implementation Details

- **Basic Structure**:  
  - DETR + ResNet-50 backbone  
  - CLIP-based dual encoder structure (prompt insertion in both text/visual)  

- **Hyperparameters**:
  - Batch size: `16`
  - Learning rate: `1e-3`
  - Optimizer: `AdamW`
  - GPU: `4 √ó Nvidia A5000`

- **Backbone**:
  - Visual encoder: `DETR (ResNet-50)`
  - Text encoder: Description-based prompt generation with `LLaVA-v1.5-7b`

- **Prompt Design**:
  - Number of layers: `N = 9`, Prompt length: `p = 2`
  - Insert learnable text & visual prompts into each Transformer layer

- **Additional Techniques**:
  - Intra-HOI fusion: Feature fusion of human-object pairs
  - Inter-HOI fusion: Context injection between multiple HOI pairs within an image
  - LLM-based fine-grained prompts (including text descriptions)
  - Visual Adapter (ref: [27])
  - UTPL module (Unseen Text Prompt Learning)

---

##### 3. Experimental Results Analysis and Ablation Study  

![results](https://github.com/user-attachments/assets/9d9e6d9c-9c40-490e-8d03-e30ed0a3bf6b)  

- **Unseen-Verb Setting**  
  - `Up to 87.9% reduction` in trainable parameters compared to existing methods  
  - Slightly lower performance than CLIP4HOI, but maximizes efficiency  
  - `2.77 mAP` improvement over UniHOI, with parameter count at `26.9%` level

- **Unseen-Composition Setting (RF-UC / NF-UC)**  
  - Superior performance in all settings compared to CLIP4HOI  
  - `+5.56 mAP` in RF-UC and `+7.88 mAP` in NF-UC compared to UniHOI

- **Unseen-Object Setting**  
  - `+1.49 mAP` over CLIP4HOI, with parameter count at `12.08%`  
  - `+13.36 mAP` in unseen classes compared to UniHOI

- üî¨ Ablation Study Interpretation

![ablation](https://github.com/user-attachments/assets/8eda36ad-3552-4e75-a2c4-2fb44037f813)  

  | Component | Function Description | Performance Change | Interpretation |
  |------|-----------|------------|------|
  | **Intra-HOI Fusion** | Information combination within a single human-object (H-O) pair in an image | seen `+7.41 mAP` | Significantly improves recognition precision for learned classes (seen) by more accurately capturing human/object relationships within a pair |
  | **Visual Adapter** | Module that inserts external information (e.g., position, class) into each layer of the visual encoder | seen ‚Üë / unseen ‚Üì | This information helps with seen classes, but may cause overfitting for unseen classes ‚Üí hindrance to generalization |
  | **LLM Guidance** | Uses sophisticated text descriptions generated by LLaVA-based language model in prompts | unseen `+1.52 mAP` | Increases understanding of previously unseen classes by utilizing semantic-based descriptions rather than simple class names |
  | **UTPL (Unseen Text Prompt Learning)** | Structure that separately trains prompts dedicated to unseen classes | unseen `+2.42 mAP` | Prevents prompts from being biased toward seen classes and directly learns expressiveness for unseen classes, enhancing performance |
  | **Inter-HOI Fusion** | Information enhancement by sharing context between multiple human-object pairs | Both seen/unseen improved | Various relationships within an image provide contextual help to each other, increasing overall recognition and classification accuracy |
  | **VLM Guidance** | Strategy to induce (align) characteristics of pre-trained vision-language models like CLIP | unseen `+1.33 mAP` | Enables semantic inference for previously unseen classes by reflecting VLM's generalization properties in prompts |


---

#### üß† Final Thoughts

Research to prepare for unseen cases!  
That is, research to insert pre-trained prompts so that they can adapt well to previously unseen situations in zero-shot scenarios!!  

1) For seen cases, describe them with LLM and train text prompts to match image embeddings,  
2) For unseen cases, start with prompts from similar seen cases and perform additional training with LLM's unseen descriptions and similar seen images!  
3) Describe unseen cases with LLM and train seen case-based prompts to match unseen VLM embeddings! 
4) Through this! Create perfect prompts for zero-shot unseen cases!



---


---

### üß† (ÌïúÍµ≠Ïñ¥) EZ-HOI ÏïåÏïÑÎ≥¥Í∏∞?!!  
_üîç Zero shot, UnseenÏùÑ ÏúÑÌïú ÏôÑÎ≤ΩÌïú ÌîÑÎ°¨Ìè¨Ìä∏ ÎßåÎì§Í∏∞!!_  

![manhwa](https://github.com/user-attachments/assets/b1a9d05f-8601-4af6-8ba2-09d76271ecda)

> ÎÖºÎ¨∏: [EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection](https://arxiv.org/pdf/2410.23904)  
> Î∞úÌëú: NeurIPS 2024 (Lei, Wang, et al.)  
> ÏΩîÎìú: [ChelsieLei/EZ-HOI](https://github.com/ChelsieLei/EZ-HOI)  

---


#### üìå Î∞∞Í≤Ω: HOIÏôÄ VLM Í≤∞Ìï© Ïó∞Íµ¨Ïùò ÌïúÍ≥Ñ!?

**Human-Object Interaction (HOI)**ÎûÄ!!  
Ïù¥ÎØ∏ÏßÄ ÎòêÎäî ÎπÑÎîîÏò§ÏóêÏÑú ÏÇ¨Îûå(Human)Í≥º Í∞ùÏ≤¥(Object)Ïùò ÏåçÏùÑ Ï∞æÏïÑÍ≥†, Ïù¥Îì§ ÏÇ¨Ïù¥Ïùò ÏÉÅÌò∏ÏûëÏö©(Interaction)ÏùÑ Î∂ÑÎ•òÌïòÎäî ÏûëÏóÖÏûÖÎãàÎã§.  

![existings](https://github.com/user-attachments/assets/146faa77-b777-4d67-834c-f07652af5016)

##### ‚ùì Î¨∏Ï†ú a: VLMÍ≥º Ïó∞Í≥ÑÌïòÎäî HOI Ïó∞Íµ¨!  
> ÎÑàÎ¨¥ Î™®Îç∏Ïù¥ ÌÅ¨Í≥† ÏÑ∏ÏÑ∏Ìïú Î∂ÄÎ∂ÑÍπåÏßÄ ÌååÏïÖÏùÄ Ïñ¥Î†µÎã§Îäî Îã®Ï†ê!!  

ÏµúÍ∑ºÏùò HOI Ïó∞Íµ¨Îì§ÏùÄ **Vision-Language Models (VLMs)**ÏùÑ ÎßéÏù¥ ÌôúÏö©ÌñàÎäîÎç∞,  
ÎåÄÌëúÏ†ÅÏù∏ Í≤ÉÏù¥ HOI Í≤ÄÏ∂úÍ∏∞ÏôÄ VLMÏùò ÌäπÏßï Î≤°ÌÑ∞Î•º Ï†ïÎ†¨(alignment)ÏãúÏºú **ÌñâÎèô(action)**Í≥º Í∞ôÏùÄ Í∞úÎÖêÏùÑ ÏñëÏ™Ω Î™®Îç∏Ïù¥ Ïú†ÏÇ¨ÌïòÍ≤å Ïù¥Ìï¥Ìï† Ïàò ÏûàÎèÑÎ°ù ÎßåÎìúÎäî Î∞©Î≤ïÏù¥ÏóàÏùå!!  
Ïù¥Î•º ÌÜµÌï¥ Ï†ïÎ†¨Îêú ÌäπÏßïÏùÄ Ï†úÎ°úÏÉ∑(zero-shot) ÏÉÅÌô©ÏóêÏÑúÎèÑ Î™®Îç∏Ïù¥ Î≥∏ Ï†Å ÏóÜÎäî ÏÉÅÌò∏ÏûëÏö©ÎèÑ Ïù¥Ìï¥Ìï†Ïàò ÏûàÏóàÏßÄÎßå!!  
ÏïÑÎûòÏôÄ Í∞ôÏùÄ Îã®Ï†êÎì§Ïù¥ ÏûàÏóàÏùå  

- üí∏ **Í≥†ÎπÑÏö©Ïùò Ï†ïÎ†¨ ÌïôÏäµ Í≥ºÏ†ï**: VLMÍ≥ºÏùò Ï†ïÎ†¨ÏùÄ ÎåÄÍ∞ú Ìä∏ÎûúÏä§Ìè¨Î®∏ Íµ¨Ï°∞ Í∏∞Î∞òÏúºÎ°ú, Ïó∞ÏÇ∞ ÎπÑÏö©/ÌïôÏäµ ÏãúÍ∞Ñ Îì±Ïù¥ ÌÅ∞ Î¨∏Ï†ú!  
- üîí **Ï†úÎ°úÏÉ∑ ÏùºÎ∞òÌôîÏùò Ïñ¥Î†§ÏõÄ**:  VLM Ï†ïÎ†¨ÏùÄ ÌïôÏäµÎêú ÌÅ¥ÎûòÏä§(Seen classes)ÏóêÎßå ÏµúÏ†ÅÌôîÎêòÏñ¥, **Î≥¥ÏßÄ Î™ªÌïú ÌÅ¥ÎûòÏä§(Unseen classes)**Ïóê ÎåÄÌïú ÏòàÏ∏° ÏÑ±Îä•Ïù¥ ÎÇÆÏùå!  
- üß† **ÏßÄÏãù Ï†ÑÏù¥Ïùò ÌïúÍ≥Ñ**: VLMÏùÄ ÎÑìÏùÄ Í∞úÎÖêÏùÄ Ïûò Ïù¥Ìï¥ÌïòÏßÄÎßå, HOIÏ≤òÎüº ÏÇ¨ÎûåÏùò ÎØ∏ÏÑ∏Ìïú ÌñâÎèô Ï∞®Ïù¥Î•º Íµ¨Î∂ÑÌï¥Ïïº ÌïòÎäî Í≥ºÏ†úÏóêÎäî ÏïΩÏ†êÏù¥ ÏûàÏùå!  

##### ‚ùó Î¨∏Ï†ú b: ÌîÑÎ°¨Ìè¨Ìä∏ÎßåÏùÑ ÌäúÎãùÌï¥ÏÑú Í∞ÄÎ≤ºÏö¥ ÌïôÏäµ!!  
> Îã§Îßå, ÌîÑÎ°¨Ìè¨Ìä∏ ÌäúÎãùÏùÄ SeenÏúÑÏ£ºÎ°úÎßå ÏßÑÌñâÎêòÏñ¥ UnseenÏóêÏÑúÎäî ÏÑ±Îä•Ïù¥ Ï¢ãÏßÄÏïäÏùå!  

Ïù¥Ïóê, ÏµúÍ∑ºÏóêÎäî Ï†ïÎ†¨ Í≥ºÏ†ïÏùÑ ÏÉùÎûµÌïòÍ≥†, VLMÏùò ÌëúÌòÑÎ†•ÏùÑ Í∑∏ÎåÄÎ°ú ÌôúÏö©ÌïòÎäî **ÌîÑÎ°¨ÌîÑÌä∏ ÌäúÎãù(prompt tuning)** Í∏∞Î∞ò Ï†ëÍ∑º Î∞©ÏãùÏù¥ ÎåÄÏïàÏúºÎ°ú Ï£ºÎ™©Î∞õÍ≥† ÏûàÏßÄÎßå,  
Ïù¥ ÎòêÌïú Ï†úÎ°úÏÉ∑ Î¨∏Ï†úÏóêÏÑúÎäî ÏïÑÏßÅ Ï∂©Î∂ÑÌïú ÏÑ±Í≥ºÎ•º Î≥¥Ïó¨Ï£ºÏßÄ Î™ªÌñàÏùå!! 

> Ï∞∏Í≥† : VLMÏùò ÌëúÌòÑÎ†•ÏùÑ Í∑∏ÎåÄÎ°ú ÌôúÏö©ÌïòÎäî **ÌîÑÎ°¨ÌîÑÌä∏ ÌäúÎãù(prompt tuning)** Í∏∞Î∞ò Ï†ëÍ∑º Î∞©Ïãù Ïù¥ÎûÄ!?  
> "A photo of a cat" Î•º "[P1] [P2] [P3] cat"  ÏôÄ Í∞ôÏù¥ ÎÑ£Í≥† P1 P2 P3ÏùÑ ÌïôÏäµÏãúÌÇ¥!  
> ÎÖºÎ¨∏ÏóêÏÑú ÏòàÎ•ºÎì† MaPLeÏùò ÌîÑÎ°¨Ìè¨Ìä∏ ÌäúÎãùÏùÄ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÖçÏä§Ìä∏Î•º Ìï®Íºê ÌäúÎãùÌï®!!  

- Í≤∞Í≥ºÏ†ÅÏúºÎ°ú, **HOIÏôÄ VLMÏùò Í≤∞Ìï©ÏùÄ Ïú†ÎßùÌïòÏßÄÎßå**, **Í∞ÄÎ≤ºÏö¥ Î™®Îç∏&ÏùºÎ∞òÌôî Îä•Î†• ÌôïÎ≥¥**ÎùºÎäî ÌïúÍ≥ÑÍ∞Ä ÏûàÏóàÏäµÎãàÎã§!  

---

#### üí° EZ-HOI Îì±Ïû•!!!   

##### üß© Inference (Ï∂îÎ°†)
> ÏÇ¨Ï†Ñ Fine Tuning Îêú learnable promptÍ∞Ä Í∏∞Ï°¥ foundation Î™®Îç∏Í≥º Í≤∞Ìï©ÎêòÏñ¥ Ïì∞ÏûÑ!!  
> Í∑∏ÎûòÏÑú Í∏∞Ï°¥ foundation Î™®Îç∏ÏùÄ ÌïôÏäµÎêúÍ≤É ÏóÜÎäî, ÌîÑÎ°¨Ìè¨Ìä∏ ÌäúÎãù Í∏∞Î∞òÏùò Zero shot!!

![ezHOI_structure](https://github.com/user-attachments/assets/1fc6c8d8-6705-4878-a55c-e222700218de)  

```text
[Input] Îã®Ïùº Ïù¥ÎØ∏ÏßÄ
    ‚Üì
Stage 1: Human-Object Detection
    - ÏÇ¨ÎûåÍ≥º Î™®Îì† Í∞ùÏ≤¥ bbox Ï∂îÏ∂ú
    - Í∞ÄÎä•Ìïú Î™®Îì† (human, object) pair ÏÉùÏÑ±

Stage 2: HOI Ïù∏Ïãù
    - Í∞Å human-object pair ‚Üí CLIPÏùò visual encoder + vision learnable prompt  ‚Üí Ïù¥ÎØ∏ÏßÄ ÏûÑÎ≤†Îî© (f_vis)
    - Î™®Îì† HOI ÌÅ¥ÎûòÏä§Ïùò (object-action pair) ‚Üí CLIPÏùò text encoder + test learnable prompt ‚Üí ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî© (f_txt)
    - cosine similarity(f_vis, f_txt) Í∏∞Î∞òÏúºÎ°ú Í∞ÄÏû• Ïú†ÏÇ¨Ìïú HOI class ÏÑ†ÌÉù
    ‚Üí ÏµúÏ¢Ö HOI ÏòàÏ∏°
```

---

##### üõ†Ô∏è Training (ÌïôÏäµ)

.1. LLM Í∏∞Î∞ò HOI ÌÅ¥ÎûòÏä§ ÏÑ§Î™Ö ÏÉùÏÑ±  
- Î™®Îì† object-interaction (HOI class) ÏåçÏóê ÎåÄÌï¥ LLMÏúºÎ°ú ÌíçÎ∂ÄÌïú Î¨∏Ïû• ÏÉùÏÑ±  
  `"Swinging a baseball bat describes a person..."`

---

.2. VLM Í∏∞Î∞òÏùò Ïù¥ÎØ∏ÏßÄ ÌîÑÎ°¨Ìè¨Ìä∏ (VLM Guidance)  

```
‚Üí Cross-Attention (Ïù¥ÎØ∏ÏßÄ MHCA, Multi-Head Cross Attention, Ï¥àÍ∏∞ÌöåÎêòÏñ¥ ÏãúÏûë ÌõÑ ÌïôÏä¥Îê®)  
  - Q: vision learnable prompt (Ï¥àÍ∏∞ÌôîÎêòÏñ¥ ÏãúÏûë ÌõÑ ÌïôÏäµÎê®)  
  - K/V: CLIP(VLM)ÏúºÎ°ú Ïù∏ÏΩîÎî©Îêú Î≤°ÌÑ∞ (UnseenÏùò Í≤ΩÏö∞ llmÏúºÎ°ú ÏÉùÏÑ±Îêú ÏÑ§Î™ÖÏùÑ Ïù∏ÏΩîÎî©)
‚Üí Attention Í≤∞Í≥ºÎ¨ºÏù¥ CLIP(VLM)ÏúºÎ°ú Ïù∏ÏΩîÎî©Í≤∞Í≥ºÍ≥º Ïú†ÏÇ¨Ìï¥ÏßÄÎèÑÎ°ù MHCA Î∞è Learnable prompt ÌïôÏäµ
```
 -  vision MHCAÎäî visionÌîÑÎ°¨Ìè¨Ìä∏+Vision MHCAÏùò Í≤∞Í≥ºÍ∞Ä LLMÏù¥ ÎßåÎì† unseenÏùò ÏÑ§Î™Ö ÏûÑÎ≤†Îî©Í≥º Ïú†ÏÇ¨ÌïòÎèÑÎ°ù!!  

---

.3. Seen ÌÅ¥ÎûòÏä§ ÌïôÏäµ  
> Ïù¥Îïå Seen ClassÏùò learnable promptÏôÄ MHCA weightÍ∞Ä Ï†ïÌï¥Ïßê!!  

```
‚Üí Cross-Attention (ÌÖçÏä§Ìä∏ MHCA, Multi-Head Cross Attention, Ï¥àÍ∏∞ÌöåÎêòÏñ¥ ÏãúÏûë ÌõÑ ÌïôÏä¥Îê®)  
  - Q: text learnable prompt (Ï¥àÍ∏∞ÌôîÎêòÏñ¥ ÏãúÏûë ÌõÑ ÌïôÏäµÎê®)  
  - K/V: LLM ÏÑ§Î™ÖÏùò ÌÜ†ÌÅ∞ ÏûÑÎ≤†Îî©  
‚Üí Attention Í≤∞Í≥ºÎ¨ºÏùÑ Ïù¥ÎØ∏ÏßÄ ÏûÑÎ≤†Îî©Í≥º Ïú†ÏÇ¨ÌïòÎèÑÎ°ù ÌïôÏäµ (cosine similarity Í∏∞Î∞ò)  
```

 -  text MHCAÎäî ÌÖçÏä§Ìä∏ÌîÑÎ°¨Ìè¨Ìä∏+MHCAÏùò Í≤∞Í≥ºÍ∞Ä Ïù¥ÎØ∏ÏßÄ ÏûÑÎ≤†Îî©Í≥º Ïú†ÏÇ¨ÌïòÎèÑÎ°ù ÎêòÎäîÍ≤É (SEEN)ÏúÑÏ£º!!  

---

.4. Unseen Class ÌïôÏäµ : 3Îã®Í≤å!! (UPTL: Unseen Text Prompt Learning)
> Ïù¥Îïå Seen ClassÏùò learnable promptÏôÄ MHCA weightÎ•º Î∞îÌÉïÏúºÎ°ú Unseen ClassÏùò learnable promptÍ∞Ä Ï†ïÌï¥Ïßê!!  

1Îã®Í≥Ñ: Cross-Attention (MHCA) - Seen ÏóêÏÑú Ï†ïÌï¥ÏßÑ MHCA weight  
    - Q: learnable prompt (Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Seen ClassÏùò ÏµúÏ¢Ö learnable promptÎ°ú ÏãúÏûë)  
    - K/V: Unseen classÏùò LLM ÏÑ§Î™ÖÏùò ÌÜ†ÌÅ∞ ÏûÑÎ≤†Îî©  
  ‚Üí Attention Í≤∞Í≥ºÎ¨ºÏùÑ Ïú†ÏÇ¨ seen classÏùò prompt Í≤∞Í≥ºÏôÄ Ïú†ÏÇ¨ÌïòÎèÑÎ°ù ÌïôÏäµ (cosine similarity Í∏∞Î∞ò)  

2Îã®Í≥Ñ: Class-relation ÌïôÏäµ - SeenÍ≥º UnseenÏùò LLM ÏÑ§Î™Ö ÏûÑÎ≤†Îî©ÎÅºÎ¶¨Ïùò Ïú†ÏÇ¨ÎèÑ ÎßåÌÅº learnable promptÍ∞Ä Ïú†ÏÇ¨ÌïòÍ≤å ÎêòÎèÑÎ°ù ÌïôÏäµ!  

3Îã®Í≥Ñ: Negative ÌïôÏäµ - Seen classÏùò Ïù¥ÎØ∏ÏßÄ Ïù∏ÏΩîÎî©Í≥º UnclassÏùò learnable promptÍ∞Ä Î©ÄÏñ¥ÏßÄÎèÑÎ°ù ÌïôÏäµ  

Ïù¥ÎñÑ! learnable promptÍ∞Ä Ï≤òÏùåÏóê ÌïúÎ≤à Îì§Ïñ¥Í∞ÄÎäîÍ≤å ÏïÑÎãàÎùº layerÎ≥ÑÎ°ú ÎÇòÎà†ÏÑú Îì§Ïñ¥Í∞ÑÎã§!!  

---

##### Deep Visual-Text Prompt Learning

- Í∏∞Ï°¥ÏóêÎäî Îã®ÏàúÌûà ÏûÖÎ†•ÎêòÎäî ÌîÑÎ°¨Ìè¨Ìä∏Î•º ÌäúÎãùÌñàÎã§Î©¥(Ïù∏ÏΩîÎçî ÏûÖÎ†• ÏïûÎã®Ïóê Í≥†Ï†ïÎêú ÌÜ†ÌÅ∞ Ï∂îÍ∞Ä),  
- Ïù¥Î≤à Ïó∞Íµ¨Îäî text Î∞è Vision Ïù∏ÏΩîÎçîÏùò Transformer Î†àÏù¥Ïñ¥Ïóê Í∞ÅÍ∞ÅÏùò learnable ÌîÑÎ°¨Ìè¨Ìä∏Î•º ÏÇΩÏûÖ!  

‚úÖ **Í∏∞Î≥∏ Prompt Tuning vs. Deep Visual-Text Prompt Learning**

| Ìï≠Î™©         | Í∏∞Î≥∏ Prompt Tuning | Deep Visual-Text Prompt Learning |
|--------------|--------------------|----------------------------------|
| **Ï†ÅÏö© ÏúÑÏπò**   | Ïù∏ÏΩîÎçî ÏûÖÎ†• ÏïûÎã®Ïóê Í≥†Ï†ïÎêú ÌÜ†ÌÅ∞ Ï∂îÍ∞Ä | Ïù∏ÏΩîÎçîÏùò Î™®Îì† Transformer Î†àÏù¥Ïñ¥Ïóê ÌïôÏäµ Í∞ÄÎä•Ìïú ÌîÑÎ°¨ÌîÑÌä∏ ÏÇΩÏûÖ |
| **ÌïôÏäµ ÎåÄÏÉÅ**   | Î≥¥ÌÜµ Î™á Í∞úÏùò ÌïôÏäµ Í∞ÄÎä•Ìïú ÌîÑÎ°¨ÌîÑÌä∏ Î≤°ÌÑ∞Îßå ÌäúÎãù | Î†àÏù¥Ïñ¥Î≥ÑÎ°ú ÌÖçÏä§Ìä∏/ÎπÑÏ£ºÏñº ÌîÑÎ°¨ÌîÑÌä∏ Ï†ÑÏ≤¥ ÏãúÌÄÄÏä§ ÌïôÏäµ |
| **ÌëúÌòÑÎ†•**     | Ï†úÌïúÏ†Å (shallow), ÏÉÅÎ•ò Ï†ïÎ≥¥Îßå Ï°∞Ï†à | ÍπäÏùÄ ÏúÑÏπòÏùò ÌëúÌòÑÍπåÏßÄ Ï°∞Ï†à Í∞ÄÎä• (deep) |
| **Ïú†Ïó∞ÏÑ±**     | Îã®Ïàú Íµ¨Ï°∞Î°ú Îπ†Î•¥Í≤å ÌäúÎãù | Î¨∏Îß•/Í¥ÄÍ≥Ñ/Î≥µÌï© Ï†ïÎ≥¥ Î∞òÏòÅ Í∞ÄÎä• (e.g. HOI) |
| **Ïòà: CLIP**  | ÌÖçÏä§Ìä∏ÏóêÎßå ÌÜ†ÌÅ∞ ÏÇΩÏûÖ ‚Üí Îã®Ïùº Î¨∏Ïû• ÏùòÎØ∏ Ï°∞Ï†à | ÌÖçÏä§Ìä∏ & ÎπÑÏ£ºÏñº Îëò Îã§ Ï°∞Ï†ïÌïòÎ©∞ ÏãúÍ∞Å-Ïñ∏Ïñ¥ Ï†ïÎ†¨ ÏûêÏ≤¥Î•º Ïû¨ÏÑ§Í≥Ñ |


---

üéØ **Ïôú Deep Visual-Text Prompt Learning Î∞©ÏãùÏù¥ Îçî Ï¢ãÏùÑÍπå??**

---

###### 1. Î†àÏù¥Ïñ¥Î≥Ñ ÏùòÎØ∏/Í∏∞Îä• Î∂ÑÌôî Í≥†Î†§

TransformerÏùò Í∞Å Î†àÏù¥Ïñ¥Îäî ÏÑúÎ°ú Îã§Î•∏ ÏàòÏ§ÄÏùò ÏùòÎØ∏Î•º Îã¥ÎãπÌï©ÎãàÎã§:

- **Ï¥àÍ∏∞ Î†àÏù¥Ïñ¥**: Ï†ÄÏàòÏ§Ä (local) ÌäπÏßï  
- **Ï§ëÍ∞Ñ Î†àÏù¥Ïñ¥**: Í¥ÄÍ≥Ñ (contextual) Ï†ïÎ≥¥  
- **ÎßàÏßÄÎßâ Î†àÏù¥Ïñ¥**: Í∞úÎÖêÏ†Å Ï∂îÏÉÅ (high-level semantics)

‚û°Ô∏è Îã®ÏàúÌûà ÏûÖÎ†• ÏïûÏóêÎßå ÌîÑÎ°¨ÌîÑÌä∏Î•º Î∂ôÏù¥Î©¥, Ïù¥ Î™®Îì† Î†àÏù¥Ïñ¥Ïóê Ï†ïÎ≥¥Î•º Ï†ÑÎã¨ÌïòÍ±∞ÎÇò Ï°∞ÏûëÌïòÍ∏∞ Ïñ¥Î†µÏäµÎãàÎã§.

üîπ Î∞òÎ©¥ **Deep Prompt Learning**ÏùÄ Í∞Å Î†àÏù¥Ïñ¥ÎßàÎã§ **Ï†ÅÏ†àÌïú ÌîÑÎ°¨ÌîÑÌä∏Î•º ÏÇΩÏûÖ**Ìï®ÏúºÎ°úÏç®, **Í≥ÑÏ∏µÎ≥Ñ ÏùòÎØ∏ ÌùêÎ¶ÑÏùÑ ÎØ∏ÏÑ∏ÌïòÍ≤å Ï°∞Ï†à**Ìï† Ïàò ÏûàÏäµÎãàÎã§.

---

###### 2. ÏãúÍ∞Å/ÌÖçÏä§Ìä∏ Î™®ÎëêÏóê Ï†ÅÏö© ‚Üí Modal Alignment Í∞úÏÑ†

Í∏∞Ï°¥ Prompt TuningÏùÄ Ï£ºÎ°ú **ÌÖçÏä§Ìä∏ Ï™Ω**ÏóêÎßå ÌîÑÎ°¨ÌîÑÌä∏Î•º ÏÇΩÏûÖÌï©ÎãàÎã§. ÌïòÏßÄÎßå:

- **HOI (Human-Object Interaction)**  
- **VQA (Visual Question Answering)**

Ï≤òÎüº **ÌÖçÏä§Ìä∏ÏôÄ ÎπÑÏ£ºÏñºÏùò Ï°∞Ìï©Ïù¥ ÌïµÏã¨Ïù∏ ÏûëÏóÖ**ÏóêÏÑúÎäî,  
**ÏãúÍ∞Å ÌëúÌòÑÎèÑ ÎèôÏãúÏóê Ï†ïÎ†¨/Ï°∞Ï†à**Ìï¥Ïïº ÏÑ±Îä•Ïù¥ Ìñ•ÏÉÅÎê©ÎãàÎã§.

üîπ **Deep Visual-Text Prompt**Îäî **ÌÖçÏä§Ìä∏ÏôÄ Ïù¥ÎØ∏ÏßÄ Ïù∏ÏΩîÎçî Î™®ÎëêÏóê Î≥ëÎ†¨Ï†ÅÏúºÎ°ú ÌîÑÎ°¨ÌîÑÌä∏Î•º ÏÇΩÏûÖ**ÌïòÏó¨,  
**Îëê Î™®Îã¨Î¶¨Ìã∞ Í∞ÑÏùò Ï†ïÎ†¨ ÌíàÏßà (alignment)**ÏùÑ ÎÜíÏûÖÎãàÎã§.

---

###### 3. Fine-grained Control & Context Adaptation

ÌîÑÎ°¨ÌîÑÌä∏Í∞Ä **Í∞Å Î†àÏù¥Ïñ¥Ïóê ÎèÖÎ¶ΩÏ†ÅÏúºÎ°ú Ï°¥Ïû¨**ÌïòÎØÄÎ°ú Îã§ÏùåÏù¥ Í∞ÄÎä•Ìï©ÎãàÎã§:

- ÌäπÏ†ï ÏûëÏóÖ / ÌÅ¥ÎûòÏä§ / Î¨∏Îß•Ïóê ÎßûÎäî **ÏÑ∏Î∂ÄÏ†Å Ï°∞Ï†ï**  
- ÌîÑÎ°¨ÌîÑÌä∏Î•º **HOI ÌÅ¥ÎûòÏä§Î≥ÑÎ°ú Îã§Î•¥Í≤å ÌïôÏäµ**ÏãúÏºú **ÏÑ∏Î∞ÄÌïú ÌëúÌòÑ Ï†úÏñ¥**  
- Îã®ÏàúÌûà "Ïù¥Í±¥ Í≥†ÏñëÏù¥Ïïº"Í∞Ä ÏïÑÎãàÎùº ‚Üí **"ÏÇ¨ÎûåÏù¥ Í≥†ÏñëÏù¥Î•º ÏïàÍ≥† ÏûàÎã§"** Í∞ôÏùÄ **Î≥µÌï© Í¥ÄÍ≥Ñ ÌëúÌòÑ**Ïóê Ïú†Î¶¨



---

#### üî¨ EZ-HOIÏùò ÏÑ±Îä• Ïã§Ìóò!!   

---

##### 1. üìò Zero-Shot HOI ÏÑ§Ï†ïÏùò Ï†ïÏùò

- Í∏∞Ï°¥ zero-shot HOI Î∞©ÏãùÍ≥º ÎèôÏùºÌïòÍ≤å, **unseen HOI ÌÅ¥ÎûòÏä§Ïùò Ïù¥Î¶ÑÏùÑ ÌõàÎ†® Ï§ëÏóê ÌôúÏö©**  
- Í∏∞Ï°¥ Ïó∞Íµ¨Îì§:
  - **VCL, FCL, ATL**: unseen HOI ÌÅ¥ÎûòÏä§ Ïù¥Î¶ÑÏùÑ Ï°∞Ìï©ÌïòÏó¨ ÏÉàÎ°úÏö¥ ÏÉòÌîå Íµ¨ÏÑ±
  - **EoID**: CLIPÏùÑ ÏÇ¨Ï†Ñ Ï†ïÏùòÎêú HOI ÌîÑÎ°¨ÌîÑÌä∏Î°ú ÎîîÏä§Ìã∏ (seen + unseen ÌÅ¥ÎûòÏä§)
  - **HOICLIP**: verb class representation ÎèÑÏûÖ (seen/unseen Ìè¨Ìï®)

---

##### 2. ‚öôÔ∏è Íµ¨ÌòÑ ÏÑ∏Î∂Ä ÏÑ§Ï†ï

- **Í∏∞Î≥∏ Íµ¨Ï°∞**:  
  - DETR + ResNet-50 Î∞±Î≥∏  
  - CLIP Í∏∞Î∞ò dual encoder Íµ¨Ï°∞ (ÌÖçÏä§Ìä∏/ÎπÑÏ£ºÏñº Î™®Îëê ÌîÑÎ°¨ÌîÑÌä∏ ÏÇΩÏûÖ)  

- **ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞**:
  - Î∞∞Ïπò ÏÇ¨Ïù¥Ï¶à: `16`
  - ÌïôÏäµÎ•†: `1e-3`
  - ÏòµÌã∞ÎßàÏù¥Ï†Ä: `AdamW`
  - GPU: `4 √ó Nvidia A5000`

- **Î∞±Î≥∏(backbone)**:
  - Visual encoder: `DETR (ResNet-50)`
  - Text encoder: `LLaVA-v1.5-7b`Î°ú ÏÑ§Î™Ö Í∏∞Î∞ò ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±

- **ÌîÑÎ°¨ÌîÑÌä∏ ÏÑ§Í≥Ñ**:
  - Î†àÏù¥Ïñ¥ Ïàò: `N = 9`, ÌîÑÎ°¨ÌîÑÌä∏ Í∏∏Ïù¥: `p = 2`
  - ÌïôÏäµ Í∞ÄÎä•Ìïú text & visual promptsÎ•º Transformer Î†àÏù¥Ïñ¥ÎßàÎã§ ÏÇΩÏûÖ

- **Ï∂îÍ∞Ä Í∏∞Î≤ï**:
  - Intra-HOI fusion: ÏÇ¨Îûå-Í∞ùÏ≤¥ ÏåçÏùò feature ÏúµÌï©
  - Inter-HOI fusion: Ïù¥ÎØ∏ÏßÄ ÎÇ¥ Ïó¨Îü¨ HOI Ïåç Í∞Ñ Î¨∏Îß• Ï£ºÏûÖ
  - LLM Í∏∞Î∞ò ÏÑ∏Î∞Ä ÌîÑÎ°¨ÌîÑÌä∏ (ÌÖçÏä§Ìä∏ ÏÑ§Î™Ö Ìè¨Ìï®)
  - Visual Adapter (Ï∞∏Í≥†: [27])
  - UTPL Î™®Îìà (Unseen Text Prompt Learning)

---

##### 3. Ïã§ÌóòÍ≤∞Í≥º Î∂ÑÏÑù Î∞è Ablation Study  

![results](https://github.com/user-attachments/assets/9d9e6d9c-9c40-490e-8d03-e30ed0a3bf6b)  

- **Unseen-Verb Setting**  
  - Í∏∞Ï°¥ ÎåÄÎπÑ ÌïôÏäµ Í∞ÄÎä•Ìïú ÌååÎùºÎØ∏ÌÑ∞ Ïàò `ÏµúÎåÄ 87.9% Í∞êÏÜå`  
  - CLIP4HOIÎ≥¥Îã§ ÏÑ±Îä•ÏùÄ ÏïΩÍ∞Ñ ÎÇÆÏßÄÎßå, Ìö®Ïú®ÏÑ± Í∑πÎåÄÌôî  
  - UniHOI ÎåÄÎπÑ `2.77 mAP` Ìñ•ÏÉÅ, ÌååÎùºÎØ∏ÌÑ∞ ÏàòÎäî `26.9%` ÏàòÏ§Ä

- **Unseen-Composition Setting (RF-UC / NF-UC)**  
  - CLIP4HOI ÎåÄÎπÑ Î™®Îì† ÏÑ§Ï†ïÏóêÏÑú ÏÑ±Îä• Ïö∞Ïàò  
  - UniHOI ÎåÄÎπÑ RF-UCÏóêÏÑú `+5.56 mAP`, NF-UCÏóêÏÑú `+7.88 mAP`

- **Unseen-Object Setting**  
  - CLIP4HOIÎ≥¥Îã§ `+1.49 mAP`, ÌååÎùºÎØ∏ÌÑ∞ ÏàòÎäî `12.08%`  
  - UniHOI ÎåÄÎπÑ unseen ÌÅ¥ÎûòÏä§ÏóêÏÑú `+13.36 mAP`

- üî¨ Ablation Study Ìï¥ÏÑù

![ablation](https://github.com/user-attachments/assets/8eda36ad-3552-4e75-a2c4-2fb44037f813)  

  | Ìï≠Î™© | Í∏∞Îä• ÏÑ§Î™Ö | ÏÑ±Îä• Î≥ÄÌôî | Ìï¥ÏÑù |
  |------|-----------|------------|------|
  | **Intra-HOI Fusion** | Ìïú Ïù¥ÎØ∏ÏßÄ ÎÇ¥ÏóêÏÑú Îã®Ïùº ÏÇ¨Îûå-Í∞ùÏ≤¥(H-O) Ïåç ÎÇ¥Î∂ÄÏùò Ï†ïÎ≥¥ Í≤∞Ìï© | seen `+7.41 mAP` | Ìïú Ïåç ÎÇ¥ ÏÇ¨Îûå/Í∞ùÏ≤¥ Í¥ÄÍ≥ÑÎ•º Îçî Ï†ïÌôïÌûà Ìè¨Ï∞©Ìï®ÏúºÎ°úÏç®, ÌïôÏäµÎêú ÌÅ¥ÎûòÏä§(seen)Ïóê ÎåÄÌïú Ïù∏Ïãù Ï†ïÎ∞ÄÎèÑ ÌÅ¨Í≤å Ìñ•ÏÉÅ |
  | **Visual Adapter** | ÏãúÍ∞Å Ïù∏ÏΩîÎçî Í∞Å Î†àÏù¥Ïñ¥Ïóê Ïô∏Î∂Ä Ï†ïÎ≥¥(Ïòà: ÏúÑÏπò, ÌÅ¥ÎûòÏä§)Î•º ÏÇΩÏûÖÌïòÎäî Î™®Îìà | seen ‚Üë / unseen ‚Üì | seen ÌÅ¥ÎûòÏä§ÏóêÏÑúÎäî Ïù¥ Ï†ïÎ≥¥Í∞Ä ÎèÑÏõÄÏù¥ ÎêòÏßÄÎßå, unseen ÌÅ¥ÎûòÏä§ÏóêÎäî Í≥ºÏ†ÅÌï© Í∞ÄÎä•ÏÑ± ‚Üí ÏùºÎ∞òÌôîÏóê Î∞©Ìï¥ ÏöîÏù∏ |
  | **LLM Guidance** | LLaVA Í∏∞Î∞ò Ïñ∏Ïñ¥Î™®Îç∏Ïù¥ ÏÉùÏÑ±Ìïú Ï†ïÍµêÌïú ÌÖçÏä§Ìä∏ ÏÑ§Î™ÖÏùÑ ÌîÑÎ°¨ÌîÑÌä∏Ïóê ÏÇ¨Ïö© | unseen `+1.52 mAP` | Îã®Ïàú ÌÅ¥ÎûòÏä§ Ïù¥Î¶ÑÎ≥¥Îã§ ÏùòÎØ∏ Í∏∞Î∞òÏùò Î¨òÏÇ¨Î•º ÌôúÏö©Ìï®ÏúºÎ°úÏç®, Ï≤òÏùå Î≥¥Îäî ÌÅ¥ÎûòÏä§Ïóê ÎåÄÌïú Ïù¥Ìï¥Î†• Ï¶ùÍ∞Ä |
  | **UTPL (Unseen Text Prompt Learning)** | unseen Ï†ÑÏö© ÌïôÏäµ ÌîÑÎ°¨ÌîÑÌä∏Î•º Îî∞Î°ú ÌõàÎ†®ÌïòÎäî Íµ¨Ï°∞ | unseen `+2.42 mAP` | ÌîÑÎ°¨ÌîÑÌä∏Í∞Ä seenÏóê Ìé∏Ï§ëÎêòÏßÄ ÏïäÍ≤å ÌïòÍ≥†, unseenÏùÑ ÏúÑÌïú ÌëúÌòÑÎ†•ÏùÑ ÏßÅÏ†ë ÌïôÏäµÌïòÍ≤å ÌïòÏó¨ ÏÑ±Îä• Í∞ïÌôî |
  | **Inter-HOI Fusion** | Ïó¨Îü¨ ÏÇ¨Îûå-Í∞ùÏ≤¥ Ïåç Í∞Ñ Î¨∏Îß•ÏùÑ Í≥µÏú†ÌïòÏó¨ Ï†ïÎ≥¥ Î≥¥Í∞ï | seen/unseen Î™®Îëê Ìñ•ÏÉÅ | Ìïú Ïù¥ÎØ∏ÏßÄ ÎÇ¥Ïùò Îã§ÏñëÌïú Í¥ÄÍ≥ÑÎì§Ïù¥ ÏÑúÎ°ú Í∞ÑÏóê Î¨∏Îß•Ï†Å ÎèÑÏõÄÏùÑ Ï£ºÏñ¥, Ï†ÑÎ∞òÏ†ÅÏù∏ Ïù∏ÏãùÎ†•Í≥º Î∂ÑÎ•ò Ï†ïÌôïÎèÑ ÏÉÅÏäπ |
  | **VLM Guidance** | CLIP Îì± ÏÇ¨Ï†ÑÌïôÏäµÎêú ÏãúÍ∞ÅÏñ∏Ïñ¥Î™®Îç∏Ïùò ÌäπÏÑ±ÏùÑ Ïú†ÎèÑ(align)ÌïòÎäî Ï†ÑÎûµ | unseen `+1.33 mAP` | VLMÏùò ÏùºÎ∞òÌôî ÏÑ±ÏßàÏùÑ ÌîÑÎ°¨ÌîÑÌä∏Ïóê Î∞òÏòÅÌï®ÏúºÎ°úÏç®, Ï≤òÏùå Î≥¥Îäî ÌÅ¥ÎûòÏä§ÏóêÎèÑ ÏùòÎØ∏ Ïú†Ï∂î Í∞ÄÎä• |


---

#### üß† ÎßàÎ¨¥Î¶¨ ÏÉùÍ∞Å

Unseen ÏºÄÏù¥Ïä§Ïóê ÎåÄÌïú ÎåÄÎπÑÎ•º ÏúÑÌïú Ïó∞Íµ¨!  
Ï¶â Zero shotÏúºÎ°ú Ï≤òÏùå Î≥¥Îäî ÏÉÅÌô©ÏóêÎèÑ Ïûò Ï†ÅÏùë Ìï†Ïàò ÏûàÎèÑÎ°ù,  
ÎØ∏Î¶¨ ÌïôÏäµÎêú ÌîÑÎ°¨Ìè¨Ìä∏Î•º ÎÑ£Îäî Ïó∞Íµ¨!!  

1) Seen ÏºÄÏù¥Ïä§Ïóê ÎåÄÌïòÏó¨ LLMÏúºÎ°ú ÏÑ§Î™ÖÌïòÍ≥† Ïù¥Î•º Ïù¥ÎØ∏ÏßÄ ÏûÑÎ≤†Îî©Í≥º Îß§Ïπ≠Îê† Ïàò ÏûàÎèÑÎ°ù ÌÖçÏä§Ìä∏ ÌîÑÎ°¨Ìè¨Ìä∏Î•º ÌïôÏäµÏãúÌÇ® Îã§Ïùå,  
2) Unseen ÏºÄÏù¥Ïä§Ïùò ÌîÑÎ°¨Ìè¨Ìä∏Îäî Ïú†ÏÇ¨Ìïú Seen ÌîÑÎ°¨Ìè¨Ìä∏ÏóêÏÑú ÏãúÏûëÌï¥ÏÑú, LLMÏùò Unseen ÏÑ§Î™Ö Î∞è Ïú†ÏÇ¨Ìïú Seen Ïù¥ÎØ∏ÏßÄÎ°ú Ï∂îÍ∞ÄÌïôÏäµ!  
3) LLMÏúºÎ°ú Unseen ÏºÄÏù¥Ïä§Ïóê ÎåÄÌïòÏó¨ ÏÑ§Î™ÖÌïòÍ≥†, Seen caseÍ∏∞Î∞òÏùò ÌîÑÎüºÌè¨Ìä∏Í∞Ä UnseenÏùò VLM ÏûÑÎ≤†Îî©Í≥º Îß§Ïπ≠ÎêòÎèÑÎ°ù ÌïôÏäµ! 
4) Ïù¥Î•º ÌÜµÌï¥ÏÑú! Unseen Ïùò Zero shotÏùÑ ÏúÑÌïú ÏôÑÎ≤ΩÌïú ÌîÑÎ°¨Ìè¨Ìä∏Î•º ÎßåÎì†Îã§!



---
