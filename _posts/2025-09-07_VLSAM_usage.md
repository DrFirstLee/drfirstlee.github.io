---
layout: post
title: "ğŸ” VL-SAM Hands-on: VL-SAM ì„ ì‹¤ìŠµí•´ë³´ì!"
author: [DrFirst]
date: 2025-09-07 07:00:00 +0900
categories: [AI, Research]
tags: [Open-Vocabulary, Detection, Segmentation, SAM, VLM,python, Open-ended]
sitemap:
  changefreq: monthly
  priority: 0.8
---

---

### ğŸ§¬ (í•œêµ­ì–´) EfficientSAM ì‹¤ìŠµ!!

ì˜¤ëŠ˜ì€ [ì˜ˆì „í¬ìŠ¤íŒ…](https://drfirstlee.github.io/posts/SAM2/)ì—ì„œ ì´ë¡ ì— ëŒ€í•˜ì—¬ ê³µë¶€í•´ë³´ì•˜ë˜!  
ê·¸ë¦¬ê³  [ì‹¤ìŠµë„ í•´ë³´ì•˜ë˜](https://drfirstlee.github.io/posts/SAM2_usage/)
ìƒˆë¡œìš´ SAM ëª¨ë¸! `SAM2` ì˜ ì‹¤ìŠµì„ ë‹¤ì‹œ í•œ ë²ˆ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤!!  
ì´ë²ˆì—” ì§€ë‚œë²ˆê³¼ ë‹¬ë¦¬ `ultralytics` ê°€ ì•„ë‹ˆë¼ huggingfaceì—ì„œ ëª¨ë¸ì„ ë°›ì•„ì„œê³ ê³ !!  

> ê·¸ëŸ°ë°,, ì´ SAM2, EfficientSAMë³´ë‹¤ëŠ” ê²°ê³¼ë¬¼ì´ ë§˜ì—ë“¤ì§€ ì•Šêµ¬ë§Œìœ ,,

---

### ğŸ”§ 1. ì„¤ì¹˜ ë° ì…‹ì—…

GitHubì—ì„œ ì§ì ‘ í´ë¡ í•˜ì—¬ ì„¤ì¹˜í•©ë‹ˆë‹¤.  
ì €ëŠ” ê·¸ì „ì— sam2ë¼ëŠ” ê°€ìƒí™˜ê²½ì„ ë§Œë“¤ê³  ì§„í–‰í–ˆìŠµë‹ˆë‹¤!!  

```bash
conda create --name sam2 python=3.12
git clone https://github.com/facebookresearch/sam2.git && cd sam2

pip install -e .
```

ì¶”ê°€ë¡œ SAM2 gitì˜ readmeì—ì„œëŠ” `./download_ckpts.sh`ë¡œ ëª¨ë¸ì„ ë°›ìœ¼ë¼ê³ í–ˆì§€ë§Œ,  
ì‹¤ì œ ì½”ë“œëŠ” `hf_hub_download`ë¥¼ í†µí•´ weightë¥¼ ë°›ì•„ì˜¤ëŠ” ê¸°ëŠ¥ì´ ìˆê¸°ì— ìŠ¤í‚µ!!

---

### ğŸ–¼ï¸ 2. ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜    

ì§€ë‚œ [EfficientSAM ì‹¤ìŠµ](https://drfirstlee.github.io/posts/efficientSAM_usage/)ê³¼ ë™ì¼í•˜ê²Œ!! ë©ë©ì´ ì‚¬ì§„ìœ¼ë¡œ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤!!  
í”„ë¡¬í¬íŠ¸ë„~ ì ì„ 2ê°œë§Œ!!  

```python
import torch
import numpy as np
from PIL import Image, ImageDraw
import os
from sam2.sam2_image_predictor import SAM2ImagePredictor

# 1. ê¸°ë³¸ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# 2. SAM2 ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
predictor = SAM2ImagePredictor.from_pretrained("facebook/sam2-hiera-large")

# 3. ì´ë¯¸ì§€ ì¤€ë¹„
image_path = "./EfficientSAM_gdino/figs/examples/dogs.jpg"
output_image_path = "output_masked_image.png"


image_pil = Image.open(image_path).convert("RGB")
image_np = np.array(image_pil)


# -----------------------------------------------------
# 4. í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ 
# -----------------------------------------------------
# input_pointsë¥¼ (batch, num_points, 2) í˜•íƒœì˜ 3D í…ì„œë¡œ ë§Œë“­ë‹ˆë‹¤.
input_points = torch.tensor([[[580, 350], [650, 350]]], device=device)
input_labels = torch.tensor([[1, 1]], device=device)

# input_labels = torch.tensor([[1, 1, 1, 1]], device=device) # ëª¨ë“  ì ì´ ì „ê²½
# 5. ì˜ˆì¸¡ ì‹¤í–‰
with torch.inference_mode(), torch.autocast(device_type=device.split(":")[0], dtype=torch.bfloat16):
    predictor.set_image(image_np)
    masks, scores, _ = predictor.predict(
        point_coords=input_points,
        point_labels=input_labels,
    )
mask = masks[0]
print(f"mask shape :{mask.shape}")

# ì›ë³¸ ì´ë¯¸ì§€ì™€ ê°™ì€ í¬ê¸°ì˜ ì™„ì „íˆ ê²€ì •ìƒ‰ NumPy ë°°ì—´ ìƒì„±
segmented_image_np = np.zeros_like(image_np)

# ë§ˆìŠ¤í¬ê°€ Trueì¸ ì˜ì—­ì—ë§Œ ì›ë³¸ ì´ë¯¸ì§€ í”½ì…€ì„ ë³µì‚¬
# maskëŠ” boolean ë°°ì—´ (True/False)ì´ê±°ë‚˜ 0~1 ì‚¬ì´ì˜ float ë°°ì—´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# float ë°°ì—´ì´ë¼ë©´ ì„ê³„ê°’ì„ ì ìš©í•˜ì—¬ boolean ë§ˆìŠ¤í¬ë¡œ ë§Œë“­ë‹ˆë‹¤.
binary_mask = (mask > 0.5) # 0.5ë¥¼ ê¸°ì¤€ìœ¼ë¡œ True/False ë§ˆìŠ¤í¬ ìƒì„±

# ë§ˆìŠ¤í¬ê°€ Trueì¸ ìœ„ì¹˜ì— ì›ë³¸ ì´ë¯¸ì§€ í”½ì…€ì„ í• ë‹¹
segmented_image_np[binary_mask] = image_np[binary_mask]

# NumPy ë°°ì—´ì„ PIL Imageë¡œ ë³€í™˜
result_image = Image.fromarray(segmented_image_np)

# í”„ë¡¬í”„íŠ¸ ì ë„ ì´ë¯¸ì§€ ìœ„ì— ê·¸ë¦¬ê¸° (ì„ íƒ ì‚¬í•­)
draw_result = ImageDraw.Draw(result_image)
points_np = input_points[0].cpu().numpy()
labels_np = input_labels[0].cpu().numpy()

for i, (x, y) in enumerate(points_np):
    label = labels_np[i]

    fill_color = "green" if label == 1 else "red"
    outline_color = "white"
    radius = 5

    if label == 1:
        draw_result.ellipse((x - radius, y - radius, x + radius, y + radius), fill=fill_color, outline=outline_color, width=1)
    else:
        draw_result.line((x - radius, y - radius, x + radius, y + radius), fill=fill_color, width=2)
        draw_result.line((x + radius, y - radius, x - radius, y + radius), fill=fill_color, width=2)

result_image.save(output_image_path)
print(f"ê²°ê³¼ ì´ë¯¸ì§€ê°€ '{output_image_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
```

ê²°ê³¼ ì´ë¯¸ì§€ëŠ”!!!??

![Image](https://github.com/user-attachments/assets/a175eae3-a5e8-463b-bb96-15dcd5c94c51)

ì™„ì „ ì‹¤ë§ìŠ¤ëŸ¬ìš´ë°,,??  
ê·¸ë˜ì„œ, í”„ë¡¬í¬íŠ¸ ì ì„ 4ê°œë¡œ, ëª¨ë¸ë„ `sam2.1_hiera_large.pt`ë¡œ ë°”ê¿”ì„œ í•´ë³´ì•˜ëŠ”ë°!!

![Image](https://github.com/user-attachments/assets/83a1eb1c-3d3a-4b9a-812b-eb951fdd7ee4)

 ê·¸ë˜ë„ ê²°ê³¼ê°€ ì‹¤ë§ìŠ¤ëŸ½ë„¤ìš”,,
GPTì— ë¬¼ì–´ë³´ë‹ˆ í”„ë¡¬í¬íŠ¸ì˜ í•´ì„ì°¨ì´ë¼ê³ í•˜ëŠ”ë°,,  
ì €ëŠ” ê·¸ë˜ë„ EfficientSAMì´ ì¢‹ë„¤ìš”!!

```text
EfficientSAMì´ ë” "ì˜" í–ˆë‹¤ê¸°ë³´ë‹¤ëŠ”, ì‚¬ìš©ìì˜ ë¶€ì •í™•í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ë” "ê´€ëŒ€í•˜ê²Œ" í•´ì„í•´ ì¤€ ê²ƒì…ë‹ˆë‹¤.

ë°˜ë©´ì— SAM2ëŠ” í›¨ì”¬ ê°•ë ¥í•˜ê³  ì •ë°€í•œ ë„êµ¬ì´ê¸° ë•Œë¬¸ì—, ê·¸ ì„±ëŠ¥ì„ ì œëŒ€ë¡œ í™œìš©í•˜ë ¤ë©´ ì‚¬ìš©ìë„ ë” ì •í™•í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. ì´ì „ ë‹µë³€ì—ì„œ ì œì•ˆí•´ ë“œë¦° ê²ƒì²˜ëŸ¼ ê°•ì•„ì§€ë“¤ì˜ ëª¸í†µê³¼ ë¨¸ë¦¬ì— ì—¬ëŸ¬ ê°œì˜ ì ì„ ì°ì–´ì£¼ì‹œë©´, SAM2ê°€ EfficientSAMë³´ë‹¤ í›¨ì”¬ ë” ê³ í’ˆì§ˆì˜ ì •êµí•œ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆì„ ê²ë‹ˆë‹¤.
```
 

---

### ğŸ§ª 3. ì˜ìƒ Segmentation    

![Image](https://github.com/user-attachments/assets/0ef426cb-262a-42ab-b714-d109844500b0)

ê²°ê³¼ë¶€í„°!!! ë§˜ì—ë“œëŠ”ë°ìš”~~  

ì½”ë“œëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤!

```python
import torch
import numpy as np
import cv2
import os
from sam2.sam2_video_predictor import SAM2VideoPredictor
from moviepy.editor import VideoFileClip

# 1. ê¸°ë³¸ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# ì›ë³¸ ë° ì¶œë ¥ ê²½ë¡œ ì„¤ì •
output_video_path = "output_segmented_video_50_55s.mp4"
clipped_video_path = "temp_clip.mp4"


# 3. ë¹„ë””ì˜¤ ë¡œë”©
cap = cv2.VideoCapture(clipped_video_path)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

video_frames = []
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    video_frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
cap.release()
print(f"í´ë¦½ ë¡œë“œ ì™„ë£Œ: {width}x{height}, {total_frames} í”„ë ˆì„, {fps:.2f} FPS")

# 4. SAM2 ëª¨ë¸ ë¡œë“œ
print("SAM2 ë¹„ë””ì˜¤ ì˜ˆì¸¡ê¸° ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
predictor = SAM2VideoPredictor.from_pretrained("facebook/sam2-hiera-large")

# -----------------------------------------------------
# 5. í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (â˜…â˜…â˜…â˜…â˜… ì´ ë¶€ë¶„ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤ â˜…â˜…â˜…â˜…â˜…)
# -----------------------------------------------------
prompt_frame_idx = 0  # í”„ë¡¬í”„íŠ¸ë¥¼ ì ìš©í•  í”„ë ˆì„ ì¸ë±ìŠ¤ (0ì€ ì²« ë²ˆì§¸ í”„ë ˆì„)
prompt_obj_id = 1     # ì¶”ì í•  ê°ì²´ì˜ ê³ ìœ  ID (ì²« ë²ˆì§¸ ê°ì²´ì´ë¯€ë¡œ 1)

# ì  ì¢Œí‘œ: (NumPoints, 2) í˜•íƒœì˜ NumPy ë°°ì—´
points = np.array([[width // 2, height // 2]], dtype=np.float32)
# ë ˆì´ë¸”: (NumPoints,) í˜•íƒœì˜ NumPy ë°°ì—´
labels = np.array([1], dtype=np.int32)

# -----------------------------------------------------
# 6. ëª¨ë¸ ì´ˆê¸°í™” ë° ì²« í”„ë ˆì„ ì˜ˆì¸¡
# -----------------------------------------------------
with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
    print("ì˜ˆì¸¡ê¸° ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
    state = predictor.init_state(clipped_video_path) 
    
    print("ì²« ë²ˆì§¸ í”„ë ˆì„ì— í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤...")
    # ì˜ˆì œ ì½”ë“œì— ë§ì¶° í•¨ìˆ˜ ì´ë¦„ê³¼ ì¸ìë¥¼ ëª¨ë‘ ë³€ê²½
    _, _, masks = predictor.add_new_points_or_box(
        inference_state=state,
        frame_idx=prompt_frame_idx,
        obj_id=prompt_obj_id,
        points=points,
        labels=labels,
    )
    # 7. ë¹„ë””ì˜¤ ì „íŒŒ ë° ê²°ê³¼ ì €ì¥
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))
    
    print("í´ë¦½ ì „ì²´ì— ë§ˆìŠ¤í¬ë¥¼ ì „íŒŒí•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...")
    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):
        original_frame = video_frames[frame_idx]
        # segmented_image_np = np.zeros_like(original_frame)
        # segmented_image_np = np.zeros_like(original_frame)
        segmented_image_np = np.full_like(original_frame, 255)

        # prompt_obj_id (1)ê°€ ì¶”ì ë˜ê³  ìˆëŠ”ì§€ í™•ì¸
        if prompt_obj_id in object_ids:
            mask_logits = masks[0][0].cpu().numpy()
            binary_mask_before_resize = (mask_logits > 0.0).astype(np.uint8)
            resized_mask = cv2.resize(binary_mask_before_resize, (width, height), interpolation=cv2.INTER_NEAREST)

            # 4. ë¦¬ì‚¬ì´ì¦ˆëœ ë§ˆìŠ¤í¬ë¥¼ boolean íƒ€ì…ìœ¼ë¡œ ìµœì¢… ë³€í™˜
            boolean_mask = (resized_mask == 1)

            # 5. ì˜¬ë°”ë¥¸ ë§ˆìŠ¤í¬ë¡œ ì¸ë±ì‹±
            segmented_image_np[boolean_mask] = original_frame[boolean_mask]
            # # ì´ì œ shapeì´ ì¼ì¹˜í•˜ë¯€ë¡œ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.
            # segmented_image_np[mask] = original_frame[mask]   
        # --- ì¶”ê°€ëœ ë¶€ë¶„: ëª¨ë“  í”„ë ˆì„ì— í”„ë¡¬í”„íŠ¸ ì  ê·¸ë¦¬ê¸° ---
        for x, y in points:
            # cv2.circleì„ ì‚¬ìš©í•˜ì—¬ ë¹¨ê°„ìƒ‰ ì ì„ ê·¸ë¦½ë‹ˆë‹¤.
            # segmented_image_npëŠ” RGB ìƒíƒœì´ë¯€ë¡œ, ë¹¨ê°„ìƒ‰ì€ (255, 0, 0)ì…ë‹ˆë‹¤.
            # thickness=-1ì€ ì±„ì›Œì§„ ì›ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
            cv2.circle(segmented_image_np, (int(x), int(y)), radius=5, color=(255, 0, 0), thickness=-1)

        output_frame = cv2.cvtColor(segmented_image_np, cv2.COLOR_RGB2BGR)
        out_writer.write(output_frame)
        
        print(f"\r- ì²˜ë¦¬ ì¤‘: í”„ë ˆì„ {frame_idx + 1}/{total_frames}", end="")

    out_writer.release()
    print(f"\në¹„ë””ì˜¤ ë¶„í•  ì™„ë£Œ! ê²°ê³¼ê°€ '{output_video_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

```

SAM2, ì˜ìƒì—ì„œ íŠ¸ë˜ì´ì‹±ì€ ì •ë§ ë§˜ì—ë“œëŠ”êµ°ìš”!^^