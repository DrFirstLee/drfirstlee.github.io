---
layout: post
title: "📝 TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering"
author: [DrFirst]
date: 2025-08-04 09:00:00 +0900
categories: [AI, Research]
tags: [TIFA, Evaluation, Image Retrieval, CIR, Metrics, VQA, Text-to-Image]
sitemap :
  changefreq : monthly
  priority : 0.8
---

---
layout: post
title: "📊 TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering"
author: [DrFirst]
date: 2025-08-03 09:00:00 +0900
categories: [AI, Research]
tags: [TIFA, Evaluation, Metrics, Image Generation, CIR, VQA, ICCV 2023]
sitemap :
  changefreq : monthly
  priority : 0.8
---

### 📊 TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering  

- **Title**: [TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering](https://arxiv.org/pdf/2303.11897)  
- **Conference**: ICCV 2023 (Hu et al.)  
- **Code**: [TIFA (GitHub)](https://tifa-benchmark.github.io/)  
- **Keywords**: `Faithfulness`, `Text-to-Image`, `Evaluation`, `VQA`, `Interpretability`  

---

### 🧠 What is TIFA?  

![Image](https://github.com/user-attachments/assets/0f95d15b-8ba2-4ca1-a7dd-6ec2ccb2ba51)

- The key question: *How can we evaluate the performance of Text-to-Image generation models and Composed Image Retrieval (CIR) models?*  
- One of the recent solutions is **TIFA (Text-to-Image Faithfulness Assessment)**, proposed by Hu et al. at ICCV 2023.  
- The core idea is to automatically evaluate **how faithfully the text condition (prompt)** is reflected in the generated image.  

---

### 🕰️ Image Evaluation Metrics before TIFA  

Before TIFA (Hu et al., 2023), image generation evaluation developed along two main dimensions:  
**(1) Image Quality Evaluation** and **(2) Image-Text Alignment Evaluation**.  

---

#### 1. 👩‍⚖️ Human Evaluation  
- **Method**: Show humans two images generated by different models and perform **pairwise comparison** to decide which is better.  
- **Advantage**: Most reliable and intuitive.  
- **Limitation**: Expensive, slow, and impractical for large-scale evaluation.  

---

#### 2. 🖼️ Image Quality Evaluation  
> An older approach, less accurate than text-image evaluation.  
- **Inception Score (IS, Salimans et al. 2016)**  
  - **Definition**: Analyzes feature distributions of generated images using an Inception-V3 classifier.  
  - **Advantage**: Captures both diversity and quality of generated images.  
  - **Limitation**: Does not require ground-truth images, but does **not reflect semantic faithfulness**.  

- **FID (Fréchet Inception Distance, Heusel et al. 2017)**  
  - **Definition**: Compares the feature distribution of generated images and real images using Inception-V3.  
  - **Advantage**: Evaluates both **fidelity (realism)** and **diversity**.  
  - **Limitation**: Requires real images (GT) and struggles on complex datasets.  

---

#### 3. 🔗 Image-Text Alignment Evaluation  
> Emerged with the development of Detection Models and Vision-Language Models.  

- **CLIP-based**  
  - **CLIPScore (Hessel et al. 2021)**, **CLIP-R**  
  - Compute **cosine similarity** between image and text embeddings from CLIP.  
  - Simple and fully automatic, but lacks granularity in fine-grained attributes (color, material, spatial relations, etc.).  

- **Captioning-based**  
  - Use an image captioning model to describe the generated image, then compare the caption with the original prompt.  
  - Use NLP metrics such as **CIDEr, SPICE**.  
  - Limitation: Highly dependent on the performance of the captioning model.  

- **Object Detection-based** (SOA, DALL-Eval, etc.)  
  - Use object detectors to check whether objects/attributes/relations from the prompt exist in the image.  
  - Advantage: Can directly verify attributes like existence, color, count, and spatial relations.  
  - Limitation: Evaluates only **limited axes**; cannot capture material, shape, activity, or context.  

---

#### 4. 💡 Motivation for TIFA  
- Existing metrics only partially capture either **image quality** or **image-text alignment**.  
- Detection-based evaluations are restricted to limited attributes.  
- TIFA introduces **QA-based evaluation**:  
  - Prompt → (Object/Attribute/Relation parsing) → Question generation → VQA-based checking → Scoring.  
  - This allows evaluation of **broad attributes** including material, shape, activity, and context.  

---

👉 **Summary:**  
- **IS, FID** → Focus on *image quality* (fidelity, diversity).  
- **CLIPScore, CIDEr, SPICE** → Focus on *image-text alignment*.  
- **SOA, DALL-Eval** → Focus on limited attributes.  
- **TIFA** → QA-based, interpretable, and highly correlated with human judgments ✅  

---

### 📌 TIFA Evaluation Pipeline  

![Image](https://github.com/user-attachments/assets/09e80d2c-0549-4e61-b16a-6e4638cd56f0)

TIFA does not only check whether the correct image is retrieved, but **evaluates the faithfulness to text conditions** in a fine-grained manner.  

1. **Prompt Input**  
   - Example: *"A brown dog playing on the beach."*  

2. **Parsing with LLM**  
   - Extract Objects, Attributes, Relations.  
   - Example: `dog`, `brown`, `beach`.  

3. **Question Generation**  
   - "Is there a dog in the image?"  
   - "Is the dog brown?"  
   - "Is the background a beach?"  

- Steps 2 and 3 are done in a single step using the official TIFA prompt:  

```text
Given an image description, generate
multiple-choice questions that verify if
the image description is correct.
First extract elements from the image
description. Then classify each element
into a category (object, human, animal,
food, activity, attribute, counting, color,
material, spatial, location, shape, other).
Finally, generate questions for each
element.
```

4. **Answer Verification with VQA Model**  
   - Input the image and generated questions → the model provides answers.  
   - Check whether the answers align with the conditions in the prompt.  

5. **Scoring (Faithfulness Score Calculation)**  
   - Aggregate the results to compute a **faithfulness score** based on condition satisfaction.  

---

### ✅ Advantages of TIFA  

- **Faithfulness to Text Conditions**: Goes beyond simply checking if the correct image is in the Top-K; directly measures how well conditions are satisfied.  
- **Interpretability**: Each question corresponds to a condition, allowing easy inspection of which conditions were satisfied or failed.  
- **High Correlation with Human Evaluation**: Strongly aligns with human judgment results.  

---

### ⚠️ Limitations  

> Multiple question generation + VQA answering = computationally slow!  

- **VQA Model Dependency**: Performance depends heavily on the strength of the VQA model.  
- **Extra Computational Cost**: Unlike Recall, it requires both LLM parsing and VQA answering.  
- **Not Yet a CIR Standard**: Adoption rate is still low compared to Recall@K and mAP.  

---

👉 **Summary:**  
TIFA is a powerful tool for automatically measuring **“text-to-image faithfulness”**.  
It complements traditional metrics such as Recall and mAP while providing **human-friendly interpretability**, making it a strong candidate to become a **core metric for CIR and image generation evaluation** in the future.  

- The authors also used TIFA to evaluate existing image generation models and created a leaderboard.  
![Image](https://github.com/user-attachments/assets/49b89bad-9585-4562-9770-7dabcba23c12)


---

### 📊 TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering  

- **제목**: [TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering](https://arxiv.org/pdf/2303.11897)  
- **학회**: ICCV 2023 (Hu et al.)  
- **코드**: [TIFA (GitHub)](https://tifa-benchmark.github.io/)  
- **핵심 키워드**: `Faithfulness`, `Text-to-Image`, `Evaluation`, `VQA`, `Interpretability`  

---

### 🧠 (한국어) TIFA란 무엇인가?  

![Image](https://github.com/user-attachments/assets/0f95d15b-8ba2-4ca1-a7dd-6ec2ccb2ba51)

- **Text-to-Image** 생성 모델과 **Composed Image Retrieval(CIR)** 모델 성능을 평가를 어떻게 해야하는가! 하는 문제!!!  
- 이에 등장한 방법 중 하나가 바로 **TIFA (Text-to-Image Faithfulness Assessment)**, Hu et al. (ICCV 2023).  
- 핵심 아이디어는 **텍스트 조건(prompt)**이 결과 이미지에 얼마나 충실하게 반영되었는지를 자동으로 평가하는 것.  

---

### 🕰️ TIFA 이전의 이미지 평가 지표  

TIFA(Hu et al., 2023)가 제안되기 전까지, 이미지 생성 모델을 평가하는 방법은 크게 **(1) 이미지 품질 평가**, **(2) 텍스트-이미지 정합성 평가** 두 가지 관점에서 발전해왔음.  

---

#### 1. 👩‍⚖️ Human Evaluation (사람 평가)  
- **방법**: 두 모델이 생성한 이미지를 사람에게 보여주고 **pairwise comparison**(쌍 비교) 방식으로 어느 쪽이 더 좋은지 평가.  
- **장점**: 가장 신뢰도가 높고 직관적.  
- **한계**: 대규모 실험이 어렵고, 시간·비용이 많이 듦.  

---

#### 2. 🖼️ 이미지 품질 평가 (Image Quality)  
> 오래된 평가방식으로 텍스트-이미지 평가보다 정확도가 낮음  
- **Inception Score (IS, Salimans et al. 2016)**  
  - **정의**: Inception-V3 분류 모델을 이용해 생성 이미지의 feature 분포를 분석.  
  - **장점**: 이미지의 다양성과 품질을 동시에 반영.  
  - **한계**: GT(ground-truth) 이미지 필요 없음 → 하지만 실제 의미적 충실성은 반영 X.  

- **FID (Fréchet Inception Distance, Heusel et al. 2017)**  
  - **정의**: 생성 이미지와 실제 이미지의 feature 분포(FID score)를 비교하여 거리 측정.  
  - **장점**: 이미지의 **fidelity(진짜 같음)**와 **diversity(다양성)** 평가 가능.  
  - **한계**: 실제 이미지(GT) 필요, 복잡한 데이터셋에서는 한계.  

---

#### 3. 🔗 텍스트-이미지 정합성 평가 (Image-Text Alignment)  
> Detection Model, VLM 등이 발전하면서 등장!!  
- **CLIP 기반**  
  - **CLIPScore (Hessel et al. 2021)**, **CLIP-R**  
  - 이미지와 텍스트를 CLIP 임베딩에 넣고 **코사인 유사도**로 정합성 평가.  
  - 단순하면서도 자동화가 가능하지만, 세밀한 속성(색, 재질, 위치 등) 반영이 부족.  

- **Captioning 기반**  
  - 생성된 이미지를 캡셔닝 모델로 설명(텍스트 생성) 후, 원래 프롬프트와 비교.  
  - **CIDEr, SPICE** 같은 전통적 NLP 유사도 지표 활용.  
  - 한계: 캡셔닝 모델의 성능에 크게 의존.  

- **Object Detection 기반** (SOA, DALL-Eval 등)  
  - 객체 검출기로 "프롬프트에 있는 객체/속성/관계"가 이미지 안에 존재하는지 체크.  
  - 장점: 특정 속성(객체 존재 여부, 색, 위치, 개수 등)을 직접 확인 가능.  
  - 한계: **제한된 축(axis)만 평가** 가능 → 재질(material), 모양(shape), 맥락(context) 같은 요소는 반영 불가.  

---

#### 4. 💡 TIFA의 등장 배경  
- 기존 지표들은 **품질(quality)** 또는 **정합성(alignment)**만 부분적으로 평가.  
- 특히 object detection 기반 평가는 제한된 속성만 반영.  
- TIFA는 **QA 기반 평가**를 도입하여,  
  - 프롬프트 → (객체·속성·관계 파싱) → 질문 생성 → VQA 검사 → 점수화  
  - 이런 과정으로 **광범위한 속성 차원(material, shape, activity, context 등)**까지 충실도를 평가 가능.  

---

👉 정리:  
- **IS, FID** → 이미지 품질 자체(fidelity, diversity).  
- **CLIPScore, CIDEr, SPICE** → 이미지-텍스트 정합성.  
- **SOA, DALL-Eval** → 제한적 속성 기반 정합성.  
- **TIFA** → QA 기반으로 확장, **사람 평가와 높은 상관성** 확보 ✅  

---


### 📌 TIFA의 평가 파이프라인  

![Image](https://github.com/user-attachments/assets/09e80d2c-0549-4e61-b16a-6e4638cd56f0)

TIFA는 단순히 정답 이미지 여부만을 확인하지 않고, **텍스트 조건 충실성(faithfulness)**을 정밀하게 평가합니다.  

1. **프롬프트 입력**  
   - 예: "갈색 강아지가 해변에서 뛰어노는 장면"  

2. **LLM을 이용한 파싱**  
   - 객체(Object), 속성(Attribute), 관계(Relation) 추출  
   - 예: `강아지`, `갈색`, `해변`  

3. **질문 생성 (Question Generation)**  
   - "이미지에 강아지가 있나요?"  
   - "강아지는 갈색인가요?"  
   - "배경은 해변인가요?"  

- 2. 3번은 아래의 프롬포트를 통하여 한번에 생성됨!!

> TIFA의 Question Generation Prompt (논문 원문)
```text
Given an image description, generate
multiple-choice questions that verify if
the image description is correct.
First extract elements from the image
description. Then classify each element
into a category (object, human, animal,
food, activity, attribute, counting, color,
material, spatial, location, shape, other).
Finally, generate questions for each
element.
  ```

4. **VQA 모델을 통한 답변 비교**  
   - 이미지와 질문을 입력 → 모델이 답변 생성  
   - 답변이 프롬프트 조건과 일치하는지 확인  

5. **점수화 (Faithfulness Score 산출)**  
   - 조건 충족 여부를 종합하여 **충실성 점수**를 계산  

---

### ✅ TIFA의 장점  

- **텍스트 조건 충실성 평가**: 단순히 Top-K 안에 정답이 있느냐가 아니라, 조건을 얼마나 충실히 만족하는지 측정  
- **해석 가능성 (Interpretability)**: 어떤 조건을 만족/불만족했는지 질문 단위로 확인 가능  
- **인간 평가와 높은 상관성**: 실제 사람 평가 결과와 잘 맞아떨어짐  

---

### ⚠️ 한계  

> 여러개 질문 만들고, VLM에 답변받기. 너무 느려!!  

- **VQA 모델 의존성**: 질문에 답하는 성능이 VQA 모델에 따라 크게 달라짐  
- **추가 연산 비용**: Recall처럼 간단히 계산되지 않고, LLM + VQA 과정을 거쳐야 함  
- **아직 CIR 표준 아님**: Recall@K, mAP에 비해 채택률이 낮음  

---


👉 **정리**:  
TIFA는 **“텍스트 조건 충실성(faithfulness)”**을 자동으로 측정할 수 있는 강력한 도구입니다.  
Recall, mAP 같은 전통적 지표를 보완하면서, 인간 친화적인 해석을 제공한다는 점에서 **향후 CIR과 생성 모델 평가의 핵심 지표**로 자리잡을 가능성이 큽니다.  

- 야들이 만든 TIFA로 기존 이미지 생성모델을 평가, Leaderboard를 만들기도함!!  
![Image](https://github.com/user-attachments/assets/49b89bad-9585-4562-9770-7dabcba23c12)
