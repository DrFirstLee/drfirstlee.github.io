---
layout: post
title: "Using CLIP with Python - íŒŒì´ì¬ìœ¼ë¡œ CLIPì„ ì‚¬ìš©í•´ë³´ê¸°"
author: [DrFirst]
date: 2025-04-08 11:00:00 +0900
categories: [AI, Research]
tags: []
lastmod : 2025-04-08 11:00:00
sitemap :
  changefreq : weekly
  priority : 0.9

---
## (English) Using CLIP with Python

Hello! ğŸ˜Š  

![thumbnail](https://github.com/user-attachments/assets/7e34a71b-d976-4341-850c-de1c8cc70249)

In the [previous post](https://drfirstlee.github.io/posts/CLIP/), we explored CLIP's theory based on its paper!  
Today, weâ€™ll download the actual CLIP model and test it in a Python environment!!

Weâ€™ll use the image **bike.jpg** shown below for todayâ€™s test! ^^  
![bike](https://github.com/user-attachments/assets/0772db0b-ff4f-481d-a22c-4ed3f8db55fc)

---
### Preparing Python Packages!!

Weâ€™ll use the following packages!! A GPU is not required!!  
If youâ€™re reading this, Iâ€™m sure this level is already basic for you~!? ^^

```python
import clip
import torch
from PIL import Image
import numpy as np
```

> If CLIP is new to you, you can easily install it using:  
> `pip install clip` or `pip install git+https://github.com/openai/CLIP.git`

### Letâ€™s Start CLIP Right Away!!!

Now weâ€™re all set!  
Letâ€™s try using CLIP easily and quickly!!

```python
import clip
import torch
from PIL import Image
import numpy as np
device = "cuda" if torch.cuda.is_available() else "cpu"

# CLIP model Load
model, preprocess = clip.load("ViT-B/32", device="cuda" if torch.cuda.is_available() else "cpu")
# read image
image = preprocess(Image.open("bike.jpg")).unsqueeze(0).to(device)
#  make embedding!
image_features = model.encode_image(image)
print(np.shape(image_features))
image_features
```

How is it? Super easy, right!?

![clip_res](https://github.com/user-attachments/assets/fffa04b5-4d6d-44a7-aa80-d22692d32160)

As shown above, CLIP converts the image into a vector of shape (1, 512)!!

Next, letâ€™s create vector representations of some text!  
Iâ€™ll use the following two sentences:

 - "a man riding bicycle"
 - "a man climbing mountain"

```python
import clip
import torch
from PIL import Image
import numpy as np
device = "cuda" if torch.cuda.is_available() else "cpu"

# CLIP model Load
model, preprocess = clip.load("ViT-B/32", device="cuda" if torch.cuda.is_available() else "cpu")
# read text
text = clip.tokenize(["a man riding bicycle", "a man climbing mountain"]).to(device)
#  make embedding!
text_features = model.encode_text(text)
print(np.shape(text_features))
text_features
```

So, whatâ€™s the result!?  
You can see two 512-dimensional vectors are generated as shown below!!

![clip_res2](https://github.com/user-attachments/assets/2edd7c50-d1b9-4ca7-9b2c-d1daec309acb)


### Checking CLIPâ€™s Performance!!!!

But wait! Is it enough just to make vectors?  
Of course not~~ We have to check whether the vectors are meaningful!!

So far, weâ€™ve created the following 3 vectors:

- Image1: A person riding a bicycle
- Sentence1: A man riding a bicycle
- Sentence2: A man climbing a mountain

Letâ€™s compare the similarity between Image1 and Sentence1/Sentence2!

```python
# Find similarity
similarity = (image_features @ text_features.T).softmax(dim=-1)
print(similarity)
```

The result was!!!!!

```output
tensor([[0.9902, 0.0096]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SoftmaxBackward0>)
```

This shows that CLIP found the image to be very similar to the **first sentence**, and different from the **second**!

 - Similarity between image and first sentence: 0.9902
 - Similarity between image and second sentence: 0.0096

What do you think? CLIP! The theory may be complicated, but using it is really simple, right!?

This research became the foundation for the development of **multimodal models** that combine text and images!!


### Comparing CLIP Model Performance

In our code, we used a base model called ViT-B/32! But did you know there are many other versions?

| Model Name     | Backbone  | Size     | # of Parameters | Zero-shot ImageNet Accuracy | Embedding Size |
|----------------|-----------|----------|------------------|------------------------------|----------------|
| RN50           | ResNet-50 | Small    | ~102M            | ~63.3%                       | 1024           |
| RN101          | ResNet-101| Small    | ~123M            | ~64.2%                       | 512            |
| ViT-B/32       | ViT       | Small    | ~149M            | ~62.0%                       | 512            |
| ViT-B/16       | ViT       | Medium   | ~151M            | ~68.6%                       | 512            |
| ViT-L/14       | ViT       | Large    | ~428M            | ~75.5%                       | 768            |
| ViT-L/14@336px | ViT       | Large    | ~428M            | ~76.2%                       | 768            |

> **Backbone**: The base neural network structure used to process images or text, such as ResNet or ViT.  
> **Zero-shot ImageNet Accuracy**: The accuracy achieved when classifying ImageNet images **without additional fine-tuning**, using only pre-trained knowledge.  
> **Embedding Size**: The dimension of the final output embedding vector!!

There are 6 representative models like this!  
(The actual Python CLIP package supports 9 available models:  
`['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']`)

Depending on your use case:

 - If you need **faster speed**, choose a **small** model with fewer parameters
 - If **accuracy** is your top priority, choose a model with larger embedding size and more parameters

Pick the model that suits your goal~!

--- 

Thank you!

If you have any questions or topics youâ€™d like to know more about, feel free to leave a comment ğŸ’¬

---

## (í•œêµ­ì–´) íŒŒì´ì¬ìœ¼ë¡œ CLIPì„ ì‚¬ìš©í•´ë³´ê¸°

ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š  

![thumbnail](https://github.com/user-attachments/assets/7e34a71b-d976-4341-850c-de1c8cc70249)

[ì§€ë‚œ í¬ìŠ¤íŒ…](https://drfirstlee.github.io/posts/CLIP/) ì—ì„œëŠ” CLIPì˜ Paperë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ë¡ ì„ ì•Œì•„ë³´ì•˜ëŠ”ë°ìš”!
ì˜¤ëŠ˜ì€ ì‹¤ì œ ì´ CLIPëª¨ë¸ì„  ë‹¤ìš´ë°›ì•„ Python í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸í•´ë³´ê² ìŠµë‹ˆë‹¤!!

ì˜¤ëŠ˜ì˜ í…ŒìŠ¤íŠ¸ëŠ” ì•„ë˜ì˜ ì´ë¯¸ì§€ **bike.jpg** ë¥¼ ëŒ€ìƒìœ¼ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤!^^
![bike](https://github.com/user-attachments/assets/0772db0b-ff4f-481d-a22c-4ed3f8db55fc)

---
### Python íŒ¨í‚¤ì§€ ì¤€ë¹„!!

ì‚¬ìš©í•  íŒ¨í‚¤ì§€ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤!! GPUëŠ” ì—†ì–´ë„ ë˜ì–´ìš”!!
ì´ ê¸€ì„ ë³´ì‹œëŠ”ë¶„ë“¤ì´ë¼ë©´ ì´ë¯¸ ì´ì •ë„ëŠ” ê¸°ë³¸ì´ê² ì§€ìš”~!?^^

```python
import clip
import torch
from PIL import Image
import numpy as np
```

> CLIP íŒ¨í‚¤ì§€ëŠ” ì²˜ìŒì¼ìˆ˜ ìˆì–´ìš”! "pip install clip" í˜¹ì€ "pip install git+https://github.com/openai/CLIP.git" ë¡œ ê°„ë‹¨í•˜ê²Œ ì„¤ì¹˜í•´ë³´ì„¸ìš”!!

### ë°”ë¡œ CLIP ì‹œì‘!!!

ì´ì œ ì¤€ë¹„ëŠ” ëë‚¬ìŠµë‹ˆë‹¤!
ê°„ë‹¨íˆê²Œ CLIPì„ ì‚¬ìš©í•´ë´…ì‹œë‹¤!!

```python
import clip
import torch
from PIL import Image
import numpy as np
device = "cuda" if torch.cuda.is_available() else "cpu"

# CLIP model Load
model, preprocess = clip.load("ViT-B/32", device="cuda" if torch.cuda.is_available() else "cpu")
# read image
image = preprocess(Image.open("bike.jpg")).unsqueeze(0).to(device)
#  make embedding!
image_features = model.encode_image(image)
print(np.shape(image_features))
image_features
```

ì–´ë–„ìš”! ì°¸ ì‰½ì£ !>?

![clip_res](https://github.com/user-attachments/assets/fffa04b5-4d6d-44a7-aa80-d22692d32160)

ìœ„ ì´ë¯¸ì§€ì™€ ê°™ì´ ì´ë¯¸ì§€ì˜ CLIP ê²°ê³¼ë¬¼ë¡œ (1,512) ì‚¬ì´ì¦ˆì˜ ë²¡í„°ê°’ì´ ì‚°ì¶œì´ ë©ë‹ˆë‹¤!!

ì´ë²ˆì—” ìœ ì‚¬í•˜ê²Œ í…ìŠ¤íŠ¸ë“¤ì„ ë²¡í„°ê°’ìœ¼ë¡œ ë§Œë“¤ì–´ë³´ê³˜ìŠµë‹ˆë‹¤!!
ì €ëŠ” ì•„ë˜ì˜ 2ê°œ ë¬¸ì¥ì„ ê°ê° ë²¡í„°í™”í•´ë³¼ê²Œìš”~!

 - "a man riding bicycle"
 - "a man climbing mountain"

```python
import clip
import torch
from PIL import Image
import numpy as np
device = "cuda" if torch.cuda.is_available() else "cpu"

# CLIP model Load
model, preprocess = clip.load("ViT-B/32", device="cuda" if torch.cuda.is_available() else "cpu")
# read text
text = clip.tokenize(["a man riding bicycle", "a man climbing mountain"]).to(device)
#  make embedding!
text_features = model.encode_text(text)
print(np.shape(text_features))
text_features
```

ê·¸ëŸ¼~ ê²°ê³¼ëŠ”!? 
ì•„ë˜ì™€ ê°™ì´ 512 ë²¡í„°ê°€ 2ê°œ ìƒì„±ë¨ì„ í™•ì¸í• ìˆ˜ ìˆìŠµë‹ˆë‹¤!!

![clip_res2](https://github.com/user-attachments/assets/2edd7c50-d1b9-4ca7-9b2c-d1daec309acb)


### CLIP ì„±ëŠ¥ í™•ì¸!!!!

ê·¸ëŸ°ë°! ì´ë ‡ê²Œ ë²¡í„°ë§Œ ë§Œë“¤ë©´ ëì¼ê¹Œìš”!? 
ì•„ë‹ˆì£ ~~ ë§Œë“¤ì–´ì§„ ë²¡í„°ê°€ ì •ë§ ìœ ì˜ë¯¸í•œê²ƒì¸ì§€ë¥¼ í™•ì¸í•´ë³´ì•„ì•¼í•´ìš”!!

ì €ëŠ” ì§€ê¸ˆê¹Œì§€ ì•„ë˜ 3ê°œì˜ ë²¡í„°ë¥¼ ë§Œë“¤ì–´ë³´ì•˜ëŠ”ë°ìš”~

- ì´ë¯¸ì§€1 : ìì „ê±°ë¥¼ íƒ€ê³ ìˆìŒ
- ë¬¸ì¥1 : ë‚¨ìê°€ ìì „ê±°íƒ€ê³ ìˆìŒ
- ë¬¸ì¥2 : ë‚¨ìê°€ ì‚°ì„ ì˜¤ë¥´ê³ ìˆìŒ

ì´ë¯¸ì§€1 <-> ë¬¸ì¥1/ë¬¸ì¥2ì˜ ë²¡í„°ì˜ ìœ ì‚¬ì„±ì„ í•œë²ˆ ë¹„êµí•´ë³´ê² ìŠµë‹ˆë‹¤!!

```python
# Find similarity
similarity = (image_features @ text_features.T).softmax(dim=-1)
print(similarity)
```

ìœ„ì˜ ê²°ê³¼ëŠ”!!!!!
ì•„ë˜ì™€ ê°™ì•˜ìŠµë‹ˆë‹¤~!

```output
tensor([[0.9902, 0.0096]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SoftmaxBackward0>)
```

ëœ»ì„ ìƒê°í•´ë³´ë©´ CLIPì´ ì´ë¯¸ì§€ì™€ ì²«ë²ˆì§¸ ë¬¸ì¥ê³¼ëŠ” ê±°ì˜ ë™ì¼í•˜ê³ !! ë‘ë²ˆì¨° ë¬¸ì¥ë ëŠ” ë‹¤ë¥´ë‹¤ëŠ”ê²ƒì„ ë³´ì—¬ì£¼ì—ˆì–´ìš”!

 - ì´ë¯¸ì§€ì™€, ì²«ë²ˆì§¸ ë¬¸ì¥ê³¼ ìœ ì‚¬ì„±: 0.9902
 - ì´ë¯¸ì§€ì™€, ë‘ë²ˆì§¸ ë¬¸ì¥ê³¼ ìœ ì‚¬ì„±: 0.0096

ì–´ë–„ìš”? CLIP! ì´ë¡ ì€ ë³µì¡í–ˆì§€ë§Œ ì‚¬ìš©ì€ ì°¸ ì‰½ì§€ìš”~!?

ì´ ì—°êµ¬ê°€ ê¸°ì´ˆê°€ ë˜ì–´ ì´í›„ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ê°€ í•©ì³ì§€ëŠ” ë©€í‹°ëª¨ë‹¬ë¡œ ë°œì „í•˜ê²Œ ë©ë‹ˆë‹¤!!


### CLIP ì˜ ëª¨ë¸ë³„ ì„±ëŠ¥ì•Œì•„ë³´ê¸°

ì €í¬ëŠ” ì½”ë“œì—ì„œ ViT-B/32 ë¼ëŠ” Base ëª¨ë¸ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤! ê·¸ëŸ°ë°, ë” ë§ì€ ì¢…ë¥˜ì˜ ëª¨ë¸ì´ ìˆë‹¤ëŠ”ê²ƒì„ ì•„ì‹œë‚˜ìš”!?


| ëª¨ë¸ ì´ë¦„     | ë°±ë³¸ íƒ€ì… | í¬ê¸° ë¶„ë¥˜ | íŒŒë¼ë¯¸í„° ìˆ˜ | Zero-shot ImageNet ì •í™•ë„ | ì¶œë ¥ ë²¡í„° í¬ê¸° |
|---------------|-----------|-----------|--------------|-----------------------------|----------------|
| RN50          | ResNet-50 | Small     | ì•½ 102M      | ì•½ 63.3%                    | 1024           |
| RN101         | ResNet-101| Small     | ì•½ 123M      | ì•½ 64.2%                    | 512            |
| ViT-B/32      | ViT       | Small     | ì•½ 149M      | ì•½ 62.0%                    | 512            |
| ViT-B/16      | ViT       | Medium    | ì•½ 151M      | ì•½ 68.6%                    | 512            |
| ViT-L/14      | ViT       | Large     | ì•½ 428M      | ì•½ 75.5%                    | 768            |
| ViT-L/14@336px| ViT       | Large     | ì•½ 428M      | ì•½ 76.2%                    | 768            |

> ë°±ë³¸ íƒ€ì…ì´ë€, CLIP ëª¨ë¸ì—ì„œ ì´ë¯¸ì§€ë‚˜ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ë§í•˜ë©°, ì˜ˆë¥¼ ë“¤ì–´ ResNetì´ë‚˜ ViT ê°™ì€ ëª¨ë¸ì´ ë°±ë³¸ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
> Zero-shot ImageNet ì •í™•ë„ : ëª¨ë¸ì´ ì‚¬ì „ í•™ìŠµë§Œìœ¼ë¡œ ë³„ë„ ì¶”ê°€ í•™ìŠµ ì—†ì´ ImageNet ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í–ˆì„ ë•Œì˜ ì •í™•ë„ë¥¼ ì˜ë¯¸
> ì¶œë ¥ ë²¡í„° í¬ê¸° : ìµœì¢… ì‚°ì¶œë˜ëŠ” ì„ë² íŒ… ë²¡í„°ì˜ ì°¨ì›!! 

ìœ„ì™€ê°™ì´ ëŒ€í‘œì ìœ¼ë¡œ 6ê°œì˜ ëª¨ë¸ì´ ìˆìŠµë‹ˆë‹¤!
(ì‹¤ì œ Python CLIP íŒ¨í‚¤ì§€ì—ì„œëŠ” 9ê°œê°€ available í•©ë‹ˆë‹¤~ ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px'] )
ì—¬ëŸ¬ë¶„ì€ ì•ìœ¼ë¡œ ì‚¬ìš© ëª©ì ì— ë”°ë¼!!

 - ë³´ë‹¤ ë¹ ë¥¸ì†ë„ê°€ ì¤‘ìš”í• ë–„ëŠ” Smallì˜, íŒŒë¼ë¯¸í„°ê°€ ì ì€ ëª¨ë¸ì„
 - ì •í™•ì„±ì´ ê°€ì¥ ì¤‘ìš”íë–„ëŠ” ë²¡í„°ì°¨ì›ë„ í¬ê³  íŒŒë¼ë¯¸í„°ê°€ ë§ì€ ëª¨ë¸ì„

 ì•Œë§ì€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤~!

--- 

ê°ì‚¬í•©ë‹ˆë‹¤!

ê¶ê¸ˆí•œ ì ì´ë‚˜ ë” ì•Œê³  ì‹¶ì€ ì£¼ì œê°€ ìˆë‹¤ë©´ ëŒ“ê¸€ë¡œ ë‚¨ê²¨ì£¼ì„¸ìš” ğŸ’¬
