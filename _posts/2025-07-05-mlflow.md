---
layout: post
title: "🚀Understanding MLflow -MLOps의 필수 도구 MLflow 알아보기?!!"
author: [DrFirst]
date: 2025-07-05 07:00:00 +0900
categories: [AI, Experiment]
tags: [MLflow, MLOps, Machine Learning, Experiment Tracking, Model Registry, Model Deployment]
sitemap :
  changefreq : monthly
  priority : 0.8
---

---

### 🧠 (한국어) MLflow 알아보기?!!  
_🔍 머신러닝 실험부터 배포까지 한 번에 관리하는 MLOps 플랫폼!!!_

> 알고리즘 개발시, 하이퍼파라미터의 변화에 따른 결과를 보는게 중요한데요!   
> 단순히 메모장을 쓰거나 엑셀로 정리하는것은 너무 원시적이죠!?  
> 우리가 코드를 Git으로 관리하듯이,  
> **MLflow**를 통해 다양한 지표들을 체계적으로 관리할 수 있습니다!


> 주요 참고자료: 
> - [MLflow 공식페이지지](https://mlflow.org/)  
> - [Hidden Technical Debt in Machine Learning Systems](https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf) (ML Technical Debt - 2015)

---

### 🧠 MLflow 등장의 배경

> 머신러닝 프로젝트는 코드만으로는 관리가 안 돼!  
> 데이터, 하이퍼파라미터, 모델, 성능 지표까지... 모든 걸 추적해야 해!!  

**ML 프로젝트의 복잡성:**
- **1990년대**: 단순한 통계 모델 → 수동 관리 가능
- **2000년대**: 복잡한 ML 알고리즘 → 실험 관리 필요성 대두
- **2010년대**: 딥러닝 폭발 → **체계적인 MLOps 도구 필수!**

#### 🚨 **기존 방법들의 한계점**

#### **1️⃣ 수동 실험 관리의 문제** 📊  
> 여기가 완전 공감되지 않나요!?ㅎㅎ  

- **실험 추적 불가**: "어떤 파라미터로 이 결과가 나왔지?"
- **재현 불가능**: "저번 주 실험을 다시 해보자!" → 불가능
- **결과 비교 어려움**: Excel에 손으로 복붙하는 비효율
- **협업 어려움**: 개인별로 다른 실험 관리 방식

**예시:**
```python
# 전통적인 방식의 문제점 - 파라미터 튜닝 지옥!
def train_model():
    # a, b, c 파라미터를 0~1까지 0.1씩 테스트 (총 11x11x11 = 1331번!)
    for a in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:
        for b in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:
            for c in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:
                # 모델 훈련
                model = train(param_a=a, param_b=b, param_c=c)
                accuracy = model.evaluate()
                
                # 결과를 프린트만... 😭
                print(f"a={a}, b={b}, c={c} → Accuracy: {accuracy}")
    
    # 문제: 1331개의 결과 중에서 어떤 게 최고였는지 기억 안남!
    # 또 다른 문제: 내일 컴퓨터 껐다 켜면 모든 결과가 사라짐!
```

**실제로 겪는 문제들:**
- 😱 **1331개의 실험 결과가 터미널에 쭉 출력됨**
- 📝 **"어? a=0.7, b=0.3, c=0.9일 때 성능이 좋았는데... 어디갔지?"**
- 💻 **컴퓨터 재시작하면 모든 결과가 사라짐**
- 📊 **어떤 파라미터가 성능에 가장 큰 영향을 미치는지 알 수 없음**

> 사실 저는,,  
> 주피터 노트북에서 해당 셀의 코드와 프린트를 저장하며 보거나.,  
> 따로 메모장에 정리하는 구시대적 방식을 섰었습니다ㅠㅠ  

#### **2️⃣ 모델 버전 관리의 한계** 🔄
- **모델 파일 혼재**: `model_v1.pkl`, `model_final.pkl`, `model_final_final.pkl`
- **환경 불일치**: "내 컴퓨터에서는 되는데?" 문제
- **배포 복잡성**: 모델을 어떻게 서빙할지 매번 고민

**예시:**
```bash
# 혼란스러운 모델 파일들,, 제 이야기입니다ㅠㅠ  
ls models/
model_v1.pkl
model_v2.pkl  
model_best.pkl
model_final.pkl
model_really_final.pkl
model_v3_actually_final.pkl  # 😅
```

#### **3️⃣ 배포와 모니터링의 어려움** 🚀
- **배포 일관성**: 개발 환경과 프로덕션 환경의 차이
- **모델 성능 추적**: 배포 후 성능 저하 감지 어려움
- **롤백 복잡성**: 문제 발생 시 이전 모델로 되돌리기 어려움

> 💡 **해결책**: **MLflow**가 실험부터 배포까지 모든 과정을 통합 관리합니다!

---


### 🔧 간단하게 MLflow 설치하기!!
> 이제 MLflow를 설치하고 장점을 직접 경험해봐요! 

MLflow를 사용하는 방법은 여러 가지가 있지만, Docker를 사용하면 가장 간단하고 안정적입니다!

#### **🐳 Docker로 MLflow 서버 구축하기**

**1단계: 데이터 저장용 폴더 생성**
```bash
# MLflow 데이터 저장용 폴더 생성 (실험 결과가 컴퓨터 재시작 후에도 보존됨)
mkdir -p ~/mlflow/artifacts  # 모델과 파일들 저장
mkdir -p ~/mlflow/db         # 실험 메타데이터 저장
```


**2단계: MLflow 서버 Docker 컨테이너 실행**
> 참고 : https://yijoon009.tistory.com/entry/  

- 우선 Dockerfile을 만들어주구!!  

```
Docker-mlflow-OSError-Errno-30-Read-only-file-system-mlflow
FROM continuumio/miniconda3

# MLflow 설치
RUN pip install mlflow

# 디렉토리 생성 및 권한 설정
RUN mkdir -p /mlflow/db

# 환경 변수 설정
ENV MLFLOW_TRACKING_URI=http://0.0.0.0:5001
ENV MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow/db/mlflow.db

# 컨테이너 실행 시 MLflow 서버 실행
ENTRYPOINT ["mlflow", "server"]
CMD ["--host", "0.0.0.0", "--port", "5001", "--backend-store-uri", "sqlite:///mlflow/db/mlflow.db"]
```

- 빌드하구!!  
```
docker build --network=host -t my-mlflow-image .
```

```bash
# MLflow 서버 실행 (한 번만 실행하면 됨!)
 docker run -d -p 5001:5001 --name mlflow \
            -v $(pwd)/mlflow/db:/mlflow/db \
            -v $(pwd)/mlartifacts:/mlartifacts \
            my-mlflow-image
```

**3단계: 웹 UI 접속 확인**
-브라우저에서 다음 주소로 접속 : http://localhost:5001  
- MLflow 대시보드가 보이면 성공! 🎉


#### **🐍 Python에서 MLflow 사용하기**

**1단계: MLflow 라이브러리 설치**
```bash
pip install mlflow
```

**2단계: Python 코드에서 MLflow 서버 연결**
```python
import mlflow
import numpy as np

# MLflow 서버 연결 설정
mlflow.set_tracking_uri("http://localhost:5001")  # Docker 서버 주소

# 실험 생성 (본인만의 실험명을 입력하세요!)
mlflow.set_experiment("my_first_mlflow_experiment")

# 이제 MLflow 사용 준비 완료! ✨
print("🎉 MLflow 연결 성공!")
print("🌐 브라우저에서 http://localhost:5001 접속하여 실험 결과를 확인하세요!")
```

**🎯 이제 준비 끝! 아래 예시들을 함께 실행해보세요!**

---

### 🔧 MLflow의 4가지 장점 직접 실습해보기!  

---

#### **🏗️ 1. MLflow Tracking** 📊

**핵심 아이디어**: 모든 실험을 자동으로 추적하고 비교 가능하게 저장

**주요 기능:**
- 하이퍼파라미터 자동 로깅
- 성능 지표 추적  
- 아티팩트(모델, 플롯) 저장
- 실험 간 비교 및 시각화

**간단한 실습:**   
-  a, b, c라는 파라미터를 변경해가며 그 결과를 로깅!!  

```python
import mlflow
import numpy as np
import pickle
import os

# 실험 시작
mlflow.set_tracking_uri("http://0.0.0.0:5001")  # Docker 서버 주소
mlflow.set_experiment("param_tuning_experiment")

class SimpleModel:
    """간단한 모델 클래스 - 실제로 저장/로드 가능"""
    def __init__(self, param_a, param_b, param_c):
        self.param_a = param_a
        self.param_b = param_b
        self.param_c = param_c
        self.trained = False
    
    def train(self):
        """모델 훈련 (실제로는 복잡한 과정)"""
        self.result = self.param_a + self.param_b + self.param_c
        self.trained = True
        return self.result
    
    def predict(self, x):
        """예측 함수"""
        if not self.trained:
            raise Exception("모델이 훈련되지 않았습니다!")
        return self.result * x  # 간단한 예측 로직

# 몇 개 조합만 테스트 (전체 1331개는 너무 많으니 일부만!)
test_combinations = [
    (0.1, 0.2, 0.3), (0.5, 0.5, 0.5), (0.8, 0.9, 1.0),
    (1.0, 1.0, 1.0), (0.0, 0.0, 0.0), (0.3, 0.7, 0.4)
]

for a, b, c in test_combinations:
    with mlflow.start_run():
        # 파라미터 로깅
        mlflow.log_param("param_a", a)
        mlflow.log_param("param_b", b) 
        mlflow.log_param("param_c", c)
        
        # 모델 생성 및 훈련
        model = SimpleModel(param_a=a, param_b=b, param_c=c)
        result = model.train()
        
        # 성능 지표 계산
        accuracy = result / 3.0  # 최대값 3.0으로 나누어 정규화
        loss = 3.0 - result      # 손실은 반대로
        
        # 성능 지표 로깅
        mlflow.log_metric("accuracy", accuracy)
        mlflow.log_metric("loss", loss)
        mlflow.log_metric("sum_result", result)
        
        # 모델 저장 (중요!)
        model_path = f"simple_model_{a}_{b}_{c}.pkl"
        with open(model_path, 'wb') as f:
            pickle.dump(model, f)
        
        # MLflow에 모델 아티팩트로 저장
        mlflow.log_artifact(model_path)
        
        # 임시 파일 삭제
        os.remove(model_path)
        
        # 추가 정보 로깅
        mlflow.set_tag("experiment_type", "parameter_sum")
        mlflow.set_tag("model_type", "SimpleModel")
        
        print(f"✅ a={a}, b={b}, c={c} → 합계: {result:.1f}, 정확도: {accuracy:.3f}")

print("🎉 6개의 실험이 모두 MLflow에 저장되었습니다!")
print("🌐 브라우저에서 http://0.0.0.0:5001 으로 접속해서 결과를 확인하세요!")
```

**MLflow의 마법! ✨**
- 🎯 **1331개의 실험이 모두 자동으로 저장됨**
- 🔍 **웹 UI에서 클릭 몇 번으로 최고 성능 조합 찾기 (당연히 a=1.0, b=1.0, c=1.0!)**
- 📊 **a, b, c 중 어떤 파라미터가 성능에 가장 큰 영향을 미치는지 그래프로 확인 (모두 동일하게 기여)**
- 💾 **컴퓨터 껐다 켜도 모든 실험 결과가 그대로 보존**
- 🤝 **팀원과 실험 결과 쉽게 공유**
- 🎨 **간단한 함수 예시지만 실제 복잡한 ML 모델에도 동일하게 적용 가능**

> 아래와 같이 MLflow화면에서 확인이가능해요!
![mlflow1_1](https://github.com/user-attachments/assets/13032527-ef2c-43b8-b165-e543225d2481)


> 각각의 변수별 결과가 어땠는지도 볼수있죠!!
![mlflow1_2](https://github.com/user-attachments/assets/c75da91d-2035-4ad2-8fb0-1d79392e1a0d)

---

#### **🏗️ 2. MLflow Models** 🤖

**핵심 아이디어**: 저장된 모델을 쉽게 불러와서 재사용하기

**간단한 실습: 위에서 저장한 모델을 불러와서 사용하기**

```python
import mlflow
import pickle
import os

class SimpleModel:
    """간단한 모델 클래스 - 실제로 저장/로드 가능"""
    def __init__(self, param_a, param_b, param_c):
        self.param_a = param_a
        self.param_b = param_b
        self.param_c = param_c
        self.trained = False
    
    def train(self):
        """모델 훈련 (실제로는 복잡한 과정)"""
        self.result = self.param_a + self.param_b + self.param_c
        self.trained = True
        return self.result
    
    def predict(self, x):
        """예측 함수"""
        if not self.trained:
            raise Exception("모델이 훈련되지 않았습니다!")
        return self.result * x  # 간단한 예측 로직
    
# 실험 시작
mlflow.set_tracking_uri("http://0.0.0.0:5001")  # Docker 서버 주소
mlflow.set_experiment("param_tuning_experiment")

# 1. 저장된 실험 목록 확인
experiment = mlflow.get_experiment_by_name("param_tuning_experiment")
runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])
print("저장된 실험 목록:")
for idx, run in runs.iterrows():
    print(f"Run ID: {run['run_id'][:8]}... → a={run['params.param_a']}, b={run['params.param_b']}, c={run['params.param_c']}")

# 2. 가장 성능이 좋은 모델 찾기
best_run = runs.loc[runs['metrics.accuracy'].idxmax()]
best_run_id = best_run['run_id']
print(f"\n🏆 최고 성능 모델: Run ID {best_run_id[:8]}...")
print(f"   파라미터: a={best_run['params.param_a']}, b={best_run['params.param_b']}, c={best_run['params.param_c']}")
print(f"   정확도: {best_run['metrics.accuracy']:.3f}")

# 3. 모델 아티팩트 다운로드 및 로드
client = mlflow.tracking.MlflowClient()
artifacts = client.list_artifacts(best_run_id)
print(f"\n📦 저장된 아티팩트: {[art.path for art in artifacts]}")

# 모델 파일 다운로드
model_artifact = [art for art in artifacts if art.path.endswith('.pkl')][0]
local_path = client.download_artifacts(best_run_id, model_artifact.path)
print(f"📥 모델 다운로드 완료: {local_path}")

# 모델 로드
with open(local_path, 'rb') as f:
    loaded_model = pickle.load(f)

print(f"✅ 모델 로드 성공!")
print(f"   모델 파라미터: a={loaded_model.param_a}, b={loaded_model.param_b}, c={loaded_model.param_c}")
print(f"   모델 결과: {loaded_model.result}")

# 4. 로드된 모델로 예측
test_inputs = [1.0, 2.0, 0.5]
for test_input in test_inputs:
    prediction = loaded_model.predict(test_input)
    print(f"   입력 {test_input} → 예측: {prediction}")

print("\n🎉 모델 저장/로드가 완벽하게 작동합니다!")
```
아주 잘 작동됩니다!  
```text
저장된 실험 목록:
Run ID: 56d54172... → a=0.3, b=0.7, c=0.4
Run ID: 9b3b6a75... → a=0.0, b=0.0, c=0.0
Run ID: 682f736c... → a=1.0, b=1.0, c=1.0
Run ID: 934b4956... → a=0.8, b=0.9, c=1.0
Run ID: 48a5f8d4... → a=0.5, b=0.5, c=0.5
Run ID: 9b3822ea... → a=0.1, b=0.2, c=0.3

🏆 최고 성능 모델: Run ID 682f736c...
   파라미터: a=1.0, b=1.0, c=1.0
   정확도: 1.000

📦 저장된 아티팩트: ['simple_model_1.0_1.0_1.0.pkl']
Downloading artifacts: 100%
 1/1 [00:00<00:00, 21.29it/s]
📥 모델 다운로드 완료: /tmp/tmpf9lqcqc5/simple_model_1.0_1.0_1.0.pkl
✅ 모델 로드 성공!
   모델 파라미터: a=1.0, b=1.0, c=1.0
   모델 결과: 3.0
   입력 1.0 → 예측: 3.0
   입력 2.0 → 예측: 6.0
   입력 0.5 → 예측: 1.5

🎉 모델 저장/로드가 완벽하게 작동합니다!
```

---


#### **🏗️ 3. MLflow Model Registry** 🗂️

**핵심 아이디어**: 모델 버전 관리하기 (개발 → 테스트 → 프로덕션)

**실제 회사에서는:**
- **None**: 방금 개발한 모델
- **Staging**: 테스트 중인 모델
- **Production**: 실제 서비스에 사용되는 모델  
- **Archived**: 더 이상 사용하지 않는 모델

**간단한 실습: 모델을 Registry에 등록하고 관리하기**

```python
import mlflow
import mlflow.pyfunc
import os

# MLflow 연결 설정
mlflow.set_tracking_uri("http://0.0.0.0:5001")
mlflow.set_experiment("param_tuning_experiment_model_save")

# 원래 모델 클래스
class SimpleModel:
    def __init__(self, param_a, param_b, param_c):
        self.param_a = param_a
        self.param_b = param_b
        self.param_c = param_c
        self.trained = False

    def train(self):
        self.result = self.param_a + self.param_b + self.param_c
        self.trained = True
        return self.result

    def predict(self, x):
        if not self.trained:
            raise Exception("모델이 훈련되지 않았습니다!")
        return self.result * x

# MLflow에서 사용할 수 있도록 래핑 클래스 정의
class WrappedSimpleModel(mlflow.pyfunc.PythonModel):
    def __init__(self, model):
        self.model = model

    def predict(self, context, model_input):
        return self.model.predict(model_input)

# Run 시작
with mlflow.start_run() as run:
    model = SimpleModel(1.0, 1.0, 1.0)
    result = model.train()

    mlflow.log_param("param_a", 1.0)
    mlflow.log_param("param_b", 1.0)
    mlflow.log_param("param_c", 1.0)
    mlflow.log_metric("accuracy", result / 3.0)

    # pyfunc 모델로 저장
    artifact_path = "model"
    wrapped_model = WrappedSimpleModel(model)

    mlflow.pyfunc.log_model(
        artifact_path=artifact_path,
        python_model=wrapped_model
    )

    model_name = "MyBestModel_save"
    model_uri = f"runs:/{run.info.run_id}/{artifact_path}"

    # Model Registry에 등록
    registered_model = mlflow.register_model(
        model_uri=model_uri,
        name=model_name,
        tags={"team": "data_science", "version": "v1.0"}
    )

    print(f"✅ 모델 '{model_name}' 등록 완료!")
    print(f"   버전: {registered_model.version}")
    print(f"   Run ID: {run.info.run_id}")

# 모델 버전 관리
from mlflow.tracking import MlflowClient
client = MlflowClient()

client.transition_model_version_stage(
    name=model_name,
    version=registered_model.version,
    stage="Staging"
)
print("🎯 모델을 Staging으로 승격!")

client.transition_model_version_stage(
    name=model_name,
    version=registered_model.version,
    stage="Production"
)
print("🚀 모델을 Production으로 승격!")

# Production 모델 사용
production_model_uri = f"models:/{model_name}/Production"
print(f"📦 Production 모델 URI: {production_model_uri}")
print("✅ 이제 다른 팀에서도 이 모델을 안전하게 사용할 수 있습니다!")

```

**결과 확인:**
```text
✅ 모델 'MyBestModel_save' 등록 완료!
   버전: 1
   Run ID: 21070f737db74159933745c7b041cf44
🏃 View run tasteful-elk-947 at: http://0.0.0.0:5001/#/experiments/5/runs/21070f737db74159933745c7b041cf44
🧪 View experiment at: http://0.0.0.0:5001/#/experiments/5
🎯 모델을 Staging으로 승격!
🚀 모델을 Production으로 승격!
📦 Production 모델 URI: models:/MyBestModel_save/Production
✅ 이제 다른 팀에서도 이 모델을 안전하게 사용할 수 있습니다!
```

> 그럼 아래같이 모델이 나와요!! 
![mlflow_model](https://github.com/user-attachments/assets/61c16726-15dc-400c-bf7b-9632f024f742)


---

#### **🏗️ 4. MLflow Projects** 📦

> 아래 부분은.. 그냥 gpt가 써준부분이고 실행안해봤어유~~  

**핵심 아이디어**: 실험 환경을 그대로 재현할 수 있도록 패키징

**실제 사용 예시:**
- 동료가 내 실험을 그대로 재현하고 싶을 때
- 다른 컴퓨터에서 똑같은 환경으로 실험하고 싶을 때
- 프로덕션 환경에서 개발 환경과 동일하게 실행하고 싶을 때

**간단한 실습: MLflow 프로젝트 만들기**

**1단계: 프로젝트 구조 만들기**
```bash
mkdir my_mlflow_project
cd my_mlflow_project
```

**2단계: MLproject 파일 만들기**
```yaml
# MLproject 파일
name: simple_param_tuning

conda_env: conda.yaml

entry_points:
  main:
    parameters:
      param_a: {type: float, default: 0.5}
      param_b: {type: float, default: 0.5}
      param_c: {type: float, default: 0.5}
    command: "python train.py --param_a {param_a} --param_b {param_b} --param_c {param_c}"
```

**3단계: conda.yaml 파일 만들기**
```yaml
# conda.yaml 파일
name: simple_env
channels:
  - conda-forge
dependencies:
  - python=3.9
  - pip
  - pip:
    - mlflow
    - numpy
    - pandas
```

**4단계: train.py 파일 만들기**
```python
# train.py 파일
import mlflow
import argparse

class SimpleModel:
    def __init__(self, param_a, param_b, param_c):
        self.param_a = param_a
        self.param_b = param_b
        self.param_c = param_c
        self.trained = False
    
    def train(self):
        self.result = self.param_a + self.param_b + self.param_c
        self.trained = True
        return self.result

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--param_a', type=float, default=0.5)
    parser.add_argument('--param_b', type=float, default=0.5)
    parser.add_argument('--param_c', type=float, default=0.5)
    args = parser.parse_args()
    
    # MLflow 설정
    mlflow.set_tracking_uri("http://0.0.0.0:5001")
    mlflow.set_experiment("mlflow_project_experiment")
    
    with mlflow.start_run():
        # 파라미터 로깅
        mlflow.log_param("param_a", args.param_a)
        mlflow.log_param("param_b", args.param_b)
        mlflow.log_param("param_c", args.param_c)
        
        # 모델 훈련
        model = SimpleModel(args.param_a, args.param_b, args.param_c)
        result = model.train()
        
        # 성능 지표 로깅
        accuracy = result / 3.0
        mlflow.log_metric("accuracy", accuracy)
        mlflow.log_metric("sum_result", result)
        
        print(f"✅ 파라미터 a={args.param_a}, b={args.param_b}, c={args.param_c}")
        print(f"   결과: {result:.2f}, 정확도: {accuracy:.3f}")

if __name__ == "__main__":
    main()
```

**5단계: 프로젝트 실행하기**
```bash
# 기본 파라미터로 실행
mlflow run . --no-conda

# 커스텀 파라미터로 실행
mlflow run . --no-conda -P param_a=0.8 -P param_b=0.9 -P param_c=1.0

# 다른 사람이 Github에서 직접 실행
mlflow run https://github.com/your-username/my_mlflow_project --no-conda
```

**실행 결과:**
```text
✅ 파라미터 a=0.8, b=0.9, c=1.0
   결과: 2.70, 정확도: 0.900
```

**장점:**
- 🔄 **완벽한 재현성**: 환경이 달라도 똑같은 결과
- 🤝 **쉬운 공유**: Github URL만 공유하면 끝
- 📦 **패키지 관리**: conda로 의존성 자동 관리

---

### 📊 **추가!! : 웹 UI에서 결과 확인하기**

**브라우저에서 http://0.0.0.0:5001 접속하면:**
- **실험 목록**: 모든 실험을 시간순으로 확인
- **성능 지표 비교**: accuracy, loss 등을 그래프로 비교
- **하이퍼파라미터 분석**: 어떤 파라미터가 성능에 영향을 미치는지 분석
- **모델 다운로드**: 저장된 모델 파일 다운로드

**간단한 실습: Python으로 결과 분석하기**
```python
import matplotlib.pyplot as plt
import mlflow

# 실험 결과를 DataFrame으로 가져오기
mlflow.set_tracking_uri("http://0.0.0.0:5001") 
experiment = mlflow.get_experiment_by_name("parameter_tuning_experiment")
runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])

print(f"📊 총 {len(runs)}개의 실험 완료!")

# 간단한 시각화
plt.figure(figsize=(12, 4))

# 1. 파라미터 a vs 성능
plt.subplot(1, 3, 1)
plt.scatter(runs['params.param_a'].astype(float), runs['metrics.accuracy'])
plt.xlabel('Parameter A')
plt.ylabel('Accuracy')
plt.title('A vs Accuracy')

# 2. 파라미터 b vs 성능
plt.subplot(1, 3, 2)
plt.scatter(runs['params.param_b'].astype(float), runs['metrics.accuracy'])
plt.xlabel('Parameter B')
plt.ylabel('Accuracy')
plt.title('B vs Accuracy')

# 3. 파라미터 c vs 성능
plt.subplot(1, 3, 3)
plt.scatter(runs['params.param_c'].astype(float), runs['metrics.accuracy'])
plt.xlabel('Parameter C')
plt.ylabel('Accuracy')
plt.title('C vs Accuracy')

plt.tight_layout()
plt.show()

# 최고 성능 모델 찾기
best_run = runs.loc[runs['metrics.accuracy'].idxmax()]
print("🏆 최고 성능 모델:")
print(f"   📊 Accuracy: {best_run['metrics.accuracy']:.3f}")
print(f"   🎯 a={best_run['params.param_a']}, b={best_run['params.param_b']}, c={best_run['params.param_c']}")
```

**결과 해석:**
- 📈 **a, b, c 모두 값이 클수록 성능 향상** (당연히 합이니까!)
- 🎯 **a=1.0, b=1.0, c=1.0일 때 최고 성능**
- 📊 **모든 파라미터가 동일하게 성능에 기여**

---

### 🎉 **MLflow의 장점 요약**

#### **전통적 방식 vs MLflow**

| 문제 상황 | 전통적 방식 😫 | MLflow 방식 ✨ |
|----------|---------------|----------------|
| **실험 추적** | 터미널 출력만 보고 끝 | 모든 실험이 자동 저장 |
| **모델 관리** | model_final_final.pkl | 체계적인 버전 관리 |
| **결과 비교** | 엑셀에 수동 복붙 | 웹 UI에서 클릭 몇 번 |
| **협업** | 이메일로 파일 전송 | 브라우저에서 공유 |
| **재현성** | "내 컴퓨터에서는 됐는데?" | 완벽한 환경 재현 |


### 🏆 **결론**

MLflow를 통해 더 체계적이고 효율적인 머신러닝 프로젝트를 시작해보세요! 🚀

**🎯 핵심 가치:**
1. **시간 절약**: 수동 작업을 자동화하여 실제 모델 개발에 집중
2. **협업 향상**: 팀원들과 실험 결과를 쉽게 공유하고 재현  
3. **안정성 증대**: 체계적인 모델 관리로 프로덕션 리스크 감소
4. **확장성**: 개인 프로젝트부터 대규모 기업까지 확장 가능

**💡 시작 권장사항:**
- 작은 프로젝트부터 시작하여 점진적으로 확장
- 팀 내 MLflow 사용 가이드라인 수립
- 정기적인 모델 성능 리뷰 프로세스 구축

**📚 추가 학습 자료:**
- [MLflow 공식 문서](https://mlflow.org/docs/latest/index.html)
- [MLflow GitHub 저장소](https://github.com/mlflow/mlflow)

---

> 💡 **마지막 팁**: MLflow는 단순한 도구가 아니라 머신러닝 팀의 생산성을 혁신적으로 개선하는 플랫폼입니다. 작은 것부터 시작해보세요!

**🎉 Happy MLOps!** 🚀 