---
layout: post
title: "ğŸ§  EfficientSAM Hands-On Practice!! : EfficientSAM ì‹¤ìŠµ!! with Python"
author: [DrFirst]
date: 2025-08-26 09:00:00 +0900
categories: [Computer Vision, Experiment]
tags: [EfficientSAM, Segment Anything, Meta AI, Fine-tuning, Python, SAM, Vision]
sitemap:
  changefreq: weekly
  priority: 0.9
---

### ğŸ§¬ (í•œêµ­ì–´) EfficientSAM ì‹¤ìŠµ!!

ì˜¤ëŠ˜ì€ [ì§€ë‚œí¬ìŠ¤íŒ…](https://drfirstlee.github.io/posts/efficientSAM/)ì—ì„œ ì´ë¡ ì— ëŒ€í•˜ì—¬ ê³µë¶€í•´ë³´ì•˜ë˜!  
ê°€ë²¼ìš´ Segmentation ëª¨ë¸ë¸! `LOEfficientSAMRA` ì˜ ì‹¤ìŠµì„ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤!!  


> âœ”ï¸ ê¸°ë³¸ ì‹¤í—˜ ëª©ì :  
> - GPUë¥¼ í™œìš©í•œ EfficientSAMì˜ ì‚¬ìš©  
> - Prompt ê¸°ë°˜ Segmentation  
> - ì´ë¯¸ì§€ì— Box/Pointë¡œ Promptë¥¼ ì¤„ ìˆ˜ ìˆìŒ

---

### ğŸ”§ 1. ì„¤ì¹˜ ë° ì…‹ì—…

EfficientSAMì€ Hugging Face Transformersì²˜ëŸ¼ pip íŒ¨í‚¤ì§€ê°€ ì•„ë‹ˆë¯€ë¡œ,  
GitHubì—ì„œ ì§ì ‘ í´ë¡ í•˜ì—¬ ì„¤ì¹˜í•©ë‹ˆë‹¤.

```bash
# 1. EfficientSAM ë ˆí¬ í´ë¡ 
git clone https://github.com/ChaoningZhang/EfficientSAM.git
cd EfficientSAM
```

ê·¸ëŸ¼!! ì¹œì ˆíˆë„ weights í´ë”ì— `efficient_sam_vits.pt.zip` ëª¨ë¸ì´ ì˜ ì €ì¥ë˜ì–´ìˆìŠµë‹ˆë‹¤~  
---

### ğŸ–¼ï¸ 2. ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ê¸°ë³¸ ì‹¤ìŠµ - CPU&GPU!  

ìš°ì„  git ì— ì €ì¥ëœ `EfficientSAM_example.py` ë¥¼ ì‹¤í–‰í•´ë³´ë©´ ì˜ ì‹¤í–‰ì´ ë˜ê³ !  


```python
from efficient_sam.build_efficient_sam import build_efficient_sam_vitt, build_efficient_sam_vits
# from squeeze_sam.build_squeeze_sam import build_squeeze_sam

from PIL import Image
from torchvision import transforms
import torch
import numpy as np
import zipfile
    


models = {}

# Build the EfficientSAM-Ti model.
models['efficientsam_ti'] = build_efficient_sam_vitt()

# Since EfficientSAM-S checkpoint file is >100MB, we store the zip file.
with zipfile.ZipFile("weights/efficient_sam_vits.pt.zip", 'r') as zip_ref:
    zip_ref.extractall("weights")
# Build the EfficientSAM-S model.
models['efficientsam_s'] = build_efficient_sam_vits()

# Build the SqueezeSAM model.
# models['squeeze_sam'] = build_squeeze_sam()

# load an image
sample_image_np = np.array(Image.open("figs/examples/dogs.jpg"))
sample_image_tensor = transforms.ToTensor()(sample_image_np)
# Feed a few (x,y) points in the mask as input.

input_points = torch.tensor([[[[580, 350], [650, 350]]]])
input_labels = torch.tensor([[[1, 1]]])

# Run inference for both EfficientSAM-Ti and EfficientSAM-S models.
for model_name, model in models.items():
    print('Running inference using ', model_name)
    predicted_logits, predicted_iou = model(
        sample_image_tensor[None, ...],
        input_points,
        input_labels,
    )
    sorted_ids = torch.argsort(predicted_iou, dim=-1, descending=True)
    predicted_iou = torch.take_along_dim(predicted_iou, sorted_ids, dim=2)
    predicted_logits = torch.take_along_dim(
        predicted_logits, sorted_ids[..., None, None], dim=2
    )
    # The masks are already sorted by their predicted IOUs.
    # The first dimension is the batch size (we have a single image. so it is 1).
    # The second dimension is the number of masks we want to generate (in this case, it is only 1)
    # The third dimension is the number of candidate masks output by the model.
    # For this demo we use the first mask.
    mask = torch.ge(predicted_logits[0, 0, 0, :, :], 0).cpu().detach().numpy()
    masked_image_np = sample_image_np.copy().astype(np.uint8) * mask[:,:,None]
    Image.fromarray(masked_image_np).save(f"figs/examples/dogs_{model_name}_mask.png")

# í™•ì¸ìš©: íŒŒë¼ë¯¸í„° ì¥ì¹˜
print("Model param device:", next(models['efficientsam_ti'].parameters()).device)
print("Image tensor device:", sample_image_tensor.device)

```

ì•„ë˜ì™€ ê°™ì´ ê°•ì•„ì§€ê°€ segmentation ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!!  

![Image](https://github.com/user-attachments/assets/72269b64-90a7-4d35-bbf9-9d39185189d3)

ê·¸ëŸ°ë°!! Deviceê°€ CPUì´ê¸°ì—~~\
ì•„ë˜ì™€ ê°™ì´ í•´ë³´ë©´~~ GPU ë¡œ ëŒì•„ê°„ ë¡œê·¸ í™•ì¸ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!!

```python
from efficient_sam.build_efficient_sam import build_efficient_sam_vitt, build_efficient_sam_vits
# from squeeze_sam.build_squeeze_sam import build_squeeze_sam

from PIL import Image
from torchvision import transforms
import torch
import numpy as np
import zipfile
import contextlib

# -----------------------------------
# Device ì„¤ì •
# -----------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# -----------------------------------
# ëª¨ë¸ ë¡œë“œ (+ GPU ì´ë™)
# -----------------------------------
models = {}

# EfficientSAM-Ti
models['efficientsam_ti'] = build_efficient_sam_vitt().to(device).eval()

# EfficientSAM-S (zipì—ì„œ ê°€ì¤‘ì¹˜ ì¶”ì¶œ í›„)
with zipfile.ZipFile("weights/efficient_sam_vits.pt.zip", 'r') as zip_ref:
    zip_ref.extractall("weights")
models['efficientsam_s'] = build_efficient_sam_vits().to(device).eval()

# SqueezeSAM (í•„ìš”ì‹œ)
# models['squeeze_sam'] = build_squeeze_sam().to(device).eval()

# -----------------------------------
# ì…ë ¥ ì¤€ë¹„ (+ GPU ì´ë™)
# -----------------------------------
# load an image
sample_image_np = np.array(Image.open("figs/examples/dogs.jpg"))
sample_image_tensor = transforms.ToTensor()(sample_image_np).to(device)  # [C,H,W] float32 on device

# Feed a few (x,y) points in the mask as input.
input_points = torch.tensor([[[[580, 350], [650, 350]]]], device=device)  # [B=1, N=1, K=2, 2]
input_labels = torch.tensor([[[1, 1]]], device=device)                    # [B=1, N=1, K=2]

# -----------------------------------
# ì¶”ë¡  (AMPëŠ” CUDAì¼ ë•Œë§Œ)
# -----------------------------------
amp_ctx = torch.autocast(device_type="cuda") if device.type == "cuda" else contextlib.nullcontext()

for model_name, model in models.items():
    print('Running inference using', model_name)

    with torch.inference_mode(), amp_ctx:
        predicted_logits, predicted_iou = model(
            sample_image_tensor[None, ...],  # [1,C,H,W]
            input_points,
            input_labels,
        )

    # ì •ë ¬ ë° ìƒìœ„ mask ì„ íƒ
    sorted_ids = torch.argsort(predicted_iou, dim=-1, descending=True)
    predicted_iou = torch.take_along_dim(predicted_iou, sorted_ids, dim=2)
    predicted_logits = torch.take_along_dim(predicted_logits, sorted_ids[..., None, None], dim=2)

    # ì²« ë²ˆì§¸ í›„ë³´ mask ì‚¬ìš©
    mask = (predicted_logits[0, 0, 0] >= 0).to(torch.uint8).cpu().numpy()  # [H,W], uint8 (0/1)

    # ë§ˆìŠ¤í‚¹ í›„ ì €ì¥
    masked_image_np = (sample_image_np.astype(np.uint8) * mask[:, :, None])
    Image.fromarray(masked_image_np).save(f"figs/examples/dogs_{model_name}_mask.png")

# í™•ì¸ìš©: íŒŒë¼ë¯¸í„° ì¥ì¹˜
print("Model param device:", next(models['efficientsam_ti'].parameters()).device)
print("Image tensor device:", sample_image_tensor.device)

```

ê²°ê³¼ë¡œê·¸~!~  

```text
Running inference using efficientsam_ti
Running inference using efficientsam_s
Model param device: cuda:0
Image tensor device: cuda:0
```

### ğŸ§ª 3. Prompt ë¥¼ ë°”ê¿”ê°€ë©° ì‹¤í—˜í•˜ê¸°!!  

EfficientSAMì€ **box** ë˜ëŠ” **point prompt** ê¸°ë°˜ìœ¼ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.  
ë¨¼ì € ê°€ì¥ ê¸°ë³¸ì ì¸ point prompt ëŠ” ì‚¬ì‹¤!! ìœ„ì˜ ìƒ˜í”Œì½”ë“œì—ì„œ ì•Œìˆ˜ ìˆì”ë‹ˆë‹¤!!

ë°”ë¡œ ì•„ë˜ ë¶€ë¶„ì¸ë°ìš”~~ 

```python
...
# Feed a few (x,y) points in the mask as input.
input_points = torch.tensor([[[[580, 350], [650, 350]]]], device=device)  # [B=1, N=1, K=2, 2]
input_labels = torch.tensor([[[1, 1]]], device=device)                    # [B=1, N=1, K=2]
...
```

ìœ„ë¥¼ í•´ì„í•´ë³´ë©´ 2ê°œì˜ positive ì ìœ¼ë¡œ ì§„í–‰í•œ ê²ƒì´ê³ ê³ 

- [580, 350], [650, 350] â†’ ìœ ì €ê°€ í´ë¦­í•œ ì  ì¢Œí‘œ.
- 1, 1 â†’ positive point (ì´ ì˜ì—­ ì•ˆìª½ì„ í¬í•¨).

ë§Œì•½ ì ì˜ labelì´ 0ì´ì—ˆë‹¤ë©´ negative point, ì¦‰ ì˜ì—­ì˜ë°”ê¹¥ì„ ì˜ë¯¸í•˜ëŠ”ê²ƒ ì…ë‹ˆë‹¤!!

ê·¸ëŸ¼!! **bbox**ëŠ”!!?  

```python
...
box_pts = np.array([[x1, y1], [x2, y2]], dtype=np.int64)
box_lbl = np.array([2, 3], dtype=np.int64)
input_points = torch.from_numpy(box_pts)[None, None, ...].to(device)   # [1,1,2,2]
input_labels = torch.from_numpy(box_lbl)[None, None, ...].to(device)   # [1,1,2]
...
```

ìœ„ì˜ ì½”ë“œ í˜•ì‹ìœ¼ë¡œ

- ë‘ ì ì„ ì£¼ê³  labelsì„ [2,3]ë¡œ ì„¤ì •í•˜ë©´ ë°•ìŠ¤ë¡œ í•´ì„ë©ë‹ˆë‹¤.  
- ì´ë•Œ, label 2 = ì¢Œìƒë‹¨, label 3 = ìš°í•˜ë‹¨ ì ì„ì„ ì˜ë¯¸í•˜ì§€ìš”!!

---


---

### âš™ï¸ 4. ì‹¤ì „ ì‘ìš©~!!! groundingDINO + EfficientSAM ì‹¤í—˜í•´ë³´ê¸°  

- Open-Vocaì˜ Object Detection íˆ´, `GroundingDINO` ê¸°ì–µë‚˜ì‹œì£ !?  
- Groundingdino ì„¸íŒ…ì´ ë˜ì–´ì•¼í•˜ë‹ˆ ê¶ê¸ˆí•˜ì‹ ë¶„ì€ [ì˜ˆì „ í¬ìŠ¤íŒ…](https://drfirstlee.github.io/posts/groundingDINO_Detection_usage/)ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”!!  
- ì´ `GroundingDINO` ë¥¼ ë°”íƒ•ìœ¼ë¡œ bboxë¥¼ ë§Œë“¤ê³ ,  
- `EfficientSAM` ìœ¼ë¡œ Segmenetation í•´ë³´ê² ìŠµë‹ˆë‹¤!!  

```python

# 1. íŒ¨í‚¤ì§€ ë° ëª¨ë¸ ë¡œë“œ & ë³€ìˆ˜ ì„¤ì •  
import os, contextlib, zipfile, numpy as np, torch, cv2
from PIL import Image
from torchvision import transforms
from efficient_sam.build_efficient_sam import build_efficient_sam_vitt, build_efficient_sam_vits
from groundingdino.util.inference import load_model, load_image, predict

IMAGE_PATH = "{my_dir}/EfficientSAM_gdino/figs/examples/brush_with_toothbrush_000268.jpg"
TEXT_PROMPT = "toothbrush"

SAVE_PATH = "{my_dir}/EfficientSAM_gdino/figs/examples"
img_name = os.path.splitext(os.path.basename(IMAGE_PATH))[0]

GDINO_CONFIG  = "{my_dir}/EfficientSAM_gdino/grounding_dino/config/GroundingDINO_SwinT_OGC.py"
GDINO_WEIGHTS = "{my_dir}/EfficientSAM_gdino/grounding_dino/weights/groundingdino_swint_ogc.pth"
EFSAM_S_ZIP   = "{my_dir}/EfficientSAM_gdino/weights/efficient_sam_vits.pt.zip"

OUTPUT_DIR = os.path.join(SAVE_PATH, "outputs")
os.makedirs(OUTPUT_DIR, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("[Device]", device)

# ---------- GroundingDINO ----------
gdino = load_model(GDINO_CONFIG, GDINO_WEIGHTS)
image_source, image_pre = load_image(IMAGE_PATH)

# ---------- EfficientSAM ----------
models = {}
models["efficientsam_ti"] = build_efficient_sam_vitt().to(device).eval()
if os.path.isfile(EFSAM_S_ZIP):
    with zipfile.ZipFile(EFSAM_S_ZIP, "r") as zf: zf.extractall("weights")
models["efficientsam_s"]  = build_efficient_sam_vits().to(device).eval()

# ---------- ì´ë¯¸ì§€ í…ì„œ ----------
img_np = np.array(Image.open(IMAGE_PATH).convert("RGB"))
H, W = img_np.shape[:2]
img_tensor = transforms.ToTensor()(img_np).to(device)  # [C,H,W]


# 2. GDINOë¡œ bbox ì–»ê¸° 

# ---------- GDINOë¡œ bbox ì–»ê¸° ----------
boxes_norm, logits, phrases = predict(
    model=gdino, image=image_pre, caption=TEXT_PROMPT,
    box_threshold=0.35, text_threshold=0.25
)
if len(boxes_norm) == 0:
    raise SystemExit("[GroundingDINO] no box")

top = int(torch.argmax(logits).item())

# boxes_norm[top] = (cx, cy, w, h)  in [0,1]
cxcywh = boxes_norm[top].detach().cpu()  # -> CPU tensor

# í…ì„œ ì—°ì‚°ìœ¼ë¡œ x1y1x2y2 ë§Œë“¤ê³ , round + intë¡œ ë³€í™˜
x1, y1, x2, y2 = torch.tensor([
    (cxcywh[0] - cxcywh[2] / 2) * W,
    (cxcywh[1] - cxcywh[3] / 2) * H,
    (cxcywh[0] + cxcywh[2] / 2) * W,
    (cxcywh[1] + cxcywh[3] / 2) * H,
]).round().to(torch.int64).tolist()

# ì¢Œí‘œ ì •ë¦¬/í´ë¨í”„
x1, x2 = max(0, min(x1, x2)), min(W, max(x1, x2))
y1, y2 = max(0, min(y1, y2)), min(H, max(y1, y2))
print(f"[Box pixel] {x1},{y1} â†’ {x2},{y2}")


# 3. bbox promptë¡œ EfficientSamì— ë„£ê¸°!!

 ---------- ì—¬ê¸°ì„œ 'ë°•ìŠ¤ í”„ë¡¬í”„íŠ¸'ë¡œ ë³€í™˜ ----------
# SAM/ESAM ê´€ë¡€: ë‘ ì ì„ ì£¼ê³  labelsì„ [2,3]ë¡œ ì„¤ì •í•˜ë©´ ë°•ìŠ¤ë¡œ í•´ì„ë©ë‹ˆë‹¤.
# label 2 = ì¢Œìƒë‹¨, label 3 = ìš°í•˜ë‹¨
box_pts = np.array([[x1, y1], [x2, y2]], dtype=np.int64)
box_lbl = np.array([2, 3], dtype=np.int64)
input_points = torch.from_numpy(box_pts)[None, None, ...].to(device)   # [1,1,2,2]
input_labels = torch.from_numpy(box_lbl)[None, None, ...].to(device)   # [1,1,2]

# ---------- ì„ íƒ: bbox/í¬ì¸íŠ¸ ì‹œê°í™” ----------
dbg = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR).copy()
cv2.rectangle(dbg, (x1,y1), (x2,y2), (0,255,0), 2)
cv2.circle(dbg, (x1,y1), 4, (255,0,0), -1)  # label=2
cv2.circle(dbg, (x2,y2), 4, (0,0,255), -1)  # label=3
cv2.imwrite(os.path.join(OUTPUT_DIR, f"{img_name}_bbox_prompt.jpg"), dbg)


# 4. ì´ë¯¸ì§€ ì‹œê°í™”

def clip_mask_to_bbox(mask, x1, y1, x2, y2):
    out = np.zeros_like(mask, dtype=mask.dtype)
    out[y1:y2, x1:x2] = mask[y1:y2, x1:x2]
    return out

def keep_largest_component(mask_uint8):
    num, labels = cv2.connectedComponents(mask_uint8.astype(np.uint8))
    if num <= 2: return mask_uint8
    areas = [(labels == i).sum() for i in range(1, num)]
    i_best = int(np.argmax(areas)) + 1
    return (labels == i_best).astype(np.uint8)

amp_ctx = torch.amp.autocast("cuda") if device.type == "cuda" else contextlib.nullcontext()

for name, esam in models.items():
    with torch.inference_mode(), amp_ctx:
        logits, iou = esam(img_tensor[None, ...], input_points, input_labels)  # box prompt
    order = torch.argsort(iou, dim=-1, descending=True)
    logits = torch.take_along_dim(logits, order[..., None, None], dim=2)
    cand = (logits[0,0] >= 0).to(torch.uint8).cpu().numpy()  # [K,H,W]

    # ìƒìœ„ í›„ë³´ ì¤‘ì—ì„œ bbox ì•ˆì˜ ê°€ì¥ í° ì„±ë¶„ ì„ íƒ(ë…¸ì´ì¦ˆ ì–µì œ)
    best = cand[0]
    best = clip_mask_to_bbox(best, x1, y1, x2, y2)
    best = keep_largest_component(best)

    # ì €ì¥
    masked = (img_np.astype(np.uint8) * best[:, :, None])
    Image.fromarray(masked).save(os.path.join(OUTPUT_DIR, f"{img_name}_{name}_mask.png"))
    overlay = img_np.copy()
    overlay[best == 1] = (0.6 * overlay[best == 1] + 0.4 * np.array([0,255,0], np.uint8)).astype(np.uint8)
    cv2.imwrite(os.path.join(OUTPUT_DIR, f"{img_name}_{name}_overlay.jpg"),
                cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))
print("done." , os.path.join(OUTPUT_DIR, f"{img_name}_{name}_overlay.jpg"))

```

![Image](https://github.com/user-attachments/assets/e0d95285-5151-4e49-914e-6c327b530eaa)  

ìœ„ ê³¼ì •ì„ í†µí•´ì„œ bbox ë° Segmentation ê²°ê³¼ë¥¼ ì˜ ë³¼ìˆ˜ ìˆìŠµë‹ˆë‹¤!!  

ì—¬ëŸ¬ ëª¨ë¸ê³¼ ì—°ê²°ë˜ì„œ ì¬ë¯¸ìˆëŠ” í”„ë¡œì íŠ¸ë“¤ì„ í• ìˆ˜ ìˆê² ë„¤ìš”~^^