---
layout: post
title: "📍 GEM: Grounding Everything in Vision-Language Transformers"
author: [DrFirst]
date: 2025-09-08 7:00:00 +0900
categories: [AI, Research]
tags: [Training-Free, Grounding, VLM, Attention, Zero-Shot, CVPR 2024, Segmentation, CVPR]
sitemap:
  changefreq: monthly
  priority: 0.8
---

### 📍 (한국어) GEM: VLM이 가진 잠재적 Localization 능력을 끌어내다!  

![Image](https://github.com/user-attachments/assets/99aa1122-aaaa-4bbb-cccc-ddddeeee1111)

* **제목**: [Grounding Everything: Emerging Localization Properties in Vision-Language Transformers](https://arxiv.org/pdf/2312.00878)  
* **학회**: CVPR 2024  
* **코드/체크포인트**: [GitHub – GEM](https://github.com/WalBouss/GEM)  
* **핵심 키워드**: `Training-Free`, `Grounding`, `Vision-Language Transformer`, `Self-Self Attention`, `Zero-Shot`  
* **요약**: 사전 학습된 Vision-Language Transformer(VLM)의 **내재된 attention 구조**를 활용해, **추가 학습 없이(training-free)** 객체 위치 인식과 분할까지 수행하는 프레임워크 **GEM** 제안!  

---

### 🚀 GEM 핵심 요약

> 한 줄 요약: **“학습 없이, VLM이 본래 가진 attention만으로 localization까지 가능하다!”**

1) **Training-Free Grounding**  
- 추가 파인튜닝 불필요  
- 사전 학습된 VLM에서 바로 localization 성능 추출  

2) **Self-Self Attention (GEM 모듈)**  
- 기존 CLIP surgery의 value–value attention을 일반화  
- patch token 간 **자기-자기 attention**으로 토큰 클러스터링 유도  

3) **Regularization & Iterative Refinement**  
- L2 정규화, adaptive temperature, iterative rollout으로 collapse 방지  
- 안정적이고 의미 있는 attention map 확보  

4) **Zero-Shot Localization & Segmentation**  
- fine-tuned detector 수준에 맞먹는 성능  
- 추가 학습 없이 open-vocabulary grounding 달성  

---

### 🔍 기존 연구의 흐름  

- **CLIP 등 VLM**: 이미지–텍스트 매칭은 뛰어나지만 localization 성능은 약함  
- **기존 open-set grounding**: fine-tuning 필요 (예: GLIP, GroundingDINO)  
- → GEM: **VLM의 attention 구조 자체만으로 training-free grounding 가능**함을 증명  

---

### 🧱 GEM 구조 (Architecture)

![Image](https://github.com/user-attachments/assets/77bbccdd-eeee-4444-aaaa-555566667777)

#### 1) Vision-Language Transformer (VLM)  
- 이미지–텍스트 입력 후 attention map 추출  

#### 2) GEM 모듈 (Self-Self Attention)  
- patch token 간 유사도 계산  
- 동일 객체에 속하는 토큰을 클러스터로 묶음  

#### 3) Regularization  
- **Causal mask collapse** 방지  
- adaptive scaling으로 안정화  

#### 4) Localization & Segmentation  
- 클러스터링된 attention map을 object mask로 변환  
- 텍스트 프롬프트 없이도 zero-shot 객체 위치 예측  

---

### 🧪 실험 결과  

#### 🎯 Segmentation & Localization Benchmarks  
- **PascalContext, ADE20K** 등 복잡한 레이블링 데이터셋에서  
  - 기존 training-free 방법 대비 월등한 성능  
  - fine-tuned 방법에 근접하거나 능가  

#### 🎯 Zero-Shot Point Prediction (OpenImages V7)  
- **최초의 training-free SOTA 성능 달성**  
- LLM/VLM 조합 없이도 localization 가능성 확인  

---

### 👀 정성 비교  

- 단순 CLIP attention: 분산·collapse 현상  
- GEM attention: 명확히 객체 단위로 토큰 묶임  
- 결과적으로 segmentation mask 품질 대폭 개선  

---

### 🧪 Ablation 분석  

- **Self-Self Attention**이 핵심: 토큰 클러스터링 성능 직접 향상  
- **Regularization** 없으면 attention collapse 발생  
- Iterative refinement가 segmentation 세부 품질 개선  

---

## ✅ 결론  

- **GEM**은 Vision-Language Transformer가 **내재적으로 가진 localization 능력**을 발굴한 연구  
- 추가 학습 없는 **training-free grounding** 최초 제안  
- 향후 **더 큰 VLM**과 결합 시, fine-tuned detector를 대체할 잠재력 보유  
- open-world recognition, segmentation, grounding의 새로운 패러다임 제시!  
