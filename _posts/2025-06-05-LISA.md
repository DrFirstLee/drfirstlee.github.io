---
layout: post
title: "📝 Understanding LISA - LISA 알아보기?!!"
author: [DrFirst]
date: 2025-06-05 07:00:00 +0900
categories: [AI, Research]
tags: [LISA, LLM, computer vision, CVPR, CVPR 2024, Segmentation]
sitemap :
  changefreq : monthly
  priority : 0.8
---

---

## 🧠 (한국어) LISA: 추론 기반 세그멘테이션의 새로운 지평  
_🔍 복잡한 언어 지시를 이해하고, 이미지에서 해당 영역을 분할하는 혁신적인 모델!_

> 논문: [LISA: Reasoning Segmentation via Large Language Model](https://arxiv.org/abs/2308.00692)  
> 발표: CVPR 2024 (by CUHK, MSRA, SmartMore)  
> 코드: [dvlab-research/LISA](https://github.com/dvlab-research/LISA)  
> 코멘트: LLM의 언어 이해 능력을 시각 분할에 접목한 획기적인 접근!

---

### ❗ 기존 시각 인식 시스템의 한계

> 여러, 성능 좋은 Segmentation 모델들이 나왔지만!!  
> 이러한 시스템은 **암시적인 사용자 의도**를 이해하고 `추론`하는 능력이 부족하다!  

- **명시적 지시 필요**: 사용자가 직접적으로 대상 객체를 지정해야 함.  
- **사전 정의된 범주 의존**: 새로운 객체나 상황에 대한 유연한 대응이 어려움.  
- **복잡한 추론 부족**: "비타민 C가 많은 음식"과 같은 복잡한 지시를 이해하고 처리하는 데 한계가 있음.

➡️ 이러한 한계를 극복하기 위해, **복잡하고 암시적인 언어 지시를 기반으로 이미지에서 특정 영역을 분할하는 "추론 분할(reasoning segmentation)"**이라는 연구를 진행!!  

> 논문에서 나온 예로는 TV 채널을 바꿔! 했을때 사람은 이해하지만 로봇은 이해못하기에,  
> 테이블로 가서, 리모컨을 찾고, 채널변경 버튼을 눌러! 라고 명령해야하는데, 이런 단점을 해결하고자 추론기능을 넣은것임!    

---

### ✅ LISA의 핵심 특징!

### 🔍 1. 추론 분할(Reasoning Segmentation)

- **복잡한 언어 지시 이해**:  
  "이 이미지에서 미국 대통령이 누구인지 분할 마스크를 출력하고 이유를 설명하세요." 와 같은 지시 처리 가능  
- **세계 지식 활용**:  
  "비타민 C가 많은 음식" 등 실제 지식을 활용해 적절한 영역 분할  
- **설명 제공**:  
  분할 결과에 대한 **이유와 설명 생성 가능**

---

### 🧠 2. LISA 모델 구조

- **<SEG> 토큰 도입**:  
  새로운 토큰 `<SEG>`를 활용해, 임베딩 자체를 마스크로 해석하는 **embedding-as-mask** 패러다임 사용  
- **다중 모달 LLM 활용**:  
  대형 언어 모델의 언어 이해 능력을 시각 정보와 결합  
- **엔드 투 엔드 학습**:  
  언어 지시 + 이미지 → 직접 마스크 생성까지 이어지는 구조

---

### 📊 3. ReasonSeg 벤치마크

LISA의 성능을 평가하기 위해 **ReasonSeg**라는 새로운 벤치마크가 구축함!!  

- 📦 총 샘플 수: **1218**
- 🧪 데이터 구성:
  - Train: 239개
  - Validation: 200개
  - Test: 779개
- 🖼 이미지 출처: OpenImages, ScanNetv2
- 📝 지시문 구성: 짧은 구 + 복잡한 문장

ReasonSeg는 모델이 실제 추론 기반 분할 과제를 얼마나 잘 수행할 수 있는지를 평가하기 위해 설계  

---

### 🏋️‍♂️ 모델 학습 방법!  

LISA는 **end-to-end** 방식으로 학습되며, 다음 세 가지 주요 데이터 소스로 구성됩니다:

#### 1. **Semantic Segmentation Datasets**
> 사용 데이터셋: **ADE20K**, **COCO-Stuff**, **LVIS-PACO**
> `무엇이냐` (ex. 의자) 에 대한 학습!  

- 입력: 이미지 + 클래스 이름  
- 출력: 이진 마스크  
- → 픽셀 수준 시맨틱 이해 학습  
- QA 포맷 예시:  
```text
USER: <IMAGE> Can you segment the chair in this image?
ASSISTANT: It is <SEG>.
```

#### 2. **Referring Segmentation Datasets**  
> 사용 데이터셋: **refCOCO**, **refCOCO+**, **refCOCOg**, **refCLEF**  
> 위 데이터 셋은 모두 `ref*` 데이터 셋으로 `Referring Expression Comprehension`을 위한 대표적인 데이터셋!  
> 이 덕분에 추론이 가능해지는것이지유~~  
> 명시적 객체 지시문을 QA 형식으로 변환 : "the red chair on the right" → "Can you segment the red chair on the right in this image?"  
> 무엇을 넘어 `어떤 것이냐`를 (ex. 나무 의자) 학습  

- 입력: 이미지 + 명시적 객체 설명  
- 출력: 대상 객체에 대한 이진 마스크  
- → 자연어 기반 객체 지정 + 분할 능력 학습  


#### 3. **Visual Question Answering (VQA)**

> 🔎 중요한 점: 학습 데이터에는 reasoning segmentation용 예제가 **포함되지 않았음에도**,  
> LISA는 **제로샷(zero-shot)**으로 ReasonSeg에서 매우 우수한 성능을 보였다는 것!  

- 입력: 이미지 + 자연어 질문  
- 출력: 자연어 답변  
- → 텍스트 이해 + 시각 정보 통합 능력 학습  
- 사용 모델  
 - LLaVA-Instruct-150k (v1)  
 - LLaVA-v1.5-mix665k (v1.5)  

### 🏗 LISA의 구조 : Embedding-as-Mask Paradigm 

![Image](https://github.com/user-attachments/assets/2b33d999-c8c0-4571-adc4-801f45da9911)

기존의 polygon 시퀀스 기반 분할 방식은 연산 비용이 크고 일반화에 어려움이 있었기에,  
LISA는 **Embedding-as-Mask**라는 새로운 구조를 도입합니다.

#### 📌 핵심 구성요소

1. `<SEG>` 토큰 추가 → 분할 요청을 명시
2. LLM의 마지막 레이어에서 `<SEG>` 임베딩 추출
3. MLP를 통해 마스크 임베딩 생성
4. Vision Encoder에서 추출한 시각 특징과 함께 디코더에 입력
5. 최종 이진 마스크 출력

위의 SEG로 부터 마스크 출력 부분을 조금 더 쉽게 이해하고자  
아래와 같이 psuedo code로 흐름을 파악해보았습니다!!  

```python
# 이미지와 텍스트 입력
x_img = load_image_tensor(...)             # [3, H, W]
x_txt = "Can you segment the red chair in this image? It is <SEG>."

# 1. 텍스트 토크나이즈 & <SEG> 위치 확인
input_ids = tokenizer(x_txt, return_tensors='pt')
seg_token_index = input_ids.input_ids[0].tolist().index(tokenizer.convert_tokens_to_ids("<SEG>"))

# 2. Vision Encoder: 이미지 특징 추출
f_img = vision_encoder(x_img)             # shape: [B, C, H', W']

# 3. Multimodal LLM 인코딩
# (이미지 토큰 + 텍스트 토큰 → LLM으로 인코딩)
output_hidden_states = multimodal_llm(input_ids, image_features=f_img, output_hidden_states=True)

# 4. 마지막 레이어에서 <SEG> 토큰의 임베딩 추출
h_tilde_seg = output_hidden_states.last_hidden_state[0, seg_token_index]  # shape: [hidden_dim]

# 5. MLP를 통해 h_seg 생성
h_seg = mlp_projection(h_tilde_seg)       # shape: [proj_dim]

# 6. 마스크 디코더: h_seg + 이미지 피처 → 분할 마스크 생성
pred_mask = mask_decoder(h_seg, f_img)    # shape: [1, H, W], binary segmentation

# 7. Loss 계산 (학습 중일 경우)
loss = bce_loss(pred_mask, gt_mask) + dice_loss(pred_mask, gt_mask)
```

#### 🎯 학습 목표 함수

```math
𝓛 = λ_txt * 𝓛_txt + λ_mask * 𝓛_mask
𝓛_txt: 텍스트 생성 손실 (Auto-regressive CE)
𝓛_mask: 마스크 손실 = BCE + DICE
---

### 🚀 성능과 효율성

| 모델       | GPU 자원                 | 학습 시간              |
|------------|--------------------------|------------------------|
| VisionLLM  | 4 × 8 × A100 80GB        | 50 Epochs (비현실적)   |
| **LISA-7B**| 8 × RTX 3090 24GB        | **3일 미만**           |

**LISA는 효율성과 성능을 모두 만족하는 실용적 분할 모델입니다.**

---

### ✨ 결론

**LISA**는 기존 멀티모달 LLM에 **추론 기반 이미지 분할 능력**을 부여함으로써,  
단순한 명령 이행을 넘어서 **복잡한 언어적 요청을 이해하고 실행할 수 있는 모델**로 진화!!  

> 🔮 LLM 만으로도 그리고 최근나온 Multi Modal Model로 모든것을 다할 수 있을것 같았는데.  
> 향후에는 더 다양한 스타일의 Model 들이 나올것 같고 이런것을 하나로 통합하는 무언가도 나오겠군요!!  

---
