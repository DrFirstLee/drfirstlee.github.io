---
layout: post
title: "ğŸ“ Understanding LISA - LISA ì•Œì•„ë³´ê¸°?!!"
author: [DrFirst]
date: 2025-06-05 07:00:00 +0900
categories: [AI, Research]
tags: [LISA, LLM, computer vision, CVPR, CVPR 2024, Segmentation, <SEG>]
sitemap :
  changefreq : monthly
  priority : 0.8
---

## ğŸ§  (English) LISA: A New Frontier in Reasoning-Based Segmentation

*ğŸ” An innovative model that understands complex linguistic instructions and segments the corresponding regions in an image!*

![Image](https://github.com/user-attachments/assets/998174bb-00d7-47fd-b776-22b8d8795da8)

> Paper: [LISA: Reasoning Segmentation via Large Language Model](https://arxiv.org/abs/2308.00692)  
> Conference: CVPR 2024 (by CUHK, MSRA, SmartMore)  
> Code: [dvlab-research/LISA](https://github.com/dvlab-research/LISA)  
> Comment: A groundbreaking approach combining the language understanding ability of LLMs with visual segmentation!  

---

### â— Limitations of Existing Visual Recognition Systems

> Despite many high-performance segmentation models, they lack the ability to understand **implicit user intent** and perform `reasoning`!

* **Explicit instructions required**: Users must directly specify the target object.  
* **Dependent on predefined categories**: Difficult to handle new objects or scenarios flexibly.  
* **Lacks complex reasoning**: Cannot understand or process instructions like "foods rich in Vitamin C."  

â¡ï¸ To overcome these limitations,  
a new task called **"reasoning segmentation"** was introduced, based on complex and implicit language instructions!

> Example from the paper: when someone says "Change the TV channel," a robot doesn't understand.  
> Instead, it needs a command like "go to the table, find the remote, press the channel button." LISA introduces reasoning to solve such issues.  

---

### âœ… Key Features of LISA!  

### ğŸ” 1. Reasoning Segmentation  

* **Understands complex language instructions**:  
  Able to process commands like "Segment the US president in this image and explain why."  
* **Utilizes world knowledge**:  
  For example, "foods rich in Vitamin C."  
* **Provides explanation**:  
  Can **generate explanations** for the segmentation output.  

---

### ğŸ§  2. Unified Processing! LISA Model Architecture  

* **SEG Token Introduction**:  
  Introduces a new token `SEG` and uses the **embedding-as-mask** paradigm.  
* **Multimodal LLM Integration**:  
  Combines LLM's language understanding with visual information.  
* **End-to-End Training**:  
  Directly maps language instruction + image to segmentation mask.  

---

### ğŸ“Š 3. Creation of the ReasonSeg Benchmark!  

To evaluate LISA's performance, a new benchmark called **ReasonSeg** was created!  

* ğŸ“¦ Total samples: **1218**  
* ğŸ§ª Data split:  
  * Train: 239  
  * Validation: 200  
  * Test: 779  
* ğŸ–¼ Image sources: OpenImages, ScanNetv2  
* ğŸ“ Instruction types: short phrases + complex sentences  

ReasonSeg is designed to evaluate the model's reasoning-based segmentation capabilities.  

---

### ğŸ‹ï¸â€â™‚ï¸ Training Methodology

LISA is trained in an **end-to-end** manner using the following three main data sources:

#### 1. **Semantic Segmentation Datasets**

> Datasets: **ADE20K**, **COCO-Stuff**, **LVIS-PACO**
> Learn "what it is" (e.g., chair)

* Input: image + class name
* Output: binary mask
  ï¸â†’ learns pixel-level semantic understanding
* QA Format Example:

```
USER: <IMAGE> Can you segment the chair in this image?\\
ASSISTANT: It is <SEG>.
```

#### 2. **Referring Segmentation Datasets**

> Datasets: **refCOCO**, **refCOCO+**, **refCOCOg**, **refCLEF**  
> These `ref*` datasets are known to facilitate `reasoning understanding`!  
> Explicit referring expressions converted into QA format: "the red chair on the right" â†’ "Can you segment the red chair on the right in this image?"  
> Learns not only "what" but also "which one specifically" (e.g., wooden chair)  

* Input: image + explicit object description
* Output: binary mask for the target object
  ï¸â†’ learns to localize and segment based on natural language

#### 3. **Visual Question Answering (VQA)**

> ğŸ” Important: Even though **reasoning segmentation examples were not included**,  
> LISA performed impressively on ReasonSeg in **zero-shot** setting!  

* Input: image + natural language question
* Output: natural language answer
  ï¸â†’ learns to integrate visual and language understanding
* Models used:

  * LLaVA-Instruct-150k (v1)
  * LLaVA-v1.5-mix665k (v1.5)

---

### ğŸ LISA Architecture: Embedding-as-Mask Paradigm

![Image](https://github.com/user-attachments/assets/2b33d999-c8c0-4571-adc4-801f45da9911)

Prior polygon-sequence methods are expensive and less generalizable.  
LISA introduces a new structure called **Embedding-as-Mask**.  

#### ğŸ“ Key Components

1. Add `<SEG>` token to specify segmentation request
2. Extract `<SEG>` embedding from the last LLM layer
3. Pass through MLP to generate mask embedding
4. Combine with vision encoder features and pass to decoder
5. Output final binary mask

To better understand the mask output flow from SEG, we follow the pseudocode below:

```
# Image and text input\\
x_img = load_image_tensor(...)             # [3, H, W]\\
x_txt = "Can you segment the red chair in this image? It is <SEG>."

# 1. Tokenize text and find <SEG> token index\\
input_ids = tokenizer(x_txt, return_tensors='pt')\\
seg_token_index = input_ids.input_ids[0].tolist().index(tokenizer.convert_tokens_to_ids("<SEG>"))

# 2. Vision Encoder extracts image features\\
f_img = vision_encoder(x_img)             # [B, C, H', W']

# 3. Multimodal LLM encoding\\
output_hidden_states = multimodal_llm(input_ids, image_features=f_img, output_hidden_states=True)

# 4. Extract embedding for <SEG> from final hidden state\\
h_tilde_seg = output_hidden_states.last_hidden_state[0, seg_token_index]  # [hidden_dim]

# 5. Project with MLP\\
h_seg = mlp_projection(h_tilde_seg)       # [proj_dim]

# 6. Decode to segmentation mask\\
pred_mask = mask_decoder(h_seg, f_img)    # [1, H, W]

# 7. Loss function\\
loss = bce_loss(pred_mask, gt_mask) + dice_loss(pred_mask, gt_mask)
```

---

### ğŸ¯ Training Objective Function

```math
ğ“› = Î»_txt * ğ“›_txt + Î»_mask * ğ“›_mask
ğ“›_txt: Text generation loss  (Auto-regressive CE)
ğ“›_mask:  Mask loss = BCE + DICE
Î»_txt, Î»_mask	: í•˜ì´í¼íŒŒë¼ë¯¸í„°
```


#### ğŸ“‰ 1. Text Generation Loss `ğ“›_txt`

> Evaluates accuracy of the natural language portion **before `<SEG>`**

* Uses **autoregressive cross-entropy loss**, same as typical language modeling


#### ğŸ“‰ 2. Mask Loss  `ğ“›_mask`

> Evaluates segmentation mask accuracy generated from `<SEG>` token embedding

* Combines two losses:

  * **BCE** (pixel-wise accuracy)  
  * **DICE** (overall shape similarity)  


---

### ğŸš€ Efficiency and Performance

| Model       | GPU Resources     | Training Time           |
| ----------- | ----------------- | ----------------------- |
| VisionLLM   | 4 Ã— 8 Ã— A100 80GB | 50 Epochs (unrealistic) |
| **LISA-7B** | 8 Ã— RTX 3090 24GB | **< 3 days**            |

**LISA is a practical segmentation model that excels in both efficiency and performance.**  

---

### âœ¨ Conclusion

**LISA** empowers multimodal LLMs with **reasoning-based image segmentation**,  
evolving them into models capable of understanding and executing **complex natural language instructions**.

> ğŸ”® Initially, it seemed like multimodal models could do everything alone.  
> But going forward, we expect new models of varying styles and perhaps a unified solution to integrate them all!  


---

## ğŸ§  (í•œêµ­ì–´) LISA: ì¶”ë¡  ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ì˜ ìƒˆë¡œìš´ ì§€í‰  
_ğŸ” ë³µì¡í•œ ì–¸ì–´ ì§€ì‹œë¥¼ ì´í•´í•˜ê³ , ì´ë¯¸ì§€ì—ì„œ í•´ë‹¹ ì˜ì—­ì„ ë¶„í• í•˜ëŠ” í˜ì‹ ì ì¸ ëª¨ë¸!_

![Image](https://github.com/user-attachments/assets/998174bb-00d7-47fd-b776-22b8d8795da8)

> ë…¼ë¬¸: [LISA: Reasoning Segmentation via Large Language Model](https://arxiv.org/abs/2308.00692)  
> ë°œí‘œ: CVPR 2024 (by CUHK, MSRA, SmartMore)  
> ì½”ë“œ: [dvlab-research/LISA](https://github.com/dvlab-research/LISA)  
> ì½”ë©˜íŠ¸: LLMì˜ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥ì„ ì‹œê° ë¶„í• ì— ì ‘ëª©í•œ íšê¸°ì ì¸ ì ‘ê·¼!

---

### â— ê¸°ì¡´ ì‹œê° ì¸ì‹ ì‹œìŠ¤í…œì˜ í•œê³„

> ì—¬ëŸ¬, ì„±ëŠ¥ ì¢‹ì€ Segmentation ëª¨ë¸ë“¤ì´ ë‚˜ì™”ì§€ë§Œ!!  
> ì´ëŸ¬í•œ ì‹œìŠ¤í…œì€ **ì•”ì‹œì ì¸ ì‚¬ìš©ì ì˜ë„**ë¥¼ ì´í•´í•˜ê³  `ì¶”ë¡ `í•˜ëŠ” ëŠ¥ë ¥ì´ ë¶€ì¡±í•˜ë‹¤!  

- **ëª…ì‹œì  ì§€ì‹œ í•„ìš”**: ì‚¬ìš©ìê°€ ì§ì ‘ì ìœ¼ë¡œ ëŒ€ìƒ ê°ì²´ë¥¼ ì§€ì •í•´ì•¼ í•¨.  
- **ì‚¬ì „ ì •ì˜ëœ ë²”ì£¼ ì˜ì¡´**: ìƒˆë¡œìš´ ê°ì²´ë‚˜ ìƒí™©ì— ëŒ€í•œ ìœ ì—°í•œ ëŒ€ì‘ì´ ì–´ë ¤ì›€.  
- **ë³µì¡í•œ ì¶”ë¡  ë¶€ì¡±**: "ë¹„íƒ€ë¯¼ Cê°€ ë§ì€ ìŒì‹"ê³¼ ê°™ì€ ë³µì¡í•œ ì§€ì‹œë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ë° í•œê³„ê°€ ìˆìŒ.

â¡ï¸ ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´,  
ë³µì¡í•˜ê³  ì•”ì‹œì ì¸ ì–¸ì–´ ì§€ì‹œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ì—ì„œ íŠ¹ì • ì˜ì—­ì„ ë¶„í• í•˜ëŠ”  
**"ì¶”ë¡  ë¶„í• (reasoning segmentation)"**ì´ë¼ëŠ” ì—°êµ¬ë¥¼ ì§„í–‰!!  

> ë…¼ë¬¸ì—ì„œ ë‚˜ì˜¨ ì˜ˆë¡œëŠ” TV ì±„ë„ì„ ë°”ê¿”! í–ˆì„ë•Œ ì‚¬ëŒì€ ì´í•´í•˜ì§€ë§Œ ë¡œë´‡ì€ ì´í•´ë¥¼ ëª» í•˜ê¸°ì—,  
> í…Œì´ë¸”ë¡œ ê°€ì„œ, ë¦¬ëª¨ì»¨ì„ ì°¾ê³ , ì±„ë„ë³€ê²½ ë²„íŠ¼ì„ ëˆŒëŸ¬! ë¼ê³  ëª…ë ¹í•´ì•¼í•˜ëŠ”ë°,  
> ì´ëŸ° ë‹¨ì ì„ í•´ê²°í•˜ê³ ì ì¶”ë¡ ê¸°ëŠ¥ì„ ë„£ì€ê²ƒì„!    

---

### âœ… LISAì˜ í•µì‹¬ íŠ¹ì§•!

### ğŸ” 1. ì¶”ë¡  ë¶„í• (Reasoning Segmentation)

- **ë³µì¡í•œ ì–¸ì–´ ì§€ì‹œ ì´í•´**:  
  "ì´ ì´ë¯¸ì§€ì—ì„œ ë¯¸êµ­ ëŒ€í†µë ¹ì´ ëˆ„êµ¬ì¸ì§€ ë¶„í•  ë§ˆìŠ¤í¬ë¥¼ ì¶œë ¥í•˜ê³  ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”." ì™€ ê°™ì€ ì§€ì‹œ ì²˜ë¦¬ ê°€ëŠ¥  
- **ì„¸ê³„ ì§€ì‹ í™œìš©**:  
  "ë¹„íƒ€ë¯¼ Cê°€ ë§ì€ ìŒì‹" ë“± ì‹¤ì œ ì§€ì‹ì„ í™œìš©í•´ ì ì ˆí•œ ì˜ì—­ ë¶„í•   
- **ì„¤ëª… ì œê³µ**:  
  ë¶„í•  ê²°ê³¼ì— ëŒ€í•œ **ì´ìœ ì™€ ì„¤ëª… ìƒì„± ê°€ëŠ¥**

---

### ğŸ§  2. í•œë°©ìœ¼ë¡œ ì²˜ë¦¬! LISA ëª¨ë¸ êµ¬ì¡°

- **SEG í† í° ë„ì…**:  
  ìƒˆë¡œìš´ í† í° `SEG`ë¥¼ í™œìš©í•´, ì„ë² ë”© ìì²´ë¥¼ ë§ˆìŠ¤í¬ë¡œ í•´ì„í•˜ëŠ” **embedding-as-mask** íŒ¨ëŸ¬ë‹¤ì„ ì‚¬ìš©  
- **ë‹¤ì¤‘ ëª¨ë‹¬ LLM í™œìš©**:  
  ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥ì„ ì‹œê° ì •ë³´ì™€ ê²°í•©  
- **End-to-End í•™ìŠµ**:  
  ì–¸ì–´ ì§€ì‹œ + ì´ë¯¸ì§€ â†’ ì§ì ‘ ë§ˆìŠ¤í¬ ìƒì„±ê¹Œì§€ ì´ì–´ì§€ëŠ” êµ¬ì¡°

---

### ğŸ“Š 3. ReasonSeg ë²¤ì¹˜ë§ˆí¬ë¼ëŠ” ê²ƒì„ ë§Œë“¦!!

LISAì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ **ReasonSeg**ë¼ëŠ” ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ê°€ êµ¬ì¶•í•¨!!  

- ğŸ“¦ ì´ ìƒ˜í”Œ ìˆ˜: **1218**
- ğŸ§ª ë°ì´í„° êµ¬ì„±:
  - Train: 239ê°œ
  - Validation: 200ê°œ
  - Test: 779ê°œ
- ğŸ–¼ ì´ë¯¸ì§€ ì¶œì²˜: OpenImages, ScanNetv2
- ğŸ“ ì§€ì‹œë¬¸ êµ¬ì„±: ì§§ì€ êµ¬ + ë³µì¡í•œ ë¬¸ì¥

ReasonSegëŠ” ëª¨ë¸ì´ ì‹¤ì œ ì¶”ë¡  ê¸°ë°˜ ë¶„í•  ê³¼ì œë¥¼ ì–¼ë§ˆë‚˜ ì˜ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ ì„¤ê³„  

---

### ğŸ‹ï¸â€â™‚ï¸ ëª¨ë¸ í•™ìŠµ ë°©ë²•!  

LISAëŠ” **end-to-end** ë°©ì‹ìœ¼ë¡œ í•™ìŠµë˜ë©°, ë‹¤ìŒ ì„¸ ê°€ì§€ ì£¼ìš” ë°ì´í„° ì†ŒìŠ¤ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

#### 1. **Semantic Segmentation Datasets**  
> ì‚¬ìš© ë°ì´í„°ì…‹: **ADE20K**, **COCO-Stuff**, **LVIS-PACO**  
> `ë¬´ì—‡ì´ëƒ` (ex. ì˜ì) ì— ëŒ€í•œ í•™ìŠµ!  

- ì…ë ¥: ì´ë¯¸ì§€ + í´ë˜ìŠ¤ ì´ë¦„  
- ì¶œë ¥: ì´ì§„ ë§ˆìŠ¤í¬  
â†’ í”½ì…€ ìˆ˜ì¤€ ì‹œë§¨í‹± ì´í•´ í•™ìŠµ  
- QA í¬ë§· ì˜ˆì‹œ:  

```text
USER: <IMAGE> Can you segment the chair in this image?
ASSISTANT: It is <SEG>.
```

#### 2. **Referring Segmentation Datasets**  
> ì‚¬ìš© ë°ì´í„°ì…‹: **refCOCO**, **refCOCO+**, **refCOCOg**, **refCLEF**  
> ìœ„ ë°ì´í„° ì…‹ì€ ëª¨ë‘ `ref*` ë°ì´í„° ì…‹ìœ¼ë¡œ `ì¶”ë¡  ì´í•´`ë¥¼ ìœ„í•œ ëŒ€í‘œì ì¸ ë°ì´í„°ì…‹!  
> ì´ ë•ë¶„ì— ì¶”ë¡ ì´ ê°€ëŠ¥í•´ì§€ëŠ”ê²ƒì´ì§€ìœ ~~  
> ëª…ì‹œì  ê°ì²´ ì§€ì‹œë¬¸ì„ QA í˜•ì‹ìœ¼ë¡œ ë³€í™˜ : "the red chair on the right" â†’ "Can you segment the red chair on the right in this image?"  
> ë¬´ì—‡ì„ ë„˜ì–´ `ì–´ë–¤ ê²ƒì´ëƒ`ë¥¼ (ex. ë‚˜ë¬´ ì˜ì) í•™ìŠµ  

- ì…ë ¥: ì´ë¯¸ì§€ + ëª…ì‹œì  ê°ì²´ ì„¤ëª…  
- ì¶œë ¥: ëŒ€ìƒ ê°ì²´ì— ëŒ€í•œ ì´ì§„ ë§ˆìŠ¤í¬  
â†’ ìì—°ì–´ ê¸°ë°˜ ê°ì²´ ì§€ì • + ë¶„í•  ëŠ¥ë ¥ í•™ìŠµ  


#### 3. **Visual Question Answering (VQA)**

> ğŸ” ì¤‘ìš”í•œ ì : í•™ìŠµ ë°ì´í„°ì—ëŠ” reasoning segmentationìš© ì˜ˆì œê°€ **í¬í•¨ë˜ì§€ ì•Šì•˜ìŒì—ë„**,  
> LISAëŠ” **ì œë¡œìƒ·(zero-shot)**ìœ¼ë¡œ ReasonSegì—ì„œ ë§¤ìš° ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤ëŠ” ê²ƒ!  

- ì…ë ¥: ì´ë¯¸ì§€ + ìì—°ì–´ ì§ˆë¬¸  
- ì¶œë ¥: ìì—°ì–´ ë‹µë³€  
â†’ í…ìŠ¤íŠ¸ ì´í•´ + ì‹œê° ì •ë³´ í†µí•© ëŠ¥ë ¥ í•™ìŠµ  
- ì‚¬ìš© ëª¨ë¸  
  - LLaVA-Instruct-150k (v1)  
  - LLaVA-v1.5-mix665k (v1.5)  

### ğŸ— LISAì˜ êµ¬ì¡° : Embedding-as-Mask Paradigm 

![Image](https://github.com/user-attachments/assets/2b33d999-c8c0-4571-adc4-801f45da9911)

ê¸°ì¡´ì˜ polygon ì‹œí€€ìŠ¤ ê¸°ë°˜ ë¶„í•  ë°©ì‹ì€ ì—°ì‚° ë¹„ìš©ì´ í¬ê³  ì¼ë°˜í™”ì— ì–´ë ¤ì›€ì´ ìˆì—ˆê¸°ì—,  
LISAëŠ” **Embedding-as-Mask**ë¼ëŠ” ìƒˆë¡œìš´ êµ¬ì¡°ë¥¼ ë„ì…í•©ë‹ˆë‹¤.

#### ğŸ“Œ í•µì‹¬ êµ¬ì„±ìš”ì†Œ

1. `<SEG>` í† í° ì¶”ê°€ â†’ ë¶„í•  ìš”ì²­ì„ ëª…ì‹œ
2. LLMì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ì—ì„œ `<SEG>` ì„ë² ë”© ì¶”ì¶œ
3. MLPë¥¼ í†µí•´ ë§ˆìŠ¤í¬ ì„ë² ë”© ìƒì„±
4. Vision Encoderì—ì„œ ì¶”ì¶œí•œ ì‹œê° íŠ¹ì§•ê³¼ í•¨ê»˜ ë””ì½”ë”ì— ì…ë ¥
5. ìµœì¢… ì´ì§„ ë§ˆìŠ¤í¬ ì¶œë ¥

ìœ„ì˜ SEGë¡œ ë¶€í„° ë§ˆìŠ¤í¬ ì¶œë ¥ ë¶€ë¶„ì„ ì¡°ê¸ˆ ë” ì‰½ê²Œ ì´í•´í•˜ê³ ì  
ì•„ë˜ì™€ ê°™ì´ psuedo codeë¡œ íë¦„ì„ íŒŒì•…í•´ë³´ì•˜ìŠµë‹ˆë‹¤!!  

```python
# ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì…ë ¥
x_img = load_image_tensor(...)             # [3, H, W]
x_txt = "Can you segment the red chair in this image? It is <SEG>."

# 1. í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì¦ˆ & <SEG> ìœ„ì¹˜ í™•ì¸
input_ids = tokenizer(x_txt, return_tensors='pt')
seg_token_index = input_ids.input_ids[0].tolist().index(tokenizer.convert_tokens_to_ids("<SEG>"))

# 2. Vision Encoder: ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
f_img = vision_encoder(x_img)             # shape: [B, C, H', W']

# 3. Multimodal LLM ì¸ì½”ë”©
# (ì´ë¯¸ì§€ í† í° + í…ìŠ¤íŠ¸ í† í° â†’ LLMìœ¼ë¡œ ì¸ì½”ë”©)
output_hidden_states = multimodal_llm(input_ids, image_features=f_img, output_hidden_states=True)

# 4. ë§ˆì§€ë§‰ ë ˆì´ì–´ì—ì„œ <SEG> í† í°ì˜ ì„ë² ë”© ì¶”ì¶œ
h_tilde_seg = output_hidden_states.last_hidden_state[0, seg_token_index]  # shape: [hidden_dim]

# 5. MLPë¥¼ í†µí•´ h_seg ìƒì„±
h_seg = mlp_projection(h_tilde_seg)       # shape: [proj_dim]

# 6. ë§ˆìŠ¤í¬ ë””ì½”ë”: h_seg + ì´ë¯¸ì§€ í”¼ì²˜ â†’ ë¶„í•  ë§ˆìŠ¤í¬ ìƒì„±
pred_mask = mask_decoder(h_seg, f_img)    # shape: [1, H, W], binary segmentation

# 7. Loss ê³„ì‚° (í•™ìŠµ ì¤‘ì¼ ê²½ìš°)
loss = bce_loss(pred_mask, gt_mask) + dice_loss(pred_mask, gt_mask)
```

#### ğŸ¯ í•™ìŠµ ëª©í‘œ í•¨ìˆ˜  

```math
ğ“› = Î»_txt * ğ“›_txt + Î»_mask * ğ“›_mask
ğ“›_txt: í…ìŠ¤íŠ¸ ìƒì„± ì†ì‹¤ (Auto-regressive CE)
ğ“›_mask: ë§ˆìŠ¤í¬ ì†ì‹¤ = BCE + DICE
Î»_txt, Î»_mask	: ë‘ ì†ì‹¤ í•­ëª©ì˜ ê°€ì¤‘ì¹˜ (í•˜ì´í¼íŒŒë¼ë¯¸í„°)
```


##### ğŸ“Œ 1. í…ìŠ¤íŠ¸ ìƒì„± ì†ì‹¤ `ğ“›_txt`

> LLMì´ ìƒì„±í•œ ì‘ë‹µ ë¬¸ì¥ì—ì„œ `<SEG>` ì´ì „ì˜ **ìì—°ì–´ í…ìŠ¤íŠ¸ ë¶€ë¶„ì˜ ì •í™•ë„**ë¥¼ í‰ê°€!!

- ì¼ë°˜ì ì¸ ì–¸ì–´ ëª¨ë¸ í•™ìŠµ ë°©ì‹ê³¼ ë™ì¼í•œ  
  **Autoregressive Cross-Entropy Loss** ì‚¬ìš©

##### ğŸ“Œ 2. ë§ˆìŠ¤í¬ ì†ì‹¤ `ğ“›_mask`

`<SEG>` í† í°ì—ì„œ ì¶”ì¶œí•œ ì„ë² ë”©ì„ í†µí•´ ìƒì„±ëœ **ë¶„í•  ë§ˆìŠ¤í¬ì˜ ì •í™•ë„**ë¥¼ í‰ê°€!  

ë‘ ê°€ì§€ ì†ì‹¤ì„ ì¡°í•©í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤:

- **BCE (Binary Cross-Entropy)**: í”½ì…€ ë‹¨ìœ„ ì •í™•ë„  
- **DICE Loss**: ì „ì²´ ë§ˆìŠ¤í¬ì˜ í˜•íƒœ ìœ ì‚¬ë„ ë°˜ì˜  

---

### ğŸš€ ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±

| ëª¨ë¸       | GPU ìì›                 | í•™ìŠµ ì‹œê°„              |
|------------|--------------------------|------------------------|
| VisionLLM  | 4 Ã— 8 Ã— A100 80GB        | 50 Epochs (ë¹„í˜„ì‹¤ì )   |
| **LISA-7B**| 8 Ã— RTX 3090 24GB        | **3ì¼ ë¯¸ë§Œ**           |

**LISAëŠ” íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì„ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ì‹¤ìš©ì  ë¶„í•  ëª¨ë¸ì…ë‹ˆë‹¤.**

---

### âœ¨ ê²°ë¡ 

**LISA**ëŠ” ê¸°ì¡´ ë©€í‹°ëª¨ë‹¬ LLMì— **ì¶”ë¡  ê¸°ë°˜ ì´ë¯¸ì§€ ë¶„í•  ëŠ¥ë ¥**ì„ ë¶€ì—¬í•¨ìœ¼ë¡œì¨,  
ë‹¨ìˆœí•œ ëª…ë ¹ ì´í–‰ì„ ë„˜ì–´ì„œ **ë³µì¡í•œ ì–¸ì–´ì  ìš”ì²­ì„ ì´í•´í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ëª¨ë¸**ë¡œ ì§„í™”!!  

> ğŸ”® LLM ë§Œìœ¼ë¡œë„ ê·¸ë¦¬ê³  ìµœê·¼ë‚˜ì˜¨ Multi Modal Modelë¡œ ëª¨ë“ ê²ƒì„ ë‹¤í•  ìˆ˜ ìˆì„ê²ƒ ê°™ì•˜ëŠ”ë°.  
> í–¥í›„ì—ëŠ” ë” ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì˜ Model ë“¤ì´ ë‚˜ì˜¬ê²ƒ ê°™ê³  ì´ëŸ°ê²ƒì„ í•˜ë‚˜ë¡œ í†µí•©í•˜ëŠ” ë¬´ì–¸ê°€ë„ ë‚˜ì˜¤ê² êµ°ìš”!!  

---
