---
layout: post
title: "ðŸ‘ï¸ MLLMs Know Where to Look: Training-free Visual Detail Perception"
author: [DrFirst]
date: 2025-07-25 09:00:00 +0900
categories: [AI, Research]
tags: [MLLM, Attention, Cropping, Visual Reasoning, ICLR, ICLR 2025 ]
sitemap :
  changefreq : monthly
  priority : 0.8

---

---

### ðŸ‘ï¸ MLLMs Know Where to Look: Training-free Perception of Visual Details

- **Title**: [MLLMs know where to look: Training-free perception of small visual details with multimodal LLMs](https://arxiv.org/abs/2502.17422)
- **Conference**: ICLR 2025 (Zhang, Jiarui et al.)
- **Code**: [saccharomycetes/mllms_know](https://github.com/saccharomycetes/mllms_know)
- **Keywords**: `Multimodal LLM`, `Small Visual Details`, `Attention Map`, `Cropping`, `Gradient`, `Inference`

---

### ðŸ§  TL;DR in 3 Lines

1. MLLMs are generally good at **knowing where to look**,  
   but often **fail to understand what theyâ€™re seeing**.

2. Simply **cropping the relevant part of the image** and feeding it back  
   significantly improves detail-level recognition.

3. If the image is too large, it is **split and reprocessed to ensure accurate attention**.

---

### âš ï¸ Problem Background

![prob1](https://github.com/user-attachments/assets/d959e40b-4cda-40b5-8d29-2184607d97e5)  

- MLLMs often fail on questions about small objects in an image,  
  but they succeed if we crop and provide only the relevant region.

---

### ðŸ“š Datasets Used

The authors validate their method on the following 6 datasets:

| Dataset     | Purpose                                         | Image Type             | Question Focus                          | External Knowledge | Example Models                |
|-------------|--------------------------------------------------|-------------------------|------------------------------------------|---------------------|-------------------------------|
| **DocVQA**   | Document-level question answering                | Document images (PDFs) | Text extraction + layout understanding   | âŒ                  | LayoutLM, Donut, DocFormer   |
| **TextVQA**  | Scene text-based VQA                             | Natural images w/ text | Text in context of visual scene          | âŒ                  | M4C, GRILL, LLaVA            |
| **POPE**     | Evaluating model bias and hallucination          | Mixed image types       | Robustness to misleading contexts        | âŒ                  | BLIP2, Pythia                |
| **A-OKVQA**  | Knowledge-based multiple-choice VQA              | Natural images          | External knowledge + choice selection    | âœ…                  | ReGAT, RAVQA, NoteMR         |
| **GQA**      | Relation reasoning and scene understanding       | Complex scenes          | Logic and spatial reasoning              | âŒ                  | MAC, NS-VQA, GraftNet        |
| **VQAv2**    | General-purpose VQA benchmark                    | Natural images          | Object, attribute, and general questions | âŒ                  | UpDn, Pythia, LXMERT         |

---

### ðŸ”§ Three Key Investigations

0. **Can humans solve these problems better just by cropping?**  
   â†’ Manually cropping the region significantly improved model performance!

1. **Do LLMs fail because they donâ€™t know where to look, or because they canâ€™t understand even when looking correctly?**  
   â†’ Itâ€™s the latter: they look in the right place but misinterpret it.

2. **Then what if we just show them the right region only?**  
   â†’ That works very well!

#### 0. Human cropping improves accuracy

![crop_Effect](https://github.com/user-attachments/assets/a7a2ab90-5998-41bd-8bea-31fdb64acd76)  
- When humans crop only the relevant region of the image,
- MLLMs answer detail-based questions much more accurately.

#### ðŸ” 1. Do MLLMs attend to the right place?

![looking](https://github.com/user-attachments/assets/e73e8306-f992-43cd-b948-8723e1884ca2)  
- By visualizing attention layers,  
- It turns out the model **does look in the right area** even when it gives a wrong answer.

#### âœ‚ï¸ 2. Just give the right region â†’ better performance!

![cropping](https://github.com/user-attachments/assets/74214622-05ed-4dc8-9b43-ce3f2ff8857d)

- As seen above, **cropping and reinserting alone greatly boosts performance**
- So, how to crop effectively?
- The authors propose **3 attention-based cropping strategies**:

| Method      | Description |
|-------------|-------------|
| **Rel-Att** (Relative Attention) | Compares attention maps between the true question and a generic one to highlight the difference |
| **Grad-Att** (Gradient-weighted Attention) | Uses gradients to find regions most sensitive to the model's confidence |
| **Pure-Grad** (Input Gradient) | Uses input image gradients to locate visually salient pixels |

**Cropping pipeline**:

- **Input**: image + question  
- **Process**: compute attention map via one of the above â†’ derive ROI crop  
- **Output**: crop image â†’ reinsert to MLLM â†’ generate answer

The paper also compares cropping methods using **external tools like YOLO, CLIP, and SAM**:

> Surprisingly, even against SOTA external methods, their proposed internal methods held up well.

![crop_res](https://github.com/user-attachments/assets/3d5510f0-4e4d-4e1a-a054-2a213158e9a2)

| Method         | One-line Summary |
|----------------|------------------|
| **CLIP ViCrop** | Uses CLIP similarity to iteratively crop toward the most semantically aligned region |
| **YOLO ViCrop** | Selects bounding boxes from YOLO with highest CLIP similarity to the question |
| **SAM ViCrop**  | Converts segmentation masks from SAM into bounding boxes, then selects the one with best CLIP match |

---

### ðŸ§ª Experiment Results

- The system performs **inference-only cropping**â€”no retraining required
- Large images are pre-cropped to better guide attention
- Evaluation covers multiple datasets and question types

---

### ðŸ“ˆ Key Results

![res1](https://github.com/user-attachments/assets/ebb016dc-7ef6-448a-859c-d8a1bc8284d2)  
- Attention-based crops like Rel-Att and Grad-Att outperform other approachesâ€”especially for small-object questions.

![res2](https://github.com/user-attachments/assets/9a546460-7318-4318-bf91-6a2e1b4f988d)  
- Cropping greatly helps when image resolution is high.

**Summary of Effects**:

| Setup                      | Performance Impact |
|---------------------------|--------------------|
| Full image only           | Poor on detail-based questions |
| Crop via attention-guided methods | Much higher accuracy |
| No retraining needed      | Zero-shot + Inference-time only |

Overall, this approach **greatly improves fine-grained perception**,  
even without scaling up the model size.

---

## âœ… Conclusion & Impact

- The paper shows MLLMs **already know where to look**,  
  but need help **seeing better** via focused cropping.
- Significant performance gains are possible **without any retraining**â€”just with attention-based inference.
- Has strong applicability in domains like **OCR**, **tiny-object detection**, or **interactive AI tutors**.

> "**MLLMs know where to look. Letâ€™s help them see better.**"


---

### ðŸ‘ï¸ (í•œêµ­ì–´) MLLMs Know Where to Look: Training-free ì‹œê° ë””í…Œì¼ ì¸ì‹

- **ì œëª©**: [MLLMs know where to look: Training-free perception of small visual details with multimodal llms](https://arxiv.org/abs/2502.17422)
- **í•™íšŒ**: ICLR 2025 (Zhang, Jiarui et al.)
- Code: [saccharomycetes/mllms_know](https://github.com/saccharomycetes/mllms_know)
- **í•µì‹¬ í‚¤ì›Œë“œ**: `Multimodal LLM`, `Small Visual Details`, `Attention Map`, `Cropping`, `Gradient`, `Inference`

---

### ðŸ§  3ì¤„ ìš”ì•½

1. MLLMì€ ì´ë¯¸ì§€ ë‚´ **â€˜ì–´ë””ë¥¼ ë³´ëŠ”ì§€â€™ëŠ” ìž˜ íŒŒì•…**í•˜ì§€ë§Œ,  
   **â€˜ë¬´ì—‡ì„ ë³´ëŠ”ì§€â€™ëŠ” ì •í™•ížˆ ì¸ì‹í•˜ì§€ ëª»í•˜ëŠ” í•œê³„**ê°€ ìžˆìŒ.

2. ì´ë¯¸ì§€ì˜ ì¤‘ìš”í•œ ë¶€ë¶„ì„ **cropí•´ì„œ ë‹¤ì‹œ ìž…ë ¥**í•˜ë©´,  
   ëª¨ë¸ì´ ì‹œê°ì  ë””í…Œì¼ì„ í›¨ì”¬ ì •í™•ížˆ ì¸ì‹í•¨.

3. ì´ë¯¸ì§€ê°€ ë„ˆë¬´ í° ê²½ìš°ì—ëŠ” ì •í™•í•œ attentionì„ ìœ„í•´ ìž˜ë¼ì„œ ì‚¬ìš©í•˜ê³  ë¶™ìž„!   


---

### âš ï¸ ë°°ê²½: ê¸°ì¡´ ë¬¸ì œì  ìš”ì•½

![prob1](https://github.com/user-attachments/assets/d959e40b-4cda-40b5-8d29-2184607d97e5)  

- ì´ë¯¸ì§€ ë‚´ì—ì„œ ìž‘ì€ ê°ì²´ì— ëŒ€í•œ ì§ˆë¬¸ì„ í—€ì„ë•Œ ë‹µì„ í‹€ë¦¬ì§€ë§Œ, í•´ë‹¹ ë¶€ë¶„ë§Œì„ crop í•´ì„œ ë³´ì—¬ì£¼ë©´ ë‹µì„ ìž˜í•¨  

---

### ì°¸ê³ . ì‚¬ìš©í•œ ë°ì´í„°ì…‹  
- ì—¬ê¸°ì„œëŠ” ë¡œì§ê²€ì¦ì„ ìœ„í•´ ì•„ëž˜ 6ê°€ì§€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤!!  

| ë°ì´í„°ì…‹     | ì£¼ìš” ëª©ì                                       | ì´ë¯¸ì§€ ìœ í˜•           | ì§ˆë¬¸ ì´ˆì                             | ì™¸ë¶€ ì§€ì‹ í•„ìš” | ëŒ€í‘œ ëª¨ë¸ ì˜ˆì‹œ               |
|--------------|-----------------------------------------------|------------------------|---------------------------------------|----------------|------------------------------|
| **DocVQA**   | ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ (ì¸ë³´ì´ìŠ¤, ë³´ê³ ì„œ ë“±)      | ë¬¸ì„œ ì´ë¯¸ì§€ (PDF ë“±)  | í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ + ë¬¸ì„œ êµ¬ì¡° ì´í•´     | âŒ             | LayoutLM, Donut, DocFormer  |
| **TextVQA**  | ìž¥ë©´ ë‚´ ê¸€ìžë¥¼ í¬í•¨í•œ ì§ˆì˜ì‘ë‹µ                | ìžì—° ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸  | ì‹œê° ë¬¸ë§¥ ì† í…ìŠ¤íŠ¸ ì´í•´               | âŒ             | M4C, GRILL, LLaVA           |
| **POPE**     | VQA ëª¨ë¸ì˜ íŽ¸í–¥(Bias)ê³¼ í™˜ê°(Hallucination) í‰ê°€ | ë‹¤ì–‘í•œ (í˜¼í•©í˜•) ì´ë¯¸ì§€ | ëª¨ë¸ì˜ bias robustness í‰ê°€           | âŒ             | BLIP2, Pythia               |
| **A-OKVQA**  | ì™¸ë¶€ ì§€ì‹ ê¸°ë°˜ VQA + ì •ëŸ‰ í‰ê°€                | ìžì—° ì´ë¯¸ì§€           | ì§€ì‹ ê¸°ë°˜ ì§ˆì˜ + ì„ íƒì§€ ê¸°ë°˜ ì‘ë‹µ      | âœ…             | ReGAT, RAVQA, NoteMR        |
| **GQA**      | ê´€ê³„ ì¶”ë¡ , ê°ì²´ ê°„ ì˜ë¯¸ì  ì—°ê²°                | ë³µìž¡í•œ ìž¥ë©´ ì´ë¯¸ì§€     | ìž¥ë©´ ì´í•´ + ê´€ê³„ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ         | âŒ             | MAC, NS-VQA, GraftNet       |
| **VQAv2**     | ì¼ë°˜ VQA ë²¤ì¹˜ë§ˆí¬, ë‹¤ì–‘í•œ ì§ˆë¬¸ ìœ í˜• í¬í•¨       | ìžì—° ì´ë¯¸ì§€           | ê°ì²´, ì†ì„±, ìž¥ë©´ ë“± ì „ë°˜ì  ì§ˆì˜ì‘ë‹µ    | âŒ             | UpDn, Pythia, LXMERT        |



---

### ðŸ”§ 3ê°€ì§€ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ì„œ í•´ê²° ë°©ë²•ì„ ì°¾ìŒ

0. ì •ë§ ìž‘ì€ ë¶€ìœ„ë¥¼ cropí•´ì„œ ë³´ì—¬ì£¼ë©´ ë¬¸ì œë¥¼ ìž˜ ë§žì¶œê¹Œ?  
 - ì‚¬ëžŒì´ í¬ë¡­í•´ì„œ í…ŒìŠ¤íŠ¸í•´ë´„!!  

1. LLMì€ ì–´ë””ë¥¼ ë³¼ì§€ë„ ëª°ë¼ì„œ í‹€ë¦°ê±¸ê¹Œ? í˜¹ì€ ë¶€ìœ„ëŠ” ìž˜ ì°¾ì•˜ëŠ”ë° ìž˜ëª» ì¸ì§€í•œê±¸ê¹Œ?  
 - ê²°ë¡ ì€ í›„ìž, ë¶€ìœ„ëŠ” ìž˜ ì°¾ì•˜ì§€ë§Œ ìž˜ëª» ì¸ì§€í•œê²ƒìž„!  

2.ê·¸ëŸ¼! í•´ë‹¹ ë¶€ìœ„ë§Œì„ ì œì‹œí•˜ë§Œ ìž˜ ìž‘ë™í• ê¹Œ??
 - ê·¸ë ‡ë‹¤!!

####  0. ì •ë§ ìž‘ì€ ë¶€ìœ„ë¥¼ cropí•´ì„œ ë³´ì—¬ì£¼ë©´ ë¬¸ì œë¥¼ ìž˜ ë§žì¶œê¹Œ?  
![crop_Effect](https://github.com/user-attachments/assets/a7a2ab90-5998-41bd-8bea-31fdb64acd76)  
- ì´ë¯¸ì§€ ë‚´ì˜ ìž‘ì€ ë¶€ë¶„ì„ ë§žì¶”ëŠ” ì§ˆë¬¸ì—ì„œ,  
- ì‚¬ëžŒì´ ì •ë‹µë¶€ë¶„ë§Œ cropí•´ì„œ ì œì‹œí•  ê²½ìš° í™•ì‹¤ížˆ ìž˜ ëŒ€ë‹µí•´!!  


#### ðŸ” 1. LLMì€ ì–´ë””ë¥¼ ë³¼ì§€ë„ ëª°ë¼ì„œ í‹€ë¦°ê±¸ê¹Œ? í˜¹ì€ ë¶€ìœ„ëŠ” ìž˜ ì°¾ì•˜ëŠ”ë° ìž˜ëª» ì¸ì§€í•œê±¸ê¹Œ?  
![looking](https://github.com/user-attachments/assets/e73e8306-f992-43cd-b948-8723e1884ca2)  
- MLLM ë ˆì´ì–´ì—ì„œ ì–´í…ì…˜ì„ ì¶”ì¶œí•´ì„œ ì‹œìž‘í™”í•´ë³´ë©´!!  
- ë¹„ë¡ ì •ë‹µì€ í‹€ë ¸ì§€ë§Œ ì–´ë””ë¥¼ ë´ì•¼í•˜ëŠ”ì§€ëŠ” ìž˜ ì•Œê³  ìžˆë‹¤ëŠ”ê²ƒì„ ì•Œìˆ˜ ìžˆì§€!!    



#### âœ‚ï¸ 2.ê·¸ëŸ¼! í•´ë‹¹ ë¶€ìœ„ë§Œì„ ì œì‹œí•˜ë§Œ ìž˜ ìž‘ë™í• ê¹Œ??

![cropping](https://github.com/user-attachments/assets/74214622-05ed-4dc8-9b43-ce3f2ff8857d)

- 0ì—ì„œ í™•ì¸í–ˆë“¯, **ì´ë¯¸ì§€ë¥¼ ìž˜ë¼ ë‹¤ì‹œ ë„£ê¸°ë§Œ í•´ë„ ì„±ëŠ¥ì´ ê¸‰ìƒìŠ¹!**
- ê·¸ëŸ¼, ì–´ë–»ê²Œ crop í•˜ì§€?!  
-  3ê°€ì§€ Attention ê¸°ë°˜ Cropping ì „ëžµ  

| ë°©ë²• | ì„¤ëª… |
|------|------|
| **Rel-Att** (Relative Attention) | ì •ë‹µ ì§ˆë¬¸ vs ì¼ë°˜ ì§ˆë¬¸ì˜ attention mapì„ ë¹„êµí•´, **ì°¨ì´ì ì„ ê°•ì¡°**í•˜ì—¬ crop ì˜ì—­ ë„ì¶œ |
| **Grad-Att** (Gradient-weighted Attention) | ì •ë‹µ í™•ë¥ ì— ëŒ€í•œ **gradientë¥¼ í†µí•´ ë¯¼ê° ì˜ì—­**ì„ ê°•ì¡°í•¨ |
| **Pure-Grad** (Input Gradient) | ì´ë¯¸ì§€ ìžì²´ì˜ gradientë¥¼ í†µí•´, **í”½ì…€ ë‹¨ìœ„ë¡œ ì¤‘ìš”í•œ ì˜ì—­**ì„ ì¶”ì¶œí•¨ |

- crop ë°©ë²•ì€?  

  - **ìž…ë ¥**: ì´ë¯¸ì§€ + ì§ˆë¬¸  
  - **ì²˜ë¦¬**: ìœ„ 3ê°€ì§€ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ attention map ê³„ì‚° â†’ crop ì˜ì—­ ì„¤ì •  
  - **ì¶œë ¥**: cropëœ ì´ë¯¸ì§€ë¥¼ MLLMì— ë‹¤ì‹œ ë„£ì–´ **ë‹µì„ ìƒì„±**

- ì¶”ê°€ë¡œ ì´ë²ˆ ì—°êµ¬ì—ì„œëŠ” YOLO, CLIP, SAMë“±ì„ ì‚¬ìš©í•œ crop ë°©ë²•ê³¼ ì„±ëŠ¥ì„ ë¹„êµí–ˆê³ !  

> ê¸°ì¡´ SOTA ì—°êµ¬ë¥¼ í™œìš©í•œ crop ê³¼ ë¹„êµí•´ë„ ë‚˜ì˜ì§€ ì•Šì•˜ë‹¤!! 
![crop_res](https://github.com/user-attachments/assets/3d5510f0-4e4d-4e1a-a054-2a213158e9a2)  

| ë°©ë²•              | í•œ ì¤„ ìš”ì•½                                                              |
| --------------- | ------------------------------------------------------------------- |
| **CLIP ViCrop** | CLIPì„ ì‚¬ìš©í•´ ì§ˆë¬¸ê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ê°€ìž¥ ê´€ë ¨ ìžˆëŠ” ì˜ì—­ì„ **ì ì§„ì ìœ¼ë¡œ ìž˜ë¼ê°€ë©° ë°˜ë³µ ì„ íƒ**í•˜ëŠ” ë°©ì‹.         |
| **YOLO ViCrop** | YOLOë¡œ íƒì§€ëœ ê°ì²´ ì˜ì—­ ì¤‘, ì§ˆë¬¸ê³¼ **CLIP ìœ ì‚¬ë„ê°€ ê°€ìž¥ ë†’ì€ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì„ íƒ**í•˜ëŠ” ë°©ì‹.         |
| **SAM ViCrop**  | SAMì´ ì œê³µí•˜ëŠ” **ì„¸ê·¸ë©˜íŠ¸ ë§ˆìŠ¤í¬ë¥¼ ë°”ìš´ë”© ë°•ìŠ¤ë¡œ ë³€í™˜í•œ í›„**, CLIP ìœ ì‚¬ë„ê°€ ê°€ìž¥ ë†’ì€ ì˜ì—­ì„ ì„ íƒí•˜ëŠ” ë°©ì‹. |


---

### ðŸ§ª ì‹¤í—˜ ë¶„ì„ ê²°ê³¼!!   

- ì‹¤í—˜ì€ **training ì—†ì´**, inference ì‹œ attention ê¸°ë°˜ cropì„ ìˆ˜í–‰í•˜ëŠ” êµ¬ì¡°  
- í° ì´ë¯¸ì§€ëŠ” **ì‚¬ì „ crop**í•˜ì—¬ attentionì´ ë” ìž˜ ìž¡ížˆë„ë¡ ì„¤ê³„í•¨  
- ë‹¤ì–‘í•œ ì§ˆë¬¸ ìœ í˜•ì— ëŒ€í•´ crop í›„ ë‹µë³€ì„ ìƒì„±í•˜ê³  ì„±ëŠ¥ ë¹„êµ  

---

### ðŸ“ˆ ì£¼ìš” ì„±ê³¼

![res1](https://github.com/user-attachments/assets/ebb016dc-7ef6-448a-859c-d8a1bc8284d2)  
- Rel-att ì´ë‚˜ grad-att ë°©ì‹ìœ¼ë¡œ í¬ë¡­í•œê²ƒì´ ê°€ìž¥ ê²°ê³¼ê°€ ì¢‹ë‹¤!! íŠ¹ížˆ ìž‘ì€ ê°ì²´ì— ëŒ€í•œ ì§ˆë¬¸ì—ì„œ!!

![res2](https://github.com/user-attachments/assets/9a546460-7318-4318-bf91-6a2e1b4f988d)  
- í•´ìƒë„ê°€ í° ì´ë¯¸ì§€ëŠ”, ìž˜ë¼ì„œ ìž‘ì—…í•˜ëŠ”ê²Œ íš¨ê³¼ê°€ ì¢‹ì•˜ë‹¤!!


- ì„±ê³¼ ìš”ì•½!!  
| ì¡°ê±´ | ì„±ëŠ¥ |
|------|------|
| Full image ìž…ë ¥ | ìž‘ì€ ë””í…Œì¼ ì§ˆë¬¸ì— ì·¨ì•½ |
| Attention-guided crop â†’ ìž¬ìž…ë ¥ | ë””í…Œì¼ ì§ˆë¬¸ ì •í™•ë„ **ìƒë‹¹ í–¥ìƒ** |
| No retraining | **Zero-shot + Inference-time only** ë°©ì‹ |

- ì‹¤í—˜ ê²°ê³¼, ìž‘ì€ ë””í…Œì¼ì´ ì¤‘ìš”í•œ taskì—ì„œ ì„±ëŠ¥ì´ **í™•ì—°ížˆ í–¥ìƒ**
- íŠ¹ížˆ ê¸°ì¡´ MLLM ëŒ€ë¹„, **ê³ ì„±ëŠ¥ ëŒ€í˜•ëª¨ë¸ ì—†ì´ë„ ê°œì„ ** ê°€ëŠ¥

---

## âœ… ê²°ë¡  ë° ì˜ì˜

- ì´ ë…¼ë¬¸ì€ MLLMì´ ì •í™•ížˆ "ì–´ë””ë¥¼ ë³´ì•„ì•¼ í•˜ëŠ”ì§€"ëŠ” ìž˜ ì•„ëŠ”ë°,  
  "ë³´ëŠ” ë°©ì‹"ì´ ë¶€ì¡±í•˜ë‹¤ëŠ” ì ì„ **Attention-based Cropping**ìœ¼ë¡œ í•´ê²°í•¨
- **Training ì—†ì´ inferenceë§Œìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ** ê°€ëŠ¥í•˜ë‹¤ëŠ” ì ì—ì„œ  
  **ê²½ëŸ‰í™”, ì‘ìš©ì„±, í•´ì„ë ¥ ì¸¡ë©´**ì—ì„œ ë§¤ìš° ì‹¤ìš©ì ì¸ ì ‘ê·¼
- ë‹¤ì–‘í•œ downstream task (e.g. OCR, ì„¸ë°€í•œ ë¬¼ì²´ ì¸ì‹, íŠœí„°ë§ ì‹œìŠ¤í…œ)ì— ì‘ìš© ê°€ëŠ¥

> "MLLMs know where to look. Letâ€™s help them see better."