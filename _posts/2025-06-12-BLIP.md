---
layout: post
title: "📝Understanding BLIP - BLIP 알아보기?!!"
author: [DrFirst]
date: 2025-06-12 07:00:00 +0900
categories: [AI, Research]
tags: [BLIP, VLM, Bootstrapping, 부트스트래핑, CML, ICML 2022]
sitemap :
  changefreq : monthly
  priority : 0.8
---


## 🧠 (한국어) BLIP 알아보기!  
_🔍 깔끔하지 않은 웹데이터도로 혼자서도 잘 학습했고, 그림을 보고 해석할 줄 알아요!!!_

![manhwa](https://github.com/user-attachments/assets/0c3b94c9-575c-44c6-bfbc-c257acbdb2ba)

> 논문: [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/pdf/2201.12086)  
> 발표: ICML 2022 (Salesforce Research)  
> 코드: [salesforce/BLIP](https://github.com/salesforce/BLIP)  

---

### 💡 BLIP 요약!! 

> 이름이 비슷한, OpenAI의 [CLIP](https://drfirstlee.github.io/posts/CLIP/)과의 차이점은!!,  
> **텍스트 생성까지 포함된 대화형 멀티모달 처리**도 가능하다는 점!!  


1. 🧠 양방향 멀티모달 학습  
- 기존 모델들은 이해 or 생성 중 하나에 특화  
- **BLIP**는 두 방향 모두에 유연하게 전이 가능!

2. 🧹 웹 데이터 부트스트래핑  
> 앞으로 이 부트스트래핑은 점진적 개선 이라는 의미로 받아드려주세요!  
- 웹에서 수집한 noisy image-text 쌍 개선을 위해  
  - **Captioner**: 문장 생성 (synthetic caption)  
  - **Filter**: 노이즈 제거  
- 👉 더 **신뢰도 높은 학습 데이터** 구성

3. 🚀 제로샷 전이 가능  
- 비디오-언어 작업에도 **사전학습만으로 직접 적용 가능**  
- 강력한 **범용성**과 **일반화 능력** 보유


---

### 🧠 BLIP 등장의 배경

---

#### ❌ 기존 모델의 이분화된 성능

- 대부분의 VLP(Vision-Language Pretraining) 모델들은  
  - **이해 중심 작업** (예: 이미지-텍스트 검색)  
  - 또는 **생성 중심 작업** (예: 이미지 캡션 생성)  
  중 **한쪽에만 특화**되어 있음.
- Encoder-only (예: CLIP) 또는 Encoder-Decoder 모델 구조는  
  두 작업을 동시에 잘 수행하지 못하는 **구조적 한계**가 있음.

---

#### 🌐 웹 기반 데이터의 품질 문제

- 기존 VLP는 대부분 **웹에서 수집된 noisy한 이미지-텍스트 쌍**으로 학습됨.
- 단순한 규칙 기반 필터링만으로는 **노이즈 제거가 불완전**함.
- 데이터 규모를 키우면 성능은 증가하지만,  
  **텍스트 품질 저하가 세밀한 학습에 장애가 됨**.


#### 📘 Data Augmentation 에서의 한계  

- **기존 방식**:
  - 컴퓨터 비전에서는 이미지 회전, 크롭 등의 데이터 증강이 일반적이지만,
  - **자연어 처리에서는 증강이 까다로움**.
- 기존 VLP에서는 **언어 기반 증강이 거의 적용되지 않거나**,  
  **저품질 웹 텍스트에 의존**

  > BLIP은 LLM으로 캡션을 만들어, 노이즈가 아닌 진짜 의미있는 데이터를 얻어냄!!  

---


### 🚀 BLIP 모델 구조와 학습!   

---

#### 🖇️ BLIP 모델 구조   

> BLIP은 이미지 텍스트를 각각 이해하는거랑, 함꼐 이해하는거랑, 이미지를 해석하는것 모두 하기위해 3개의 모듈이 존재!! 
> 이를 "MED" 라고 부른다!!

![structure](https://github.com/user-attachments/assets/b4e3d92b-db5c-4379-971e-c2a830fcf960)

##### ✅ BLIP 모델별 학습 목적 요약

| 항목           | 🟦 ITC (Image-Text Contrastive)       | 🟩 ITM (Image-Text Matching)               | 🟨 LM (Language Modeling)                    |
|----------------|----------------------------------------|--------------------------------------------|----------------------------------------------|
| 🌐 목적        | 표현 공간 정렬 (similarity)            | 정답/오답 판별 (match 여부)                | 이미지 기반 텍스트 생성 (텍스트 생성 능력)  |
| 🧠 인코딩 방식 | 이미지와 텍스트를 각각 따로 인코딩    | 이미지+텍스트 쌍을 cross-attention으로 주입     | 이미지 + 디코더에서 텍스트 생성             |
| ⚙️ 사용 모드   | Unimodal Encoder                       | Image-Grounded Text Encoder                | Image-Grounded Text Decoder                  |
| 🔍 학습 방식   | Contrastive Loss (양성: 가까이 / 음성: 멀리) | Binary Classification (양성/음성 분류)   | Cross-Entropy Loss + 오토리그레시브 생성    |
| 🎯 특징        | 표현 공간 구조화 → 검색에 유리         | 멀티모달 의미 정렬 → fine-grained 대응 가능 | 자연스러운 문장 생성 능력, 캡션/질문응답 강화 |


---

##### 🧩 1. Unimodal Encoder  
> 이미지와 텍스트를 각각 독립적으로 해석  
> 위의 이미지에서 ITC(image-text contrastive)에 해당  

- 이미지 인코더: ViT 기반 (Patch + [CLS])
- 텍스트 인코더: BERT 구조, 입력에 [CLS] 토큰 포함
- 사용 용도: 이미지-텍스트 매칭, retrieval 등의 **이해 중심 태스크**

---

##### 🧩 2. Image-Grounded Text Encoder  
> 텍스트 인코더 내부에 **Cross-Attention**을 삽입하여 이미지 정보를 주입  
> 위의 이미지에서 ITM(a image-text matching)에 해당  

- 구조: 각 Transformer block에 Self-Attention → Cross-Attention → FFN 순서
- [Encode] 토큰을 텍스트 입력 끝에 추가 → 이 토큰의 출력이 **멀티모달 표현**
- 사용 용도: **세밀한 이미지-텍스트 의미 정렬**이 필요한 경우

---

##### 🧩 3. Image-Grounded Text Decoder  
> 순차적 텍스트 생성을 위한 **Causal Self-Attention** 구조  
> 위의 이미지에서 LM(language modeling)에 해당  

- 구조: 양방향이 아닌 **왼쪽에서 오른쪽으로만 흐르는 Attention**
- [Decode] 토큰으로 시작, [EOS]로 종료
- 사용 용도: **이미지 캡셔닝, 질문 응답 등 생성 기반 태스크**

---


##### 🔄 모델 내의 파라미터 공유 여부  

> SA는 인코더와 디코더의 “눈의 방향”이 달라서 따로 필요하지만,  
> Embedding·CA·FFN은 “머리 속 계산기”는 같기 때문에 공유한다!!  

| 레이어 종류              | 공유 여부 | 설명                                      |
|--------------------------|-----------|-------------------------------------------|
| Self-Attention (SA)      | ❌ 미공유   | 인코더는 **양방향**, 디코더는 **인과적**이기 때문 |
| Embedding Layer          | ✅ 공유     | 단어를 벡터로 변환하는 부분               |
| Cross-Attention (CA)     | ✅ 공유     | 이미지-텍스트 연결 부분                   |
| Feed Forward Network(FFN)| ✅ 공유     | 인코딩/디코딩 후 처리하는 계산 모듈        |

- 공유되는 부분 덕분에 파라미터도 절감되고!(경량화), 학습 효율 및 비용도 감소합니다!!  

> 아래 이미지를 통해 왜 이렇게 공유하게 되었는지 테스트한 결과도 보여줍니다!  
> SA 만 빼고 공유할때가 파라미터는 작으면서도 결과가 좋지요!?  
![sharing](https://github.com/user-attachments/assets/5965fb11-a2d4-4605-8579-e475b3cc93eb)

---

#### 🧬 CapFilt : 웹 데이터 부트스트래핑을 위한, 잘 해석달기 필터!

![CapFilt](https://github.com/user-attachments/assets/2f25c1cb-c29a-4c44-a99d-9bdb10cc6942)

- 사전 재료 : 인간이 annotate 한 이미지&텍스트 `(Iₕ, Tₕ)`
- 사전 학습모델 : 사전재료를 바탕으로 `MED` 모델 학습 시킴!!
- 사용할 재료 : 웹에서 수집된 이미지&텍스트 쌍 `(I𝑤, T𝑤)`

이제 본격 시작해서!!

1. 필터를 학습  
- 인간 주석 쌍 `(Iₕ, Tₕ)` 로 Image-grounded Text Encoder를 ITC/ITM loss로 학습  
- 웹 텍스트 쌍 `(I𝑤, T𝑤)`에 대해 노이즈를 제거하여 정제된 쌍 `(I𝑤, T𝑤′)` 생성  

2. 그림설명모델(Captioner) 학습  
- 동일한 인간 주석 쌍 `(Iₕ, Tₕ)` 로 Image-grounded Text Decoder를 LM loss로 학습  
- 웹 이미지 `I𝑤`에 대해 합성 캡션 `Tₛ` 생성  

3. Captioner 결과 재 필터링
- 생성된 `(I𝑤, Tₛ)` 을 다시 필터에 통과시켜 ITM 기반으로 일치하지 않는 것 제거  
- 최종적으로 정제된 합성 쌍 `(I𝑤, Tₛ′)` 획득

> 이때 아래 이미지처럼 잘못된 웹 텍스트(붉은색)은 제거되고, 새로운 캡션(녹색)이 생성됩니다!
![filter](https://github.com/user-attachments/assets/ef7c36bc-93d8-487a-a994-8e1a0297a9e6)


그래서 최종으로 추출된 데이터셋은은!?

A. 인간 주석 쌍	`(Iₕ, Tₕ)`
B. 필터링된 웹 텍스트 쌍 `(I𝑤,T𝑤′)` 
C. 신규생성&필터링된 쌍 `(I𝑤, Tₛ′)`

이 A/B/C 데이터를 가지고 `MED`모델을 다시 학습하게 됩나다!  
이 과정을 반복하면서 데이터도 점점 더 정확해지겠고(Bootstrapping),  
MED 모델도 고도화되는 선순환이 되겠지요~?  

> 그래서 아래 이미지 처럼, captioner, Filter 가 추가될수록 정확도가 올라갑니다!!  
![captfilt_res](https://github.com/user-attachments/assets/a8b55b30-1257-4165-a8c5-452999daf7d5)

```text
📦 초기 사전학습 (Initial MED Pretraining)
├── 인간 주석 쌍: (Iₕ, Tₕ)
├── 웹 수집 쌍: (I𝑤, T𝑤)
└── ➤ MED 모델 초기 학습 (손실: ITC + ITM + LM)

🔧 CapFilt: 데이터 정제 및 구성 요소 고도화
├── Filter 학습
│   ├── 구조: Image-grounded Text Encoder
│   ├── 데이터: (Iₕ, Tₕ)
│   └── 학습 목적: ITC / ITM loss
│       └── 웹 텍스트 필터링 → (I𝑤, T𝑤′)
│
├── Captioner 학습
│   ├── 구조: Image-grounded Text Decoder
│   ├── 데이터: (Iₕ, Tₕ)
│   └── 학습 목적: LM loss
│       └── 웹 이미지 캡션 생성 → (I𝑤, Tₛ)
│                            └→ 필터 재검증 → (I𝑤, Tₛ′)

🧠 최종 재사전학습 (Re-Pretraining of MED)
├── 인간 주석 쌍: (Iₕ, Tₕ)
├── 필터링된 웹 텍스트: (I𝑤, T𝑤′)
├── 필터링된 합성 캡션: (I𝑤, Tₛ′)
└── ➤ 이 정제 데이터셋 D′로 MED 재사전학습

```


### 🚀 BLIP 결과 분석!!!   

![analy](https://github.com/user-attachments/assets/91751520-2a92-443f-bc29-376990f5060d)

BLIP은 다른 모델에 비해 Retreival, Captioning 등에서 모두 최고였다!!  


---

## 🧠 마무리 생각

지금은 이미지를 보고 답변하는 LLM, 이미지를 생성하는 MLM 등이 당연하지만,  
2022년 시절을 돌아보면 이 BLIP는 "이미지를 보고 말할 수 있는 AI"로의 진입점을 열어준 중요한 모델인것 같습니다!!  
단순한 이미지 분류나 임베딩을 넘어서, **자연어를 통해 시각적 의미를 생성하고 소통할 수 있는 능력**을 갖추기 시작한 시조세 모델!!  

또한 스스로 데이터를 만들고 필터링하며 학습해서 고도화하는 부트스트래핑 기법은  
이후 DINO와 같은 Self supervised 느낌으로 이후 다른 연구에도 영향을 미쳤을것 같습니다!!  

지금의 멀티모달 AI를 이해하기 위해,  
그 시절 중요 모델을 공부해보았습니다!:) 


---
