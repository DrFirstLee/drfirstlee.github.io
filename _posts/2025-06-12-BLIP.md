---
layout: post
title: "📝Understanding BLIP - BLIP 알아보기?!!"
author: [DrFirst]
date: 2025-06-12 07:00:00 +0900
categories: [AI, Research]
tags: [BLIP, ICML, ICML 2022]
sitemap :
  changefreq : monthly
  priority : 0.8
---


## 🧠 (한국어) BLIP 알아보기!  
_🔍 깔끔하지 않은 웹데이터도로 혼자서도 잘 학습했고, 그림을 보고 해석할 줄 알아요!!!_

![Image]()

> 논문: [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/pdf/2201.12086)  
> 발표: ICML 2022 (Salesforce Research)  
> 코드: [salesforce/BLIP](https://github.com/salesforce/BLIP)  

---

### 💡 BLIP 요약!! 

> 이름이 비슷한, OpenAI의 [CLIP](https://drfirstlee.github.io/posts/CLIP/)과의 차이점은!!, 
> **텍스트 생성까지 포함된 대화형 멀티모달 처리**도 가능하다는 점!!  


1. 🧠 양방향 멀티모달 학습  
- 기존 모델들은 이해 or 생성 중 하나에 특화  
- **BLIP**는 두 방향 모두에 유연하게 전이 가능!

2. 🧹 웹 데이터 부트스트래핑(앞으로 이 부트스트래핑은 점진적 개선 이라는 의미로 받아드려주세요!)  
- 웹에서 수집한 noisy image-text 쌍 개선을 위해  
  - **Captioner**: 문장 생성 (synthetic caption)  
  - **Filter**: 노이즈 제거  
- 👉 더 **신뢰도 높은 학습 데이터** 구성

3. 🥇 다양한 벤치마크에서 굳!!  
- 이미지-텍스트 검색: **Recall@1 +2.7%**  
- 이미지 캡셔닝: **CIDEr +2.8**  
- VQA: **+1.6점** 향상

4. 🚀 제로샷 전이 가능  
- 비디오-언어 작업에도 **사전학습만으로 직접 적용 가능**  
- 강력한 **범용성**과 **일반화 능력** 보유

---

### 5. 📦 오픈소스 공개  
- [코드 및 모델 공개 GitHub 링크](https://github.com/salesforce/BLIP)  
- 연구 및 실무 적용 모두에 활용 가능!


---

### 🧠 BLIP 등장의 배경

---

#### ❌ 기존 모델의 이분화된 성능

- 대부분의 VLP(Vision-Language Pretraining) 모델들은  
  - **이해 중심 작업** (예: 이미지-텍스트 검색)  
  - 또는 **생성 중심 작업** (예: 이미지 캡션 생성)  
  중 **한쪽에만 특화**되어 있음.
- Encoder-only (예: CLIP) 또는 Encoder-Decoder 모델 구조는  
  두 작업을 동시에 잘 수행하지 못하는 **구조적 한계**가 있음.

---

#### 🌐 웹 기반 데이터의 품질 문제

- 기존 VLP는 대부분 **웹에서 수집된 noisy한 이미지-텍스트 쌍**으로 학습됨.
- 단순한 규칙 기반 필터링만으로는 **노이즈 제거가 불완전**함.
- 데이터 규모를 키우면 성능은 증가하지만,  
  **텍스트 품질 저하가 세밀한 학습에 장애가 됨**.


#### 📘 Data Augmentation 에서의 한계  

- **기존 방식**:
  - 컴퓨터 비전에서는 이미지 회전, 크롭 등의 데이터 증강이 일반적이지만,
  - **자연어 처리에서는 증강이 까다로움**.
- 기존 VLP에서는 **언어 기반 증강이 거의 적용되지 않거나**,  
  **저품질 웹 텍스트에 의존**

  > BLIP은 LLM으로 캡션을 만들어, 노이즈가 아닌 진짜 의미있는 데이터를 얻어냄!!  

---


### 🚀 BLIP 모델 구조와 학습!   

---

#### 🖇️ BLIP 모델 구조   

> BLIP은 이미지 텍스트를 각각 이해하는거랑, 함꼐 이해하는거랑, 이미지를 해석하는것 모두 하기위해 3개의 모듈이 존재!! 
> 이를 "MED" 라고 부른다!!

![structure]()

##### ✅ BLIP 모델별 학습 목적 요약

| 항목           | 🟦 ITC (Image-Text Contrastive)       | 🟩 ITM (Image-Text Matching)               | 🟨 LM (Language Modeling)                    |
|----------------|----------------------------------------|--------------------------------------------|----------------------------------------------|
| 🌐 목적        | 표현 공간 정렬 (similarity)            | 정답/오답 판별 (match 여부)                | 이미지 기반 텍스트 생성 (텍스트 생성 능력)  |
| 🧠 인코딩 방식 | 이미지와 텍스트를 각각 따로 인코딩    | 이미지+텍스트 쌍을 cross-attention으로 주입     | 이미지 + 디코더에서 텍스트 생성             |
| ⚙️ 사용 모드   | Unimodal Encoder                       | Image-Grounded Text Encoder                | Image-Grounded Text Decoder                  |
| 🔍 학습 방식   | Contrastive Loss (양성: 가까이 / 음성: 멀리) | Binary Classification (양성/음성 분류)   | Cross-Entropy Loss + 오토리그레시브 생성    |
| 🎯 특징        | 표현 공간 구조화 → 검색에 유리         | 멀티모달 의미 정렬 → fine-grained 대응 가능 | 자연스러운 문장 생성 능력, 캡션/질문응답 강화 |



---

##### 🧩 1. Unimodal Encoder  
> 이미지와 텍스트를 각각 독립적으로 해석  
> 위의 이미지에서 ITC(image-text contrastive)에 해당  

- 이미지 인코더: ViT 기반 (Patch + [CLS])
- 텍스트 인코더: BERT 구조, 입력에 [CLS] 토큰 포함
- 사용 용도: 이미지-텍스트 매칭, retrieval 등의 **이해 중심 태스크**

---

##### 🧩 2. Image-Grounded Text Encoder  
> 텍스트 인코더 내부에 **Cross-Attention**을 삽입하여 이미지 정보를 주입  
> 위의 이미지에서 ITM(a image-text matching)에 해당  

- 구조: 각 Transformer block에 Self-Attention → Cross-Attention → FFN 순서
- [Encode] 토큰을 텍스트 입력 끝에 추가 → 이 토큰의 출력이 **멀티모달 표현**
- 사용 용도: **세밀한 이미지-텍스트 의미 정렬**이 필요한 경우

---

##### 🧩 3. Image-Grounded Text Decoder  
> 순차적 텍스트 생성을 위한 **Causal Self-Attention** 구조  
> 위의 이미지에서 LM(language modeling)에 해당  

- 구조: 양방향이 아닌 **왼쪽에서 오른쪽으로만 흐르는 Attention**
- [Decode] 토큰으로 시작, [EOS]로 종료
- 사용 용도: **이미지 캡셔닝, 질문 응답 등 생성 기반 태스크**

---


##### 🔄 모델 내의 파라미터 공유 여부  

> SA는 인코더와 디코더의 “눈의 방향”이 달라서 따로 필요하지만,  
> Embedding·CA·FFN은 “머리 속 계산기”는 같기 때문에 공유한다!!  

| 레이어 종류              | 공유 여부 | 설명                                      |
|--------------------------|-----------|-------------------------------------------|
| Self-Attention (SA)      | ❌ 미공유   | 인코더는 **양방향**, 디코더는 **인과적**이기 때문 |
| Embedding Layer          | ✅ 공유     | 단어를 벡터로 변환하는 부분               |
| Cross-Attention (CA)     | ✅ 공유     | 이미지-텍스트 연결 부분                   |
| Feed Forward Network(FFN)| ✅ 공유     | 인코딩/디코딩 후 처리하는 계산 모듈        |

- 공유되는 부분 덕분에 파라미터도 절감되고!(경량화), 학습 효율 및 비용도 감소합니다!!  

---

### 🚀 BLIP 결과 분석!!!   

## 🎯 주요 목표

- 이미지와 텍스트의 표현을 **공통 임베딩 공간에 정렬**
- **이미지 → 텍스트 생성 (Captioning)** 능력까지 내장
- **라벨이 없는 웹 이미지+텍스트** 데이터를 활용해 효율적으로 학습

---

## 🔧 핵심 기법 요약

| 구성 요소 | 설명 |
|----------|------|
| **Dual Encoder + Decoder** | 이미지와 텍스트를 개별 인코더로 인코딩한 뒤, 텍스트 디코더가 캡션을 생성 |
| **세 가지 학습 모드** | <ul><li>ITC (Image-Text Contrastive)</li><li>ITM (Image-Text Matching)</li><li>LM (Language Modeling)</li></ul> |
| **자체 데이터 정제 (Bootstrapping)** | noisy한 웹 텍스트를 필터링하고, 양질의 캡션을 생성하여 다시 학습에 사용 |

---

## 🧱 BLIP의 학습 구조

```plaintext
Image → Vision Encoder → Query Tokens → Text Decoder → Caption 생성
                 ↑                           ↑
         Image-Text Matching        Language Modeling
```

- **Q-Former (BLIP-2부터 도입)**: Query 기반 Transformer 구조로 이미지에서 필요한 정보만 추출  
- **Decoder**는 이미지 특성과 함께 다음 단어를 예측하는 **언어 생성 기능** 수행

---

## 🚀 BLIP의 강점

- 🔥 **이미지-텍스트 정렬 + 생성** 능력을 모두 보유  
- 📊 다양한 멀티모달 벤치마크에서 강력한 성능 (COCO, VQAv2 등)  
- 💬 이후 LLaVA, MiniGPT4 등 **비전 챗봇 모델의 기반 구조로 채택됨**

---

## 📌 대표 응용 작업

| 작업 | 설명 |
|------|------|
| 이미지 캡셔닝 (Captioning) | 이미지 → 설명 텍스트 생성 |
| 이미지-텍스트 검색 (Retrieval) | 텍스트에 맞는 이미지 또는 이미지에 맞는 텍스트 검색 |
| VQA (시각 질문 응답) | 이미지와 질문을 보고 정답 텍스트 생성 |

---

## 🔁 후속 모델: BLIP-2

BLIP-2에서는 기존 구조를 확장하여  
- **LLM (예: FLAN-T5, OPT)** 와 연결  
- **Query Transformer (Q-Former)** 도입  
- 더욱 유연한 멀티모달 대화형 모델로 진화

> 현재 ChatGPT와 비슷한 **이미지 이해 + 대화 모델(LLaVA 등)** 대부분이 BLIP-2 기반 구조를 채택합니다.

---

## 🧠 마무리 생각

BLIP는 "이미지를 보고 말할 수 있는 AI"로의 진입점을 열어준 중요한 모델입니다.  
단순한 이미지 분류나 임베딩을 넘어서, **자연어를 통해 시각적 의미를 생성하고 소통할 수 있는 능력**을 갖추기 시작한 모델로 의미가 큽니다.

향후 멀티모달 AI를 연구하거나, 이미지 기반 대화 시스템을 만들고 싶다면 꼭 한 번 짚고 넘어가야 할 모델입니다.

---
