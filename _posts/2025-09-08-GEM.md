---
layout: post
title: "ğŸ“ GEM: Grounding Everything in Vision-Language Transformers"
author: [DrFirst]
date: 2025-09-08 7:00:00 +0900
categories: [AI, Research]
tags: [Training-Free, Grounding, VLM, Attention, Zero-Shot, CVPR 2024, Segmentation, CVPR]
sitemap:
  changefreq: monthly
  priority: 0.8
---
---

### ğŸ“ GEM: Unlocking the Latent Localization Ability of VLMs!  

![Image](https://github.com/user-attachments/assets/8ccda093-0186-4338-a0db-33d30056f279)

* **Title**: [Grounding Everything: Emerging Localization Properties in Vision-Language Transformers](https://arxiv.org/pdf/2312.00878)  
* **Conference**: CVPR 2024  
* **Code/Checkpoints**: [GitHub â€“ GEM](https://github.com/WalBouss/GEM)  
* **Keywords**: `Training-Free`, `Grounding`, `Vision-Language Transformer`, `Self-Self Attention`, `Zero-Shot`  
* **Summary**: Feels like an extended version of [CLIP Surgery](https://drfirstlee.github.io/posts/ClipSurgery/)! Proposes **GEM**, a framework that leverages the **inherent attention structure** of pretrained Vision-Language Transformers (VLMs) to perform **object localization and segmentation in a training-free manner**!  

---

### ğŸš€ GEM Key Summary

> One-liner: **[CLIP Surgery](https://drfirstlee.github.io/posts/ClipSurgery/)** + (1) Attention Expansion + (2) Regularization  

1) **Self-Self Attention Expansion**  
- CLIP Surgery only used valueâ€“value (vâ€“v) attention  
- GEM extends this to include **queryâ€“query (qâ€“q), keyâ€“key (kâ€“k)** â†’ utilizes full selfâ€“self attention  

2) **Regularization**  
- CLIP Surgery had no normalization concept  
- GEM introduces three components for more stable and generalized localization:  
  i) Adaptive temperature: adaptively adjusts softmax temperature for each dataset/model  
  ii) L2 normalization: removes influence from token magnitude differences  
  iii) Iterative selfâ€“self attention: repeats clustering multiple times for reinforcement  

3) **Training-Free Grounding with Zero-Shot Localization & Segmentation**  
- Directly extracts localization ability from pretrained VLMs  
- Achieves performance comparable to fine-tuned detectors  
- Open-vocabulary grounding without additional training  

---

### ğŸ” Flow of Existing Research  

![Image](https://github.com/user-attachments/assets/3017d668-78f7-46e8-beff-8a47115657ba)

#### 1. Localization-first approaches
- **Idea**: first detect regions or masks, then label them using VL models
- Examples:
  - **OpenSeg**: fine-tuned with class-agnostic masks + image-text pairs
  - **OVSeg**: segmentation model + CLIP for mask classification
  - **MaskCLIP(3)**: mask proposal network + CLIP encoder
  - **GroundingSAM**: GroundingDINO (detector) + SAM (masking)

#### 2. Modifying VL model architecture/training
- **Idea**: alter ViT to encourage localization properties
- Examples:
  - **SegCLIP, GroupViT**: insert grouping blocks
  - **ViL-Seg, OVSegmentor**: clustering / Slot Attention
  - **ReCo**: retrieval-based fine supervision
  - **PACL**: add a decoder with grounding loss on top of CLIP

#### 3. Training-free adaptation
- **Idea**: adapt pretrained VL models for localization without training
- Examples:
  - **MaskCLIP**: remove final MLP, use value projection
  - **[CLIP Surgery](https://drfirstlee.github.io/posts/ClipSurgery/)**: add surgery pathway to ViT backbone (vâ€“v attention with residual)

â¡ï¸ GEMâ€™s core concept: extending the training-free CLIP Surgery approach!  

---

### ğŸ§± GEM Architecture
> Easier to understand through code than images!  

```python
class SelfSelfAttention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., ss_attn_iter=1,
                 ss_attn_temp=None):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5
        self.ss_attn_iter = ss_attn_iter
        self.ss_attn_temp = ss_attn_temp

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x, attn_bias=None, prev_attn=None):
        x = x.transpose(0, 1)
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        self.v_values = v
        # original self-attention for the original path
        attn_ori_return = (q @ k.transpose(-2, -1)) * self.scale
        attn_ori = attn_ori_return.softmax(dim=-1)
        attn_ori = self.attn_drop(attn_ori)

        x_ori = (attn_ori @ v).transpose(1, 2).reshape(B, N, C)
        x_ori = self.proj_drop(self.proj(x_ori))

        # GEM
        xs1 = v
        xs2 = k
        xs3 = q

        #  >>> i) Adaptive temperature: `inv_temp`
        if self.ss_attn_temp is None:
            pre_norm = torch.norm(x, dim=-1).mean(dim=-1, keepdim=True).unsqueeze(1).unsqueeze(-1)
            inv_temp = pre_norm * self.scale
        else:
            inv_temp = self.ss_attn_temp

        # >>> iii) Iterative selfâ€“self attention
        for it in range(self.ss_attn_iter):

          #   >>> ii) L2 normalization
            xs1 = F.normalize(xs1, dim=-1)
            xs2 = F.normalize(xs2, dim=-1)
            xs3 = F.normalize(xs3, dim=-1)

            attn_return1 = (xs1 @ xs1.transpose(-2, -1)) * inv_temp
            attn_return2 = (xs2 @ xs2.transpose(-2, -1)) * inv_temp
            attn_return3 = (xs3 @ xs3.transpose(-2, -1)) * inv_temp

            attn1 = (attn_return1).softmax(dim=-1)
            attn2 = (attn_return2).softmax(dim=-1)
            attn3 = (attn_return3).softmax(dim=-1)

            xs1 = attn1 @ xs1
            xs2 = attn2 @ xs2
            xs3 = attn3 @ xs3

        # Assignment to V
        xs1 = F.normalize(xs1, dim=-1)
        xs2 = F.normalize(xs2, dim=-1)
        xs3 = F.normalize(xs3, dim=-1)

        attn_return1 = (xs1 @ xs1.transpose(-2, -1)) * inv_temp
        attn_return2 = (xs2 @ xs2.transpose(-2, -1)) * inv_temp
        attn_return3 = (xs3 @ xs3.transpose(-2, -1)) * inv_temp

        attn1 = (attn_return1).softmax(dim=-1)
        attn2 = (attn_return2).softmax(dim=-1)
        attn3 = (attn_return3).softmax(dim=-1)

        xs1 = attn1 @ v
        xs2 = attn2 @ v
        xs3 = attn3 @ v

        ## >>> iiii : qkv ensemble!!!
        xs = (xs1 + xs2 + xs3) / 3

        x = xs.transpose(1, 2).reshape(B, N, C)
        x = self.proj_drop(self.proj(x))

        return [x.transpose(0, 1), x_ori.transpose(0, 1)]
```

1) **Self-Self Attention Expansion**  
- As seen in the code:  
- `xs1` = v-v, `xs2` = k-k, `xs3` = q-q  

2) **Regularization**  
  i) Adaptive temperature  
  ii) L2 normalization  
  iii) Iterative selfâ€“self attention  
  iiii) qkv-Ensemble (averaging all results)  

---

### ğŸ§ª Experiments & Results  

#### ğŸ¯ Segmentation & Localization Benchmarks  

![Image](https://github.com/user-attachments/assets/95a88976-ffa1-46ce-aa20-f7a25830fef9)

- On complex datasets like **PascalContext, ADE20K**:  
  - Significantly better than previous training-free methods  
  - Comparable or superior to fine-tuned approaches  

#### ğŸ¯ Zero-Shot Point Prediction (OpenImages V7)  

![Image](https://github.com/user-attachments/assets/e39321ae-e4bc-403c-8e76-fee5491c3dc7)

- **First training-free SOTA**  
- Demonstrates localization without LLM/VLM hybrid models  
- Downside: inference FPS is quite slow  

---

#### ğŸ‘€ Qualitative Results  

> Comparison with other models!  
![Image](https://github.com/user-attachments/assets/88407d76-f48f-4724-a19c-38715ab922d3)

1. Methods trained with localization (GroundingSAM, OVSeg)  
- Strength: high-quality masks if object correctly identified (e.g., Cat, Squirrel, Jet Ski)  
- Weakness: fail to detect entities not in datasets (e.g., Boxer, Violin)  
- Cause: reliance on handcrafted segmentation annotation â†’ limited scope  

2. Segmentation-specialized training methods (GroupViT, SegCLIP)  
- Strength: accurate for common objects (e.g., Cat, Squirrel, Lizard)  
- Weakness: fail on rare objects (e.g., Jet Ski, Logo, Flag)  
- Cause: limited curated vocab â†’ reduced diversity  

3. Training-free methods (MaskCLIP, CLIPSurgery, GEM)  
- Strength: leverage millions of image-text pairs from VLM pretraining â†’ recognize diverse entities  
- Weakness: masks less sharp than GroundingSAM  
- GEMâ€™s extra achievement:  
  - Sharper segmentation compared to other training-free approaches (clearer contours, fewer holes)  
  - Detects objects missed by MaskCLIP & CLIPSurgery (e.g., Logo)  

> Applicable not only to CLIP but to other VLMs as well!  
![Image](https://github.com/user-attachments/assets/fe12290a-558c-4996-a420-d958ae68294a)

> Failure cases show strong dependence on text prompts!  
![Image](https://github.com/user-attachments/assets/94f4244b-7656-40be-a2a8-79d5f5a3921d)

---

#### ğŸ§ª Ablation Analysis  

- Comparison with CLIP Surgery: adding k-k, q-q, normalization, etc. leads to improvements  
![Image](https://github.com/user-attachments/assets/e3570f3c-f5f9-4a63-8de8-d44b41edfe9d)  

- Effect of normalization: clear benefit, requires proper 1/T value  
![Image](https://github.com/user-attachments/assets/4da69524-9511-4850-b3cd-778d88cfb9f5)

- Effect of iterations:  
  - More iterations â†’ beneficial for datasets with fewer classes (VOC)  
  - Fewer iterations â†’ better for complex datasets with many classes (Context)  

![Image](https://github.com/user-attachments/assets/c90ba4c2-abf8-464c-becb-a9a794b104a9)  

---

### âœ… Conclusion  

- **GEM** reveals the **latent localization ability of Vision-Language Transformers**  
- Potential to replace fine-tuned detectors when combined with larger VLMs  
- Introduces a new paradigm for open-world recognition, segmentation, and grounding!  



---
### ğŸ“ (í•œêµ­ì–´) GEM: VLMì´ ê°€ì§„ ì ì¬ì  Localization ëŠ¥ë ¥ì„ ëŒì–´ë‚´ë‹¤!  

![Image](https://github.com/user-attachments/assets/8ccda093-0186-4338-a0db-33d30056f279)

* **ì œëª©**: [Grounding Everything: Emerging Localization Properties in Vision-Language Transformers](https://arxiv.org/pdf/2312.00878)  
* **í•™íšŒ**: CVPR 2024  
* **ì½”ë“œ/ì²´í¬í¬ì¸íŠ¸**: [GitHub â€“ GEM](https://github.com/WalBouss/GEM)  
* **í•µì‹¬ í‚¤ì›Œë“œ**: `Training-Free`, `Grounding`, `Vision-Language Transformer`, `Self-Self Attention`, `Zero-Shot`  
* **ìš”ì•½**: [CLIP Surgery](https://drfirstlee.github.io/posts/ClipSurgery/)ì˜ í™•ì¥íŒëŠë‚Œ!! ì‚¬ì „ í•™ìŠµëœ Vision-Language Transformer(VLM)ì˜ **ë‚´ì¬ëœ attention êµ¬ì¡°**ë¥¼ í™œìš©í•´, **ì¶”ê°€ í•™ìŠµ ì—†ì´(training-free)** ê°ì²´ ìœ„ì¹˜ ì¸ì‹ê³¼ ë¶„í• ê¹Œì§€ ìˆ˜í–‰í•˜ëŠ” í”„ë ˆì„ì›Œí¬ **GEM** ì œì•ˆ!  

---

### ğŸš€ GEM í•µì‹¬ ìš”ì•½

> í•œ ì¤„ ìš”ì•½: **[CLIP Surgery](https://drfirstlee.github.io/posts/ClipSurgery/)**ì— 1. Attention í™•ì¥ ë° 2. Regularization ë„ì…   

1) **Self-Self Attention í™•ì¥**  
- CLIP Surgery ì—ì„œëŠ” ì˜¤ì§ valueâ€“value (vâ€“v) attentionë§Œ ì‚¬ìš©  
- ê·¸ëŸ°ë° GEMì€!! vâ€“vë¿ë§Œ ì•„ë‹ˆë¼ **queryâ€“query (qâ€“q), keyâ€“key (kâ€“k)**ê¹Œì§€ í™•ì¥ â†’ selfâ€“self attention ì „ë°˜ í™œìš©  

2) **Regularization ë„ì…**  
- CLIP Surgeryì—ëŠ” ì •ê·œí™” ê°œë…ì´ ì—†ìŒ  
- GEMì€ ì•ˆì •ì ì´ê³  ì¼ë°˜í™”ëœ localizationì„ ìœ„í•´ ì„¸ê°€ì§€ ìš”ì†Œë¥¼ ë„ì…!!  
  i) Adaptive temperature: ë°ì´í„°ì…‹/ëª¨ë¸ë§ˆë‹¤ ì ì‘ì ìœ¼ë¡œ softmax ì˜¨ë„ ì¡°ì •  
  ii) L2 ì •ê·œí™”: í† í°ì˜ í¬ê¸°(norm) ì°¨ì´ë¡œ ìƒê¸°ëŠ” ì˜í–¥ ì œê±°  
  iii) Iterative selfâ€“self attention: í•„ìš” ì‹œ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§ ê°•í™”  

3) **Training-Free Grounding** ì´ë©´ì„œ **Zero-Shot Localization & Segmentation**  
- ì‚¬ì „ í•™ìŠµëœ VLMì—ì„œ ë°”ë¡œ localization ì„±ëŠ¥ ì¶”ì¶œ  
- fine-tuned detector ìˆ˜ì¤€ì— ë§ë¨¹ëŠ” ì„±ëŠ¥  
- ì¶”ê°€ í•™ìŠµ ì—†ì´ open-vocabulary grounding ë‹¬ì„±  


---

### ğŸ” ê¸°ì¡´ ì—°êµ¬ì˜ íë¦„  

![Image](https://github.com/user-attachments/assets/3017d668-78f7-46e8-beff-8a47115657ba)

#### 1. Localization-first ì ‘ê·¼
- **ì•„ì´ë””ì–´**: ë¨¼ì € ì˜ì—­(Region)ì´ë‚˜ ë§ˆìŠ¤í¬ë¥¼ ì°¾ì€ ë’¤ VL ëª¨ë¸ë¡œ ë¼ë²¨ë§
- ì˜ˆì‹œ:
  - **OpenSeg**: class-agnostic mask + image-text pairë¡œ íŒŒì¸íŠœë‹
  - **OVSeg**: segmentation model + CLIPìœ¼ë¡œ ë§ˆìŠ¤í¬ ë¶„ë¥˜
  - **MaskCLIP(3)**: ë§ˆìŠ¤í¬ ì œì•ˆ ë„¤íŠ¸ì›Œí¬ + CLIP ì¸ì½”ë”
  - **GroundingSAM**: GroundingDINO(ê²€ì¶œ) + SAM(ë§ˆìŠ¤í¬)

#### 2. VL ëª¨ë¸ êµ¬ì¡°/í•™ìŠµ ìˆ˜ì • ì ‘ê·¼
- **ì•„ì´ë””ì–´**: ViT êµ¬ì¡°ë¥¼ ë°”ê¾¸ì–´ localization íŠ¹ì„±ì„ ìœ ë„
- ì˜ˆì‹œ:
  - **SegCLIP, GroupViT**: grouping block ì‚½ì…
  - **ViL-Seg, OVSegmentor**: clustering/Slot Attention í™œìš©
  - **ReCo**: retrieval ê¸°ë°˜ ë¯¸ì„¸ ê°ë…
  - **PACL**: CLIP ìœ„ì— decoder + grounding loss

#### 3. Training-free ì ì‘ ì ‘ê·¼
- **ì•„ì´ë””ì–´**: í•™ìŠµ ì—†ì´ ê¸°ì¡´ VL ëª¨ë¸ì„ localizationì— ë§ê²Œ ë³€í˜•
- ì˜ˆì‹œ:
  - **MaskCLIP**: ë§ˆì§€ë§‰ MLP ì œê±°, value projection í™œìš©
  - **[CLIP Surgery](https://drfirstlee.github.io/posts/ClipSurgery/)**: ViT ë°±ë³¸ì— surgery pathway ì¶”ê°€ (valueâ€“value attention ì‚¬ìš©, residualë¡œ ëˆ„ì )


- ì´ ì¤‘ì—ì„œ Training-Freeì¸ 3ë²ˆì˜ CLIP Surgeryë¥¼ í™•ì¥í•˜ëŠ” ê²ƒì´ GEMì˜ ê¸°ë³¸ê°œë…!!


---

### ğŸ§± GEM êµ¬ì¡° (Architecture)
> ì—¬ê¸´ ì´ë¯¸ì§€ë³´ë‹¤ ì½”ë“œë¡œ ë³´ëŠ”ê°œ í¸í•¨!!!  

```python
class SelfSelfAttention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., ss_attn_iter=1,
                 ss_attn_temp=None):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5
        self.ss_attn_iter = ss_attn_iter
        self.ss_attn_temp = ss_attn_temp

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x, attn_bias=None, prev_attn=None):
        x = x.transpose(0, 1)
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        self.v_values = v
        # original self-attention for the original path
        attn_ori_return = (q @ k.transpose(-2, -1)) * self.scale
        attn_ori = attn_ori_return.softmax(dim=-1)
        attn_ori = self.attn_drop(attn_ori)

        x_ori = (attn_ori @ v).transpose(1, 2).reshape(B, N, C)
        x_ori = self.proj_drop(self.proj(x_ori))

        # GEM
        xs1 = v
        xs2 = k
        xs3 = q

        #  >>> i) Adaptive temperature: `inv_temp`
        if self.ss_attn_temp is None:
            pre_norm = torch.norm(x, dim=-1).mean(dim=-1, keepdim=True).unsqueeze(1).unsqueeze(-1)
            inv_temp = pre_norm * self.scale
        else:
            inv_temp = self.ss_attn_temp

        # >>> iii) Iterative selfâ€“self attention: ë°˜ë³µí•¨!!
        for it in range(self.ss_attn_iter):

          #   >>> ii) L2 ì •ê·œí™”: í† í°ì˜ í¬ê¸°(norm) ì°¨ì´ë¡œ ìƒê¸°ëŠ” ì˜í–¥ ì œê±°  
            xs1 = F.normalize(xs1, dim=-1)
            xs2 = F.normalize(xs2, dim=-1)
            xs3 = F.normalize(xs3, dim=-1)

            attn_return1 = (xs1 @ xs1.transpose(-2, -1)) * inv_temp
            attn_return2 = (xs2 @ xs2.transpose(-2, -1)) * inv_temp
            attn_return3 = (xs3 @ xs3.transpose(-2, -1)) * inv_temp

            attn1 = (attn_return1).softmax(dim=-1)
            attn2 = (attn_return2).softmax(dim=-1)
            attn3 = (attn_return3).softmax(dim=-1)

            xs1 = attn1 @ xs1
            xs2 = attn2 @ xs2
            xs3 = attn3 @ xs3

        # Assigment to V
        xs1 = F.normalize(xs1, dim=-1)
        xs2 = F.normalize(xs2, dim=-1)
        xs3 = F.normalize(xs3, dim=-1)

        attn_return1 = (xs1 @ xs1.transpose(-2, -1)) * inv_temp
        attn_return2 = (xs2 @ xs2.transpose(-2, -1)) * inv_temp
        attn_return3 = (xs3 @ xs3.transpose(-2, -1)) * inv_temp

        attn1 = (attn_return1).softmax(dim=-1)
        attn2 = (attn_return2).softmax(dim=-1)
        attn3 = (attn_return3).softmax(dim=-1)

        xs1 = attn1 @ v
        xs2 = attn2 @ v
        xs3 = attn3 @ v

        ## >>> iiii : qkv ensemble!!!
        xs = (xs1 + xs2 + xs3) / 3

        x = xs.transpose(1, 2).reshape(B, N, C)
        x = self.proj_drop(self.proj(x))

        return [x.transpose(0, 1), x_ori.transpose(0, 1)]
```

1) **Self-Self Attention í™•ì¥**  
- ì•„ë˜ ì½”ë“œì—ì„œ ë³¼ìˆ˜ ìˆë“¯~!!  
- `xs1` = v-v, `xs2` = k-k , `xs3`= q-q 

2) **Regularization ë„ì…**  
  i) Adaptive temperature: ë°ì´í„°ì…‹/ëª¨ë¸ë§ˆë‹¤ ì ì‘ì ìœ¼ë¡œ softmax ì˜¨ë„ ì¡°ì •  
  ii) L2 ì •ê·œí™”: í† í°ì˜ í¬ê¸°(norm) ì°¨ì´ë¡œ ìƒê¸°ëŠ” ì˜í–¥ ì œê±°  
  iii) Iterative selfâ€“self attention: í•„ìš” ì‹œ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§ ê°•í™”  
  iiii) qkv-Ensemble: ë§ˆì§€ë§‰ì— ë‹¤ ë”í•´ì„œ í‰ê· ëƒ„!! 



### ğŸ§ª ì‹¤í—˜ ë° ê²°ê³¼ ë¶„ì„  

#### ğŸ¯ Segmentation & Localization Benchmarks  

![Image](https://github.com/user-attachments/assets/95a88976-ffa1-46ce-aa20-f7a25830fef9)

- **PascalContext, ADE20K** ë“± ë³µì¡í•œ ë ˆì´ë¸”ë§ ë°ì´í„°ì…‹ì—ì„œ  
  - ê¸°ì¡´ training-free ë°©ë²• ëŒ€ë¹„ ì›”ë“±í•œ ì„±ëŠ¥  
  - fine-tuned ë°©ë²•ì— ê·¼ì ‘í•˜ê±°ë‚˜ ëŠ¥ê°€  

#### ğŸ¯ Zero-Shot Point Prediction (OpenImages V7)  

![Image](https://github.com/user-attachments/assets/e39321ae-e4bc-403c-8e76-fee5491c3dc7)

- **ìµœì´ˆì˜ training-free SOTA ì„±ëŠ¥ ë‹¬ì„±**  
- LLM/VLM ì¡°í•© ì—†ì´ë„ localization ê°€ëŠ¥ì„± í™•ì¸  
- ë‹¤ë§Œ fpsëŠ” ë„ˆë¬´ ëŠë¦¬ë‹¤..  

---

#### ğŸ‘€ ê²°ê³¼ ì´ë¯¸ì§€ ë³´ê¸°    

> ë‹¤ë¥¸ëª¨ë¸ë“¤ê³¼ì˜ ê²°ê³¼ë¹„êµì´ì§€ë¯¸!!
![Image](https://github.com/user-attachments/assets/88407d76-f48f-4724-a19c-38715ab922d3)

1. Localization ì •ë³´ë¡œ í•™ìŠµí•œ ë°©ë²• (GroundingSAM, OVSeg)
- ì¥ì : ê°ì²´ë¥¼ ì •í™•íˆ ì¸ì‹í•˜ë©´ ê³ í’ˆì§ˆ ë§ˆìŠ¤í¬ ì¶œë ¥ (ì˜ˆ: Cat, Squirrel, Jet Ski)  
- í•œê³„: ë°ì´í„°ì…‹ì— ì˜ ì•ˆ ë‚˜ì˜¤ëŠ” ê°œì²´(Boxer, Violin) íƒì§€ ë¶ˆê°€  
- ì›ì¸: ìˆ˜ì‘ì—… segmentation annotation ì˜ì¡´ â†’ ë²”ìœ„ ì œí•œ  

2. Segmentation íŠ¹í™” í•™ìŠµ ë°©ë²• (GroupViT, SegCLIP)  
- ì¥ì : í”í•œ ê°ì²´ ì˜ ë¶„í•  (ì˜ˆ: Cat, Squirrel, Lizard)  
- í•œê³„: ë“œë¬¸ ê°œì²´(Jet Ski, Logo, Flag) ë¶„í•  ì‹¤íŒ¨  
- ì›ì¸: ì œí•œëœ vocab curationìœ¼ë¡œ í•™ìŠµ â†’ ì–´íœ˜ ë‹¤ì–‘ì„± ë¶€ì¡±  

3. Training-free ë°©ë²• (MaskCLIP, CLIPSurgery, GEM)  
- ì¥ì : VL ëª¨ë¸ì´ í•™ìŠµí•œ ìˆ˜ë°±ë§Œ ê°œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒ í™œìš© â†’ ë‹¤ì–‘í•œ ê°œì²´ ì¸ì‹ ê°€ëŠ¥  
- ë‹¨ì : ë§ˆìŠ¤í¬ ê²½ê³„ê°€ ë‚ ì¹´ë¡­ì§€ ëª»í•¨ (GroundingSAMë³´ë‹¤ ëœ ì •ë°€)  
- GEMì˜ ì¶”ê°€ ì„±ê³¼:  
  - ê¸°ì¡´ training-free ëŒ€ë¹„ ë” ì„ ëª…í•œ segmentation (ê²½ê³„ ëšœë ·, êµ¬ë© ì ìŒ)  
  - MaskCLIP, CLIPSurgeryê°€ ë†“ì¹œ ê°ì²´ê¹Œì§€ íƒì§€ (ì˜ˆ: Logo)  


> CLIP ë¿ë§Œ ì•„ë‹ˆë¼ ë‹¤ë¥¸ VLMë“¤ì—ë„ ì ìš©ì´ ê°€ëŠ¥í•˜ë‹¤!
![Image](https://github.com/user-attachments/assets/fe12290a-558c-4996-a420-d958ae68294a)

> ì‹¤íŒ¨ ì¼€ì´ìŠ¤ë¥¼ ë³¸ë‹¤ë©´ í…ìŠ¤íŠ¸ í”„ë¡¬í¬íŠ¸ì— ë”°ë¼ ì°¨ì´ê°€ ì»·ë‹¤!!
![Image](https://github.com/user-attachments/assets/94f4244b-7656-40be-a2a8-79d5f5a3921d)



---

#### ğŸ§ª Ablation ë¶„ì„  

- ClipSurgeryë‘ ë¹„êµ + k-kë‘ q-q ì¶”ê°€í•˜ê³  ì •ê·œí™” ë“±ì„ ì¶”ê°€í•˜ë©´ì„œ ì¢‹ì•„ì§„ë‹¤!  
![Image](https://github.com/user-attachments/assets/e3570f3c-f5f9-4a63-8de8-d44b41edfe9d)  

- ì •ê·œí™” íš¨ê³¼ëŠ”!? : ìˆë‹¤!! ì•Œë§ì€ 1/T ê°’ ê°€ í•„ìš”í•˜ë‹¤!    
![Image](https://github.com/user-attachments/assets/4da69524-9511-4850-b3cd-778d88cfb9f5)


- ë°˜ë³µì˜ íš¨ê³¼ëŠ”!? :  ë°˜ë³µ(iteration)ì„ ëŠ˜ë¦¬ë©´ í† í°ë“¤ì´ ë” í° í´ëŸ¬ìŠ¤í„°ë¡œ ë¬¶ì—¬ì„œ ë‹¨ìˆœí•œ ë°ì´í„°ì…‹(VOC)ì—” ìœ ë¦¬í•˜ì§€ë§Œ, ë‹¤ì–‘í•œ ê°ì²´ê°€ ë§ì€ ë°ì´í„°ì…‹(Context)ì—ì„œëŠ” ê³¼ë„í•œ ë³‘í•©ì´ ì˜¤íˆë ¤ ì„±ëŠ¥ì„ ë–¨ì–´ëœ¨ë¦°ë‹¤.

![Image](https://github.com/user-attachments/assets/c90ba4c2-abf8-464c-becb-a9a794b104a9)  



---

### âœ… ê²°ë¡   

- **GEM**ì€ Vision-Language Transformerê°€ **ë‚´ì¬ì ìœ¼ë¡œ ê°€ì§„ localization ëŠ¥ë ¥**ì„ ë°œêµ´í•œ ì—°êµ¬  
- í–¥í›„ **ë” í° VLM**ê³¼ ê²°í•© ì‹œ, fine-tuned detectorë¥¼ ëŒ€ì²´í•  ì ì¬ë ¥ ë³´ìœ   
- open-world recognition, segmentation, groundingì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ ì œì‹œ!  
