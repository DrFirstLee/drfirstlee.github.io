---
layout: post
title: "ğŸ” CLIP Surgery: A Closer Look at the Explainability of Contrastive Language-Image Pre-training"
author: [DrFirst]
date: 2025-09-05 7:00:00 +0900
categories: [AI, Research]
tags: [CLIP, Explainability, CAM, Vision-Language, Training-Free, Segmentation, PatternRecognition]
sitemap:
  changefreq: monthly
  priority: 0.8
---

---
### ğŸ” (í•œêµ­ì–´) CLIP Surgery: CLIPì„ ìˆ˜ìˆ í•´ì„œ ì„¤ëª… ê°€ëŠ¥ì„±ì„ ë†’ì´ë‹¤!  

![Image](https://github.com/user-attachments/assets/c0ab1e6d-506b-46b1-bd0f-519e74812557)

* **ì œëª©**: [A Closer Look at the Explainability of Contrastive Language-Image Pre-training (CLIP Surgery)](https://arxiv.org/abs/2304.05653)  
* **ì €ë„**: Pattern Recognition (2025)  
* **ì½”ë“œ**: [GitHub â€“ CLIP Surgery](https://github.com/xmed-lab/CLIP_Surgery)  
* **í•µì‹¬ í‚¤ì›Œë“œ**: `CLIP`, `Explainability`, `CAM`, `Vision-Language`, `Open-Vocabulary`  
* **ìš”ì•½**: CLIPì€ ê°•ë ¥í•œ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì´ì§€ë§Œ, **foreground ëŒ€ì‹  backgroundì— ì§‘ì¤‘**í•˜ê±°ë‚˜ **ì¡ìŒ í™œì„±í™”(noisy activation)** ë¬¸ì œê°€ ì¡´ì¬. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **Architecture Surgery**ì™€ **Feature Surgery**ë¥¼ ì ìš©í•´ **ì„¤ëª…ê°€ëŠ¥ì„±ì„ í¬ê²Œ ê°œì„ **í•˜ëŠ” í”„ë ˆì„ì›Œí¬ ì œì•ˆ!  

---

### ğŸš€ CLIP Surgery í•µì‹¬ ìš”ì•½

> í•œ ì¤„ ìš”ì•½: **â€œì¶”ê°€ í•™ìŠµ ì—†ì´, êµ¬ì¡°Â·íŠ¹ì§• ìˆ˜ìˆ ë§Œìœ¼ë¡œ CLIPì˜ ì„¤ëª…ê°€ëŠ¥ì„±ì„ ê°•í™”í•œë‹¤!â€**

1) **CLIP ì˜ ë¬¸ì œ ì¦ëª…** : ê¸°ì¡´ self-attentionì˜ ë¶ˆì¼ê´€ì„±, ì¤‘ë³µ íŠ¹ì§•ì˜ ë¬¸ì œë“¤ì„ ë°œê²¬

2) **Feature Surgery** : ì¤‘ë³µ(redundant) íŠ¹ì§• ì œê±° ë° ë¶ˆí•„ìš”í•œ noisy activation ì–µì œ ì„±ê³µí•´ì„œ CAM ë§Œë“¬!!  

3) **Training-Free Explainability** : Fine-tuningì´ ë¶ˆí•„ìš”, ì›ë³¸ CLIP ê·¸ëŒ€ë¡œ ì„¤ëª…ê°€ëŠ¥ì„± í™•ë³´í•˜ì˜€ê¸°ì— ë‹¤ì–‘í•˜ê²Œ í™œìš© ê°€ëŠ¥!!  

---

### ğŸ” ê¸°ì¡´ ì—°êµ¬ì˜ íë¦„  

- **CAM, Grad-CAM**: CNN/ViTì—ëŠ” íš¨ê³¼ì ì´ì§€ë§Œ CLIPì—ëŠ” ì ìš© ë¶ˆê°€  
  > CLIPì—ì„œëŠ” 'noisy'í•˜ê³ , 'Opposite visualization'í•˜ë‹¤. ì¦‰ localizationì— ë¬¸ì œê°€ìˆë‹¤!!
  ![Image](https://github.com/user-attachments/assets/21e4b20a-093a-4cb0-828e-31c6e5d82d86)

- ê·¸ëŸ¼, CLIPì—ì„œëŠ” ì™œ ì•ˆë¬ì„ê¹Œ?  
  - Self-attention êµ¬ì¡°ê°€ ì¼ê´€ë˜ì§€ ì•Šì€ ì˜ë¯¸ ì˜ì—­ì„ ì—°ê²°í•˜ê³ , ì¤‘ë³µ íŠ¹ì§• ë•Œë¬¸ì— ì¡ìŒì´ ë°œìƒí•´ foreground ëŒ€ì‹  backgroundë¥¼ ê°•ì¡°í•˜ê¸° ë•Œë¬¸  
  a. Self-attention êµ¬ì¡°ê°€ ì¼ê´€ë˜ì§€ ì•Šì€ ì˜ë¯¸ ì˜ì—­ì„ ì—°ê²°í•˜ëŠ” ì´ìœ ëŠ”?
    a-1. CLIPì€ ì´ë¯¸ì§€â€“í…ìŠ¤íŠ¸ ìŒì˜ ì „ì—­ì (global) ë§¤ì¹­ë§Œ í•™ìŠµí–ˆê¸°ì— attentionì´ ì„¸ë°€í•˜ê²Œ ê°ì²´ ë‚´ë¶€ì—ë§Œ ì§‘ì¤‘í•  í•„ìš”ê°€ ì—†ì—ˆìŒ!!  
    a-2. CLIPì˜ Query, Key, Value íŒŒë¼ë¯¸í„°ê°€ ì„œë¡œ ë‹¬ë¼ì„œ(heterologous parameters) Query/Keyê°€ ë§Œë“  ê´€ê³„ê°€ ì¼ê´€ë˜ì§€ ì•Šì€ ì˜ë¯¸ ì˜ì—­ì„ ì—°ê²°
    > A_raw = Ïƒ(s Â· QK_âŠ¤)V : heterologous parameters  
    > A_con = Ïƒ(s Â· VV_âŠ¤)V : homogeneous parameters  


  b. ì¤‘ë³µ íŠ¹ì§• ë•Œë¬¸ì— ì¡ìŒì´ ë°œìƒí•˜ëŠ” ì´ìœ ëŠ”?
    b-1. CLIPì€ ë§ì€ ì¹´í…Œê³ ë¦¬ë¥¼ ë™ì‹œì— í•™ìŠµ, í´ë˜ìŠ¤ ê°„ ê³µìœ ë˜ëŠ” íŠ¹ì§•(ì˜ˆ: â€œí•˜ëŠ˜â€, â€œí’€â€, â€œë„ë¡œâ€)ì´ ìì£¼ ë“±ì¥
    b-2. ë²”ìš©ì  íŠ¹ì§•ì´ ë°°ê²½ì— ì£¼ë¡œ ê¹”ë ¤ ìˆì–´ì„œ, self-attentionì´ ì‰½ê²Œ ë°°ê²½ìœ¼ë¡œ ëŒë ¤ê°€ noisy activationì´ ë°œìƒ
    ![Image](https://github.com/user-attachments/assets/e2ca7fa6-4181-47cc-95a8-70eaea9278e6)



- **Alignment ê¸°ë°˜ ê¸°ë²•**ë„ ìˆì§€ë§Œ ì´ëŠ” ì¶”ê°€ì ì¸ ëª¨ë¸, ë ˆì´ì–´, í˜¹ì€ íŒŒì¸íŠœë‹ì„ í•„ìš”ë¡œí•¨!(Not Traininig Free)  
  - **ECLIP**: CLIP featureì™€ segmentation maskë¥¼ **self-supervised**ë¡œ ë‹¤ì‹œ ì •ë ¬(alignment)  
    - ì›ë˜ CLIPì´ localizationì„ ì§ì ‘ ëª»í•˜ë¯€ë¡œ, **mask ì •ë³´ë¥¼ ì¶”ê°€ í•™ìŠµ**í•˜ì—¬ ë³´ì™„  
  - **RCLIP**: **Bounding box annotation**ì„ í™œìš©í•´ CLIPì˜ ì´ë¯¸ì§€â€“í…ìŠ¤íŠ¸ featureë¥¼ ê°ì²´ ë‹¨ìœ„ë¡œ ë³´ì •  
    - ê²°êµ­ CLIPì„ **ì¬í•™ìŠµ(fine-tuning)**í•˜ëŠ” ë°©ì‹  

---

### ğŸ§± CLIP Surgery êµ¬ì¡° (Architecture)

![Image](https://github.com/user-attachments/assets/a2afeaa7-cf08-4156-a214-b8dcc167b5ab)

#### i) Architecture Surgery(êµ¬ì¡°ì  ë¬¸ì œ ê°œì„ )  
- Raw Self-attention(i-1)ì´ ì¼ê´€ë˜ì§€ ì•Šì€ ì˜ë¯¸ ì˜ì—­ì„ ì—°ê²°í•˜ëŠ” ë¬¸ì œê°€ ìˆëŠ”ë°,  
- Consistent self-attention(i-2)ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ë°°ê²½ ê°•ì¡° ë°©ì§€  

> mFSRì€ Self-Attentionì´ ì–¼ë§ˆë‚˜ foreground(ê°ì²´)ì— ì§‘ì¤‘í–ˆëŠ”ê°€ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œ
![Image](https://github.com/user-attachments/assets/8c327f9a-df42-4330-a191-b98221d83cf9)

  i-1) `raw self-attention : A_raw = Ïƒ(s Â· QK_T)V`
  i-2) `consistent self-attention : A_con = Ïƒ(s Â· VV_âŠ¤)V`

  - ì´ë¥¼ ì½”ë“œë¡œ ë³´ë©´ Transformer Attention ë¶€ë¶„ì¸ `Attention` forward ë¶€ë¶„ì—ì„œ,   
  ```python
    # i-1) Raw Self-attention
    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        # original self-attention for the original path
        attn_ori = (q @ k.transpose(-2, -1)) * self.scale
        attn_ori = attn_ori.softmax(dim=-1)
        attn_ori = self.attn_drop(attn_ori)

        # i-2) consistent Self-attention  
        # replace k & q by v
        k = v
        q = k
        attn = (q @ k.transpose(-2, -1)) * scale
        attn = (attn).softmax(dim=-1)
        attn = self.attn_drop(attn)

        ## ë§ˆë¬´ë¦¬!! ë‘˜ë‹¤ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ return  
        x_ori = (attn_ori @ v).transpose(1, 2).reshape(B, N, C)
        x = (attn @ v).transpose(1, 2).reshape(B, N, C) # clip_surgery
        #x = v.transpose(1, 2).reshape(B, N, C) # mask_clip
        x = self.proj_drop(self.proj(x))
        x_ori = self.proj_drop(self.proj(x_ori))
        return [x, x_ori]
  ```
- Dual Pathsë¡œ CLIP ì„ë² ë”©ìš©ê³¼ CAM ì´ë¯¸ì§€ ì œì‘ìš© ë‘ê°œ í”¼ì²˜ ì¶”ì¶œ, FFNì˜ ë¶€ì •ì  ì˜í–¥ ìµœì†Œí™”  

  ![Image](https://github.com/user-attachments/assets/ff53ea79-cfcd-422f-95df-38a5b3564764)  
  - ìœ„ ì´ë¯¸ì§€ë¥¼ ë³´ë©´, CLIP Transformerë‚´ì—ì„œ FFNì€ ì˜¤íˆë ¤ ì´ìƒí•œëŒ€ë¥¼ ì§‘ì¤‘í•˜ê³ , ì´ˆê¸° Self-Attention ë¸”ë¡ ë¶€ë¶„ë„ ë¶€ì •í™•í•˜ë‹¤!  
  - ê·¸ë ‡ê¸°ì— `ìƒˆ ê²½ë¡œ`ì—ì„œëŠ” self-attentionë§Œ ì ìš©í•˜ê³  FFNì€ ìŠ¤ë‚!!  
  - í•œí¸ `ì›ë˜ ê²½ë¡œ` ë„ ìœ ì§€í•´ì„œ CLIP ë³¸ë˜ ì„ë² ë”© ë³´ì¡´  

  - ì´ë¥¼ ì½”ë“œë¡œ ë³´ë©´, Transformer layerì¸ `ResidualAttentionBlock` forward ë¶€ë¶„ì—ì„œ,   
  ```python
    def forward(self, x):
        # dual paths for blocks deeper than "d"
        if isinstance(self.attn, Attention):
            if isinstance(x, list):
                x, x_ori = x ## x_oriëŠ” ì›ë˜ê²½ë¡œ, x ëŠ” ìƒˆë¡œìš´ ê²½ë¡œ!
                x_res = self.attention(self.ln_1(x_ori))
                x_res, x_ori_res = x_res ## self-attention ê²°ê³¼ ë°˜í™˜. x_res ëŠ” `consistent self-attention`, x_ori_resëŠ” `raw self-attention`  
                x_ori += x_ori_res # ì›ë˜ ê²½ë¡œëŠ” self attention(x_ori_res)ì„ ë”í•˜ê³ 
                x_ori = x_ori + self.mlp(self.ln_2(x_ori)) # ì›ë˜ ê²½ë¡œì— FFN(x_ori)ë¥¼ ë”í•¨!!
                x += x_res # ìƒˆë¡œìš´ ê²½ë¡œëŠ” self attention(x_ori) ë§Œ ë”í•¨
                return [x, x_ori]
  ```

#### ii) Feature Surgery(í‘œí˜„ì  ë¬¸ì œ ê°œì„ )  
- CLIPì€ ë§ì€ ì¹´í…Œê³ ë¦¬ë¥¼ ë™ì‹œì— í•™ìŠµ, í´ë˜ìŠ¤ ê°„ ê³µìœ ë˜ëŠ” íŠ¹ì§•ì´ ë§ì€ ë¬¸ì œê°€ ìˆëŠ”ë°,    
- `ê°•ì•„ì§€` ê°€ íƒ€ê²Ÿì´ë”ë¼ë„ ê³ ì–‘ì´, í•˜ëŠ˜, ë°”ë‹¤, ë¹„í–‰ê¸° ë“± ê¸°íƒ€ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ë„ í•´ë³´ê³ , ì¤‘ë³µë˜ëŠ” ë¶€ë¶„ì„ ì œê±°  
  > L1 ì´ ì‘ìœ¼ë©´ ìœ ì‚¬í•˜ë‹¤ëŠ” ëœ». ì¦‰ positiveë‘ emptyê°€ ìœ ì‚¬í•´ë²„ë¦¼! ê·¸ë˜ì„œ ì¤‘ë³µì˜ ë¬¸ì œê°€ ë°œìƒ!!
    ![Image](https://github.com/user-attachments/assets/e2ca7fa6-4181-47cc-95a8-70eaea9278e6)
- ì½”ë“œë¡œ ë³´ë©´!! **clip_feature_surgery ë¶€ë¶„ì—ì„œ ì „ì²´ì—ì„œ ê²¹ì¹˜ëŠ” ë¶€ì˜ë¥¼ ê°€ì§€ê³  redundant_featsë¥¼ ë§Œë“¤ê³  ê° featsì—ì„œ redundant_featsë¥¼ ë¹¼ì¤Œ**   
```python 
# demp.py ì—ì„œ ë¯¸ë¦¬ emptyì— í•´ë‹¹í•˜ëŠ” all_textë¥¼ ì„ ì–¸í•´ë‘ê³  target_textë„ ë‘”ë‹¤ìŒ
all_texts = ['airplane', 'bag', 'bed', 'bedclothes', 'bench', 'bicycle', 'bird', 'boat', 'book', 'bottle', 'building', 'bus', 'cabinet', 'car', 'cat', 'ceiling', 'chair', 'cloth', 'computer', 'cow', 'cup', 'curtain', 'dog', 'door', 'fence', 'floor', 'flower', 'food', 'grass', 'ground', 'horse', 'keyboard', 'light', 'motorbike', 'mountain', 'mouse', 'person', 'plate', 'platform', 'potted plant', 'road', 'rock', 'sheep', 'shelves', 'sidewalk', 'sign', 'sky', 'snow', 'sofa', 'table', 'track', 'train', 'tree', 'truck', 'tv monitor', 'wall', 'water', 'window', 'wood']
target_texts = ['dog']

with torch.no_grad():
    # Extract image features
    image_features = model.encode_image(image)
    image_features = image_features / image_features.norm(dim=-1, keepdim=True)
    
    # Prompt ensemble for text features with normalization
    text_features = clip.encode_text_with_prompt_ensemble(model, all_texts, device)

    # Similarity map from image tokens with min-max norm and resize, B,H,W,N
    # ì—¬ê¸°ì„œ clip_feature_surgery ì§„í–‰!! 
    similarity = clip.clip_feature_surgery(image_features, text_features)
    similarity_map = clip.get_similarity_map(features[:, 1:, :], cv2_img.shape[:2])

    for b in range(similarity_map.shape[0]):
        for n in range(similarity_map.shape[-1]):
            if all_texts[n] not in target_texts:
                continue
            ## ì—¬ê¸°ì„œ 
            vis = (similarity_map[b, :, :, n].cpu().numpy() * 255).astype('uint8')
            vis = cv2.applyColorMap(vis, cv2.COLORMAP_JET)
            vis = cv2_img * 0.4 + vis * 0.6
            vis = cv2.cvtColor(vis.astype('uint8'), cv2.COLOR_BGR2RGB)
            print('CLIP:', all_texts[n])
            plt.imshow(vis)
            plt.show()


## clip.py clip_feature_surgery ë¶€ë¶„ì—ì„œ ì „ì²´ì—ì„œ ê²¹ì¹˜ëŠ” ë¶€ì˜ë¥¼ ê°€ì§€ê³  redundant_featsë¥¼ ë§Œë“¤ê³  ê° featsì—ì„œ redundant_featsë¥¼ ë¹¼ì¤Œ
def clip_feature_surgery(image_features, text_features, redundant_feats=None, t=2):

    if redundant_feats != None:
        similarity = image_features @ (text_features - redundant_feats).t()

    else:
        # weights to restrain influence of obvious classes on others
        prob = image_features[:, :1, :] @ text_features.t()
        prob = (prob * 2).softmax(-1)
        w = prob / prob.mean(-1, keepdim=True)

        # element-wise multiplied features
        b, n_t, n_i, c = image_features.shape[0], text_features.shape[0], image_features.shape[1], image_features.shape[2]
        feats = image_features.reshape(b, n_i, 1, c) * text_features.reshape(1, 1, n_t, c)
        feats *= w.reshape(1, 1, n_t, 1)
        redundant_feats = feats.mean(2, keepdim=True) # along cls dim
        feats = feats - redundant_feats
        
        # sum the element-wise multiplied features as cosine similarity
        similarity = feats.sum(-1)

    return similarity

```

---

### ğŸ§ª ì‹¤í—˜ í‰ê°€ ê²°ê³¼  

#### ğŸ¯ Explainability Benchmarks  
- VOC 2012, COCO, PascalContext ë“±ì—ì„œ  
  - **mIoU +22~36% ê°œì„ **  
  - **mSC +48~65% ê°œì„ **  

#### ğŸ¯ Open-Vocabulary Tasks  
- **Semantic Segmentation**: training ì—†ëŠ” ë°©ë²• ì¤‘ ìµœê³  ì„±ëŠ¥ (ì˜ˆ: PascalContext mIoU 29.3%)  
- **Multi-label Recognition**: NUS-Wideì—ì„œ CLIP ëŒ€ë¹„ +11.61% mAP í–¥ìƒ  
- **Interactive Segmentation**: SAMì— í…ìŠ¤íŠ¸â†’í¬ì¸íŠ¸ ë³€í™˜ìœ¼ë¡œ ìˆ˜ì‘ì—… ë¼ë²¨ ëŒ€ì²´  
- **Multimodal Visualization**: CLIPì˜ í•™ìŠµ ê³¼ì • ìì²´ í•´ì„ ê°€ëŠ¥  
  ![Image](https://github.com/user-attachments/assets/bf45702f-6073-4693-b8a5-9007e63b0ca0)
  - `[end]` í† í°ì´ ê°€ì¥ í”íˆ í™œì„±í™”ëœ í…ìŠ¤íŠ¸ í† í°ì´ë©°, â€œinâ€, â€œ.â€, â€œofâ€ ê°™ì€ ë¹„ê°ì²´(non-object) ë‹¨ì–´ë„ ë†’ì€ ë°˜ì‘ì„ ë³´ì„!!  
  - ì´ëŠ” CLIPì˜ ì–´íœ˜ ì‚¬ì „ì— **ë¶ˆí•„ìš”í•œ ì¤‘ë³µ í† í°(redundant tokens)**ì´ ì¡´ì¬í•¨ì„ ì‹œì‚¬  
  - í–¥í›„ CLIP í•™ìŠµ ê³¼ì • ê°œì„  ì•„ì´ë””ì–´ë¥¼ ì œê³µ!!  


#### ğŸ‘€ ì •ì„± ë¹„êµ : ì˜í•˜ëŠ”êµ¬ë§Œ!  

![Image](https://github.com/user-attachments/assets/ddf81331-3582-4ae1-bcec-20c60b6110a0)

- ì›ë³¸ CLIP: background ê°•ì¡° + ì¡ìŒ ë‹¤ìˆ˜  
- CLIP Surgery: ì„ ëª…í•˜ê³  ê°ì²´ ì¤‘ì‹¬ heatmap ìƒì„±  
- â†’ ê¸°ì¡´ Grad-CAM, Bi-Modal, gScoreCAM ëŒ€ë¹„ **í™•ì—°íˆ í–¥ìƒëœ ì‹œê°í™” í’ˆì§ˆ**  


#### ğŸ§ª Ablation ë¶„ì„  

![Image](https://github.com/user-attachments/assets/73150a1c-13ca-4e48-bb1c-8648056b2c0b)

- **Architecture Surgery**(i)ë§Œ ì ìš© â†’ mSC +47.88%  
- Feature Surgery(ii) ì¶”ê°€ â†’ ì¶”ê°€ +3.17% í–¥ìƒ  
- Dual Paths ì—†ìœ¼ë©´ collapse ë°œìƒ, í•µì‹¬ ëª¨ë“ˆì„ì„ ê²€ì¦  

---

## âœ… ê²°ë¡   

- **CLIP Surgery**ëŠ” CLIP ëª¨ë¸ì˜ **ê·¼ë³¸ì  ì„¤ëª…ê°€ëŠ¥ì„± ë¬¸ì œ(opposite visualization, noisy activation)**ë¥¼ í•´ê²°  
- **ì¶”ê°€ í•™ìŠµ ì—†ëŠ” training-free ì ‘ê·¼**ìœ¼ë¡œ CAM ê¸°ë°˜ í•´ì„ ê°•í™”  
- Semantic Segmentation, Multi-label Recognition, Interactive Segmentation ë“± ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ì§ì ‘ í™œìš© ê°€ëŠ¥  
- CLIP ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜ ì´í•´ì™€ í–¥í›„ ëª¨ë¸ ê°œì„ ì— ì¤‘ìš”í•œ í†µì°° ì œê³µ  
