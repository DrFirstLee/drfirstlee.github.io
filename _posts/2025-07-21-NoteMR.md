---
layout: post
title: "🧠 Notes-guided MLLM Reasoning"
author: [DrFirst]
date: 2025-07-21 09:00:00 +0900
categories: [AI, Experiment]
tags: [MLLM, NoteMR, Visual Notes, LLM, CVPR 2025, CVPR]
sitemap :
  changefreq : monthly
  priority : 0.8

---

---

## 🧠 (English) Notes-guided MLLM Reasoning

* **Title**: [Notes-guided MLLM Reasoning: Enhancing MLLM with Knowledge and Visual Notes](https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Notes-guided_MLLM_Reasoning_Enhancing_MLLM_with_Knowledge_and_Visual_Notes_CVPR_2025_paper.html)
* **Conference**: CVPR 2025 (Fang et al.)
* **Key Keywords**: `Multimodal LLM`, `Visual Reasoning`, `Notes-guided Inference`, `Visual Note`, `Explainability`

---

## 🧠 3-Line Summary

1. NoteMR refines external knowledge and image context to create Knowledge Notes.
2. It identifies and extracts salient visual regions into Visual Notes to enhance perception.
3. This approach improves KB-VQA performance on OK-VQA by 5.31%.

---

### ⚠️ Two Key Limitations in Existing KB-VQA Methods

#### 0. First of all, what is KB-VQA?

> **KB-VQA** stands for **Knowledge-Based Visual Question Answering**. It involves not only understanding the image and question, but also utilizing **external knowledge** to answer complex, open-ended visual questions.

📦 Representative KB-VQA Datasets

| Dataset     | Description                                             | Question Type   | Evaluation                             |
| ----------- | ------------------------------------------------------- | --------------- | -------------------------------------- |
| **OK-VQA**  | Requires external knowledge for answers                 | Open-ended      | BLEU, ROUGE, answer matching           |
| **A-OKVQA** | OK-VQA extension with answer choices                    | Multiple choice | Accuracy                               |
| **GQA**     | Focused on relational reasoning and scene understanding | Structured QA   | Logical consistency, reasoning metrics |
| **VCR**     | Visual Commonsense Reasoning                            | QA + Rationale  | Choice + Explanation Accuracy          |

📌 Difference between General VQA and KB-VQA

| Aspect      | General VQA              | KB-VQA                       |
| ----------- | ------------------------ | ---------------------------- |
| Input       | Image + Question         | Image + Question + Knowledge |
| Example Q   | "What is the cat doing?" | "What breed is this cat?"    |
| Info Needed | Visual only              | Visual + External knowledge  |
| Models      | BLIP, GIT                | TRiG, RAVQA-V2, NoteMR       |

#### 1. External Knowledge Can Be Noisy

* External knowledge retrieved from the web may be redundant or irrelevant, which can confuse the model and lead to **incorrect answers**.

Example:

> Q: *"What do they call running around the bases after hitting the ball?"*
> With retrieved info: answers "Stealing" (wrong) due to noisy text
> Without retrieval: correctly answers "Home run"

![error1](https://github.com/user-attachments/assets/d6f9c1ab-5998-44da-b654-cd395f4543db)

#### 2. 👁️ Lacking Fine-Grained Visual Perception

* MLLMs often fail to pick up on subtle visual cues, leading to **hallucinations** or visually irrelevant answers.

Example:

> Despite a green light in the image, model answers "Stop" due to poor visual focus.

![error2](https://github.com/user-attachments/assets/395bbcd0-44f7-4d47-8dbe-3c44a52c62dc)

---

## 🔍 Method Summary

* **🧠 Knowledge Note Generation**
  Filters retrieved external knowledge + image context to generate clean and relevant knowledge notes.

* **👁️ Visual Note Generation**
  Extracts attentive visual regions informed by knowledge notes, reduces hallucinations, and strengthens perception.

* **📈 Achieves SOTA Performance**

  * +5.31% on OK-VQA
  * +3.4% on A-OKVQA

---

## 🧪 Method Architecture

![structure](https://github.com/user-attachments/assets/4d710d37-05f4-4ef8-a83c-240eab8b545c)

### 1. Creating Textual Notes (`N_kl`)

* Unlike past approaches that only extract knowledge, NoteMR combines **external and internal knowledge** to create notes.
* External knowledge sources: Google Search Corpus + Wikidata

**Top-k Selection**:

* `Q`: fused embedding of the question and visual features
* `D`: candidate documents embedded
* Use relevance score between `Q` and `D` to pick top-5 passages

**`N_kl` Construction**:

* Prompt `c_k`, image `V`, top-k passages `P`
* Text encoder: **PreFLMR**
* Image encoder (at this stage): **CLIP**

### 2. Creating Visual Notes (`N_vl`)

* Extract visual patches using GradCAM with cross-modal attention
* Convert original image `V` into 576 patches (16x16)
* Compute attention scores between `N_kl` tokens and visual patches
* Use transformer attention:

  * Q = `N_kl`
  * K = key-weighted `V`
  * V = value-weighted `V`
* Combine heads → generate heatmap `H` → apply threshold `λ = 0.6` to mask
* Masked visual embedding becomes final `N_vl`
* Image encoder: **BLIP**

### 3. Final Answer Selection

* Inputs: question `q`, image `V`, knowledge note `N_kl`, visual note `N_vl`
* Format into final prompt (see below)

![final_prompt](https://github.com/user-attachments/assets/6846f280-e248-4dd8-8b89-0b9f5695b714)

* Generate `c_0` candidate answers and choose the best (used 3 candidates in experiments)

---

## 🔮 Results

> Did it perform well? Absolutely!

![res](https://github.com/user-attachments/assets/1718c946-0c6f-40ac-9419-a741fe1068b7)

* Outperforms all baselines on OK-VQA and A-OKVQA
* Even beats 13B competitors using **LLaVA-NeXT-8B**

**Ablation (Table 3)**:

* Step-by-step improvements observed:

  1. MLLM only
  2. * Retrieved Knowledge
  3. * Knowledge Notes
  4. * Visual Notes
  5. * Candidate Output Selection

---

## ✅ Conclusion

* Introduces a modular, note-based architecture for MLLM reasoning
* Transitions MLLM from naive answering to **structured reasoning**
* High potential for use in **RAG**, **AI tutors**, and **multi-hop QA systems**


---


### 🧠 (한국어) Notes-guided MLLM Reasoning  

- **제목**: [Notes-guided MLLM Reasoning: Enhancing MLLM with Knowledge and Visual Notes](https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Notes-guided_MLLM_Reasoning_Enhancing_MLLM_with_Knowledge_and_Visual_Notes_CVPR_2025_paper.html)
- **학회**: CVPR 2025 (Fang et al.)
- **핵심 키워드**: `Multimodal LLM`, `Visual Reasoning`, `Notes-guided Inference`, `Visual Note`, `Explainability`

---

## 🧠 3줄 요약

1. NoteMR은 외부 지식과 이미지를 정제해 Knowledge Note를 만들고,  

2. 이미지의 핵심 영역만 추출해 Visual Note로 시각 정보 인식을 개선하며,

3. 이를 통해 KB-VQA 성능을 OK-VQA 기준 5.31% 향상시킨 최신 기법이다.

---

### ⚠️ 기존 방식의 주요 한계 2가지

#### 0. 우선!! KB-VQA란!??
> **KB-VQA**는 **Knowledge-Based Visual Question Answering**의 줄임말로,  
> 단순히 이미지와 질문을 이해하는 것에 그치지 않고,  
> **외부 지식(knowledge)**을 활용해 **정답을 추론**해야 하는 고차원 시각추론 과제입니다.

📦 대표 KB-VQA 데이터셋 소개

| 데이터셋 | 설명 | 질문 유형 | 평가 방식 |
|----------|------|-----------|------------|
| **OK-VQA** | Outside Knowledge VQA. 외부 지식 없이는 답이 어려운 질문을 포함 | 오픈형 (open-ended) | BLEU, ROUGE, 정답 매칭 등 |
| **A-OKVQA** | OK-VQA의 확장판. 정답 후보를 포함하여 정량 평가 가능 | 선택형 (multiple-choice) | 정답 선택 정확도 |
| **GQA** | 복잡한 관계 추론과 장면 이해 능력을 평가 | 구조화된 질문/답변 | 논리 정확도, 추론 패턴 분석 |
| **VCR** | Visual Commonsense Reasoning. 상황에 대한 상식적 추론 요구 | 질문 + 이유 설명 | 정답 선택 + rationale 평가 |


📌 일반 VQA와 KB-VQA의 차이점

| 항목 | 일반 VQA | KB-VQA |
|------|----------|--------|
| 입력 | 이미지 + 질문 | 이미지 + 질문 + 외부 지식 |
| 예시 질문 | “이 고양이는 무엇을 하고 있나요?” | “이 고양이는 어느 품종인가요?” |
| 필요한 정보 | 이미지 속 시각 정보 | 이미지 + 배경 지식 (e.g. 품종 지식) |
| 대표 모델 | BLIP, GIT 등 | TRiG, RAVQA-V2, NoteMR 등 |

- KB-VQA의 기존 방식은  
1. 정보를 제공해주기 (Retrieval Method)  
  1-1. `ConceptNet` 과 같은 고정된 지식(fxed knowledge bases) 사용하기    
  1-2. open-world knowledge(Google이나 Wikipedia) 에서 정보 가져오기  
2. LLM을 활용하기 (Implicit Method)  
  - 캡션을 추가하여 답하거나, 자체적으로 지식을 호출해서 답하기 등의 기법들!!   (PICA, PromptCap 등)  

#### 1. KB-VQA에서 외부 지식은 ‘노이즈’가 될 수 있음

- MLLM은 외부 지식을 활용해 답을 생성하지만,  
  **검색된 지식이 중복되거나 부정확한 경우**,  
  오히려 모델이 **혼란에 빠지고 오답을 생성**할 수 있습니다.

예시:

> 질문: *"What do they call running around the bases after hitting the ball?"*  
> 단순 질문에 답할떄는 Stealing이라고 잘못 답함!!  
> 외부 지식을 넣은경우 검색된 지식이 혼란을 유발해 “Stealing”이라고 오답을 냄.
> 그런데, 오른쪽 이미지처럼, MLLM이 자체적으로 고민해서 답하라고하면 모델은 “Home run”을 잘 답해!  
> 
![error1](https://github.com/user-attachments/assets/d6f9c1ab-5998-44da-b654-cd395f4543db)


#### 2. 👁️ Fine-grained 시각 정보 처리 능력 부족

- MLLM의 비전 인코더는 **이미지의 세부적인 특징**을 잘 잡아내지 못합니다.  
- 이로 인해 **hallucination**(입력과 상관없는 상상 응답) 문제가 발생합니다.

예시:
> 이미지에 **초록불**이 있음에도 모델은 “Stop”이라고 응답 →  
> **세밀한 시각 인식 실패** 사례.
>
![error2](https://github.com/user-attachments/assets/395bbcd0-44f7-4d47-8dbe-3c44a52c62dc)


---

## 🔍 연구 요약

- **🧠 Knowledge Note 생성**  
  검색된 외부 지식과 이미지를 기반으로, **불필요하거나 중복된 정보는 제거**하고  
  이미지와 관련된 **핵심 지식만 정리한 요약**을 생성함

- **👁️ Visual Note 생성**  
  이미지와 knowledge note를 바탕으로, **중요 시각 정보에 집중하도록 유도**하여  
  **정확한 시각 인지 능력 강화** → hallucination 문제 완화

- **📈 최신 성능 달성**  
  - **OK-VQA** 데이터셋에서 **5.31%** 성능 향상  
  - **A-OKVQA** 데이터셋에서 **3.4%** 성능 향상  
  → 실험을 통해 **NoteMR의 효과성 입증**

---

## 🧪 연구 방법론

![structure](https://github.com/user-attachments/assets/4d710d37-05f4-4ef8-a83c-240eab8b545c)  
- 3단계로 구성!!  
  1. 텍스트 노트 만들기,  
  2. 텍스트 노트를 바탕으로 비주얼 노트 만들기
  3. 2개의 노트, 이미지, 질문을 넣고 후보들 생성 + 후보들 중 답 선정

### 1. 텍스트 노트(`N_kl`) 만들기  
- 기존 연구들은 외부에서든 내부에서든 지식을 추출하려고만 했지만  
- 여기서는 **외부 내부를 결합**해서 `노트를 생성`하는것에 중점을 두었다!!  
  - 외부 지식의 경우는 MLLM이 내부 지식을 잘 추출하는데 활용했고, 이를 통해 내부지식과의 시너지를 일으켰다.  
  - 외부지식은 기존연구들 처럼 Google Search Corpus랑 Wikidata 사용했다 (외부지식이 꼭 필요한 OK-VQA나 A-OKVQA)  
- 텍스트 노트 재료 선정하기: tok-K 방법을 사용  
  - `Q`(쿼리임베딩) 생성 : `질문 텍스트임베딩` 과 `이미지를 벡터화하여 텍스트임베딩으로 정렬`한것을 합친다!
  - `D`(문서임베딩) 생성 : Wikidata 같은 document를 텍스트 임베딩한다  
  - `Q`와 `D`사이의 관련성점수를 구해서, 가장 점수가 높은 k 개 문서를 뽑는다!! (실험에서는 Top-k를 5로함)
- `지식 노트(N_kl) 생성` : 외부 지식(`P`)로 MLLM 내부 지식을 최대한 활용할 수 있게 한다.  
  - 이를 통해 잘못된 외부지식으로 인한 잡음을 방지한다.  
  - `N_kl` 생성 재료 
    - `c_k` : 사전 준비된 프롬포트
    - `V` : 오리지날 이미지
    - `P` : 선정된 top k 개의 데이터  
    ![ck1](https://github.com/user-attachments/assets/b2f0f4d7-5f4d-4435-9e40-62f8bf1f021a) 
- 텍스트인코더는 `PreFLMR` 이미지 인코더는 `CLIP` 사용!!

### 2. 비주얼 노트(`N_vl`) 만들기  
- 이미지의 중요한 패치를 선별하기위해서 크로스모달 매트릭스를 활용했다!! `GradCAM`
- 중복 정보를 제거하기 위해, 우리는 이미지의 집중부위를 유지했고, 언어모델이 질문과 관계되는곳에 집중하여 할루시내이션을 경감시켰다  
  - 구체적으로는, 오리지날 이미지 `V`를 `M`개의 패치로 만들고, 각 패치의 feature를 구했다.  
    - 패치사이즈는 16X16 패치가 576개인것 으로함
  - 지식노트 `N_kl`을 토큰화해서 각각의 패치와 토큰간의 멀티헤드 크로스모달 어텐션 값을 구했다
  - 이때의 멀티헤드 트랜스포머 구조!! (i게의 헤드)
    - Q : 지식노트 `N_kl`
    - K : `Key weight 행렬` X 이미지 패치 V
    - V : `value weight 행렬` X 이미지 패치 V
  - i 개를 모두 결합해서 `H`를 구하고!!
  - 임계값 `λ` 를 넘는 부분만을 남겨서 마스크 생성!! (0.6으로함)
  - 오리지날 이미지 `V` dot `Mask` 해서 최종 비쥬얼 노트 `N_vl` 생성
- 이미지 인코더는 `BLIP` 사용!!

### 3. 최종 답변 선택!!  
- 지금까지 준비된것: 질문 `q`, 오리지날 이미지 `V`, 지식 노트 `N_kl` , 비쥬얼 노트 `N_vl` 
- 준비된 프롬포트에 잘 녹여서 넣는다!!!
![final_prompt](https://github.com/user-attachments/assets/6846f280-e248-4dd8-8b89-0b9f5695b714)
- 그렇게 `co` 개의 후보 답변을 만든다음!! 그중에서 제일 좋은 답변을 뽑는다!!
- 뒤의 실험부분을 보면 후보는 3개로했음!!


##  실험 결과!!
> 결국 점수가 좋았겠죠!?ㅎㅎ  
![res](https://github.com/user-attachments/assets/1718c946-0c6f-40ac-9419-a741fe1068b7)

- OK-VQA 및 A-OKVQA 모두에서 가장 좋은 결과를 보였음!!
  - `LLaVa-NeXT-8b` 에서 최고의 성능!!  
  - 다른 연구에서의 13B 보다도 성능이 좋았다!!  
- 모듈별로 보기! (Ablation Study)  
  - 이미지의 3번 Table!! 5단계로 나누었다!  
  - 1단계: 그냥 MLLM만 가지고 문제풀기  
  - 2단계: 검색된 지식 추가  
  - 3단계: 지식노트로 추가  
  - 4단계: 지식노트 + 비주얼노트  
  - 5단계: 4단계로 여러개한뒤 선정  
  - 단계별로 모두 발전함을 확인할수 있었다!!


---


## ✅ 결론

- 노트 기반 구조를 통해 **모델의 추론 과정을 단계화**하고,
- 단순 응답형 모델에서 **사고-기반 reasoning 모델**로 발전 가능
- **RAG**, **AI 튜터**, **멀티홉 질의응답 시스템**에 응용 가능성 높음

