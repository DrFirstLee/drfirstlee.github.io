---
layout: post
title: "🔗 Understanding GLIP - CLIP이해하기!!!"
author: [DrFirst]
date: 2025-06-25 07:00:00 +0900
categories: [AI, Research]
tags: [GLIP, Vision-Language, Object Detection, CVPR, CVPR 2022]
sitemap :
  changefreq : monthly
  priority : 0.8
---
---

## 🧠 (한국어) GLIP 알아보기!  
_🔍 자연어로 객체를 찾는 혁신적인 모델!_

> 서로 다른 언어를 연결하면 새로운 소통의 가능성이 열리듯,  
> 오늘은 시각과 언어를 연결하는 혁신적인 모델 **GLIP**에 대해 알아보겠습니다!

![glip_concept](https://github.com/user-attachments/assets/placeholder-glip-concept)

> 논문: [Grounded Language-Image Pre-training](https://arxiv.org/abs/2112.03857)  
> 발표: CVPR 2022 (Microsoft Research)  
> [🔗 GitHub 저장소](https://github.com/microsoft/GLIP)

---

### 💡 GLIP의 특징 요약!!

1. **언어 기반 탐지**  
   - "빨간 모자를 쓴 사람", "책상 위의 스마트폰" 같은 **자연어 설명**으로 객체 탐지 가능!
2. **제로샷 능력**  
   - 학습 때 본 적 없는 객체도 텍스트 설명만으로 탐지 가능
3. **통합 프레임워크**  
   - 객체 탐지, 구문 그라운딩, 비전-언어 이해를 하나의 모델에서 처리

---

### 🧠 GLIP 등장의 배경

> 기존 객체 탐지는 미리 정의된 카테고리에만 국한되었습니다.  
> 하지만 자연어로 객체를 설명해서 찾을 수 있다면 어떨까요?

- **고정된 카테고리 한계**: YOLO, R-CNN 같은 기존 모델은 미리 정의된 클래스만 탐지 가능 (예: COCO 80개 클래스)
- **비싼 어노테이션 비용**: 새로운 클래스를 위한 바운딩 박스 라벨링에 많은 인력과 시간 필요
- **언어-비전 격차**: 비전과 언어 이해가 분리되어 풍부한 크로스모달 상호작용 부재
- **제로샷 도전**: 새로운 라벨 데이터 없이는 새로운 객체 탐지 불가능

---

#### 🔍 기존 방식 vs GLIP 방식

**기존 객체 탐지:**
```
입력: 이미지
출력: [클래스_ID, 바운딩박스, 신뢰도]
예시: [person, (100,50,200,150), 0.95]
```

**GLIP 방식:**
```
입력: 이미지 + 텍스트 쿼리
출력: [그라운딩된_텍스트, 바운딩박스, 신뢰도]
예시: ["빨간 셔츠를 입은 사람", (100,50,200,150), 0.89]
```

---

#### 📘 핵심 혁신: 그라운딩(Grounding)

- **구문 그라운딩**: 텍스트 구문을 이미지 영역과 연결
- **사전 학습 전략**: 대규모 이미지-텍스트 쌍으로 학습
- **통합 손실 함수**: 탐지와 그라운딩 목표를 결합

---

### 🖇️ GLIP 모델 구조

```
텍스트 인코더 (BERT 기반)
    ↓
텍스트 특징
    ↓
크로스모달 융합
    ↑
비주얼 특징
    ↑
이미지 인코더 (ResNet/Swin)
    ↓
특징 피라미드 네트워크
    ↓
탐지 헤드
```

#### 📌 GLIP 구성요소

| 구성요소 | 설명 | 목적 |
|----------|------|------|
| **텍스트 인코더** | BERT 기반 언어 모델 | 텍스트 쿼리에서 의미 특징 추출 |
| **이미지 인코더** | ResNet 또는 Swin Transformer | 이미지에서 시각 특징 추출 |
| **크로스모달 융합** | 멀티헤드 어텐션 레이어 | 텍스트와 시각 특징 정렬 |
| **탐지 헤드** | 분류 + 회귀 | 바운딩 박스와 신뢰도 예측 |

---

### 🔄 GLIP 학습 전략

#### 🎯 통합 손실 함수
```
L_total = L_detection + L_grounding + L_alignment

여기서:
- L_detection: 표준 객체 탐지 손실
- L_grounding: 구문 그라운딩 손실
- L_alignment: 비전-언어 정렬 손실
```

#### 📊 학습 데이터

| 데이터 타입 | 예시 | 목적 |
|-------------|------|------|
| **객체 탐지** | COCO, Objects365 | 바운딩 박스 회귀 학습 |
| **구문 그라운딩** | Flickr30K, Visual Genome | 텍스트-영역 정렬 학습 |
| **이미지-텍스트 쌍** | Conceptual Captions, LAION | 크로스모달 표현 학습 |

---

### 🏋️ 학습 설정

- **사전 학습 단계:**
  - **데이터셋**: 2,700만 개의 그라운딩된 이미지-텍스트 쌍
  - **배치 크기**: 256
  - **학습률**: 1e-4 (코사인 스케줄링)
  - **옵티마이저**: AdamW (가중치 감소 0.05)
  - **에폭**: 결합 데이터셋으로 12 에폭

- **파인튜닝 단계:**
  - **대상 데이터셋**: COCO, LVIS, ODinW
  - **학습률**: 1e-5
  - **배치 크기**: 16
  - **데이터 증강**: 표준 탐지 증강 기법

---

### 🧩 GLIP 성능 결과

#### 1. **제로샷 탐지 성능**

| 모델 | COCO AP | LVIS AP | ODinW (13개 데이터셋 평균) |
|------|---------|---------|---------------------------|
| CLIP + 탐지 헤드 | 12.1 | 8.3 | 15.7 |
| GLIP-T | 42.9 | 26.9 | 44.9 |
| GLIP-L | 46.7 | 31.8 | 51.4 |

#### 2. **퓨샷 학습**

| 샷 수 | COCO AP | LVIS AP |
|-------|---------|---------|
| 1샷 | 35.8 | 22.1 |
| 5샷 | 41.2 | 27.4 |
| 10샷 | 43.6 | 29.8 |

#### 3. **구문 그라운딩 결과**

| 데이터셋 | Recall@1 | Recall@5 | Recall@10 |
|----------|----------|----------|-----------|
| Flickr30K | 82.5 | 92.8 | 95.1 |
| RefCOCO | 78.9 | 87.6 | 91.2 |
| RefCOCO+ | 71.4 | 82.3 | 86.9 |

---

### 🎯 실제 활용 사례

#### 1. **자연어 쿼리**
```python
# GLIP이 처리할 수 있는 쿼리 예시:
queries = [
    "빨간 모자를 쓴 사람",
    "테이블 옆에 있는 나무 의자",
    "책상 위의 스마트폰",
    "공을 가지고 노는 강아지",
    "빨간불이 켜진 신호등"
]
```

#### 2. **도메인 적응**
- **의료 이미지**: "폐 엑스레이의 종양"
- **위성 이미지**: "강 근처의 건물들"
- **리테일**: "선반 위의 제품들"

#### 3. **인터랙티브 탐지**
- 사용자가 자연어로 객체 설명
- 새로운 객체 카테고리에 대한 재학습 불필요
- 유연하고 직관적인 인터페이스

---

### 📈 다른 모델과의 비교

| 모델 | 타입 | 제로샷 | 언어 입력 | 학습 데이터 |
|------|------|---------|-----------|-------------|
| **YOLO** | 전통적 | ❌ | ❌ | 탐지 전용 |
| **DETR** | 종단간 | ❌ | ❌ | 탐지 전용 |
| **CLIP** | 비전-언어 | ✅ | ✅ | 이미지-텍스트 쌍 |
| **GLIP** | 그라운딩된 V-L | ✅ | ✅ | 탐지 + 그라운딩 |

---

## 🧠 마무리 생각

GLIP은 객체 탐지에서 **패러다임의 전환**을 나타냅니다:  
고정된 카테고리에서 **열린 어휘 탐지**로!

📝 GLIP 연구를 통해 배운 점:

- **언어는 컴퓨터 비전 과제에서 강력한 인터페이스**
- **다양한 데이터의 대규모 사전 학습**이 일반화에 중요
- **통합 프레임워크**가 분리된 모델보다 더 나은 성능 달성 가능

❗ 이 연구는 AI의 미래가 **멀티모달 이해**에 있음을 보여줍니다,  
비전과 언어가 매끄럽게 함께 작동하는 그런 미래 말이지요!

--- 