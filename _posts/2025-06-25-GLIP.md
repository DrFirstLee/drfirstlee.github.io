---
layout: post
title: "🔗 Understanding GLIP - CLIP이해하기!!!"
author: [DrFirst]
date: 2025-06-25 07:00:00 +0900
categories: [AI, Research]
tags: [GLIP, Vision-Language, Object Detection, OVOD, Open Vocabulary, CVPR, CVPR 2022]
sitemap :
  changefreq : monthly
  priority : 0.8
---
---

## 🧠 (한국어) GLIP 알아보기!  
_🔍 Faster R-CNN의 Open Vocabulary 버전!!_

> 객채 인식(Object Detection) 중 Stage2 모델의 대표 Faster R-CNN에,  
> 자유로운 택스트 프롬포트 기능을 추가한!! **GLIP**에 대해 알아보아요!!  
> OVOD : Open Vocabulary Object Detection  

![manhwa]()

> 논문: [Grounded Language-Image Pre-training](https://arxiv.org/abs/2112.03857)  
> 발표: CVPR 2022 (Microsoft Research)  
> [🔗 GitHub 저장소](https://github.com/microsoft/GLIP)

---

### 💡 GLIP의 특징 요약!!

1. **언어 기반 탐지**  
   - "빨간 모자를 쓴 사람", "책상 위의 스마트폰" 같은 **자연어 설명**으로 객체 탐지 가능!
2. **제로샷 능력**  
   - 학습 때 본 적 없는 객체도 텍스트 설명만으로 탐지 가능
3. **통합 프레임워크**  
   - 객체 탐지, 구문 그라운딩, 비전-언어 이해를 하나의 모델에서 처리

---

### 🧠 GLIP 등장의 배경

> 기존 객체 탐지는 미리 정의된 카테고리(fixed set)에만 국한되었거나,     
> 일부 Open-Set 도 있었지만 학습데이터의 한계, 모델 구조의 한계 증이 있었습니다!!  

---

#### 🔍 기존 방식(fixed-sed) vs GLIP 방식

**기존 객체 탐지:**
```
입력: 이미지
출력: [클래스_ID, 바운딩박스, 신뢰도]
예시: [person, (100,50,200,150), 0.95]
```

**GLIP 방식:**
```
입력: 이미지 + 텍스트 쿼리 # 텍스트 쿼리 ex. 빨간 셔츠를 입은 사람이 공원에서 강아지와 놀고 있다
출력: [그라운딩된_텍스트, 바운딩박스, 신뢰도] # 그리운딩된 텍스트 ex. 빨간 셔츠 입은 사람 or 강아지
예시: ["빨간 셔츠를 입은 사람", (100,50,200,150), 0.89]
```

- **고정된 카테고리 한계**: YOLO, R-CNN 같은 기존 모델은 미리 정의된 클래스만 탐지 가능 (예: COCO 80개 클래스)
- **비싼 어노테이션 비용**: 새로운 클래스를 위한 바운딩 박스 라벨링에 많은 인력과 시간 필요
- **언어-비전 격차**: 비전과 언어 이해가 분리되어 풍부한 크로스모달 상호작용 부재
- **제로샷 도전**: 새로운 라벨 데이터 없이는 새로운 객체 탐지 불가능

---


#### 🔍 기존 Open Vocabulary Detection 연구들의 한계  
> GLIP이 첫 OVOD 모델은 아닌데,, 기존 OVOD 의 한계는?  

- **ViLD (2021)** 📋  
  -Two-stage detector 방식에서 CLIP을 Second Stage에만! 증류하는 방식  
  - **한계**: 분리된 학습으로 인한 정보 손실, CLIP 모델에 의존적
  - 한계를 쉽게 설명하면, Stage1 부분(객채 여부 파악하는곳)은 기존 모델을 그대로 쓰기에 진정 Open-Set이기에는 성능이 좋지 못함  
  
- **MDETR (2021)** 🔗  
  - End-to-end 멀티모달 detection 시도
  - **한계**: 상대적으로 작은 규모의 human-annotated 데이터에 의존

**GLIP의 핵심 차별점**:
- **Scale**: 27M개의 대규모 grounded pairs (기존 대비 압도적!)
- **Unified Learning**: Detection과 Grounding을 하나의 손실함수로 동시 학습
- **Problem Reformulation**: 별도 모듈 없이 detection = phrase grounding으로 재정의
- **Web-scale 활용**: 노이즈 있는 웹 데이터도 효과적으로 활용


### 🖇️ GLIP 모델 구조: Stage 1/2 관점에서 이해하기

#### 🔍 Faster R-CNN vs GLIP 구조 비교

**Faster R-CNN (기존 Two-stage)**:
```
📸 이미지 → CNN → RPN (Stage 1: 객체 후보 영역 제안)
                    ↓
                Classification + Regression (Stage 2: 클래스 분류)
```

**GLIP (언어 인식 Two-stage)**:
```
📸 이미지 → Vision Encoder ──┐
                           ├── 딥 퓨전 (X-MHA)
📝 텍스트 → Language Encoder ──┘
                           ↓
                    Stage 1: 언어 인식 RPN
                           ↓
                    Stage 2: Phrase Grounding
```

#### 📊 Stage별 세부 구조

**🎯 Stage 1: 언어 인식 Region Proposal**

> 주어진 프롬포트를 바탕으로 이미지에다가 bbox를 그린다!!    

```
이미지 특징 + 텍스트 특징 → 크로스모달 융합
                            ↓
                "프롬프트 관련 객체 후보" 영역 제안
```

**🎯 Stage 2: Phrase Grounding**

> stage1에서 찾은 bbox에 매칭되는 단어를 찾는다!  


```
Region Features ──┐
                  ├── Similarity Matching
Text Features ────┘
                 ↓
            Grounded Phrases + BBox
```

- 여기서, 기존 문장등의 text prompt에서 어떻게 Grounded phases를 찾느냐면!!  
  - 우선 Bert로 인코딩된 프롬포트랑  
  - stage1에서 bbox된 이미지 인코딩을 비교해서!  
  - 유사한 프롬포트 부분만 다시 토큰에서 단어로 바꾸면, 그게 Grounded phase이다!  

#### 🔄 Stage별 핵심 차이점

| 측면 | Faster R-CNN | GLIP |
|------|-------------|------|
| **Stage 1** | 🔍 "어디에 뭔가 있나?" | 🎯 "프롬프트 관련 객체가 어디에 있나?" |
| **입력** | 이미지만 | 이미지 + 텍스트 |
| **RPN 학습** | Closed-set (COCO 등) | Open-set (grounded pairs) |
| **Stage 2** | 🏷️ "이게 뭔가?" (고정 클래스) | 🔗 "프롬포트의 어떤 구문과 매칭되나?" |
| **분류 방식** | MLP → Softmax | Similarity Matching |
| **출력** | [class_id, bbox, conf] | [grounded_phrase, bbox, conf] |


#### 📌 GLIP 구성요소

| 구성요소 | 설명 | 목적 |
|----------|------|------|
| **텍스트 인코더** | BERT 기반 언어 모델 | 텍스트 쿼리에서 의미 특징 추출 |
| **이미지 인코더** | ResNet 또는 Swin Transformer | 이미지에서 시각 특징 추출 |
| **크로스모달 융합** | 멀티헤드 어텐션 레이어 | 텍스트와 시각 특징 정렬 |
| **탐지 헤드** | 분류 + 회귀 | 바운딩 박스와 신뢰도 예측 |

---

### 🔄 GLIP 학습 전략

#### 🎯 통합 손실 함수
```
L_total = L_detection + L_grounding + L_alignment

여기서:
- L_detection: 표준 객체 탐지 손실
- L_grounding: 구문 그라운딩 손실
- L_alignment: 비전-언어 정렬 손실
```

#### 📊 학습 데이터

| 데이터 타입 | 예시 | 목적 |
|-------------|------|------|
| **객체 탐지** | COCO, Objects365 | 바운딩 박스 회귀 학습 |
| **구문 그라운딩** | Flickr30K, Visual Genome | 텍스트-영역 정렬 학습 |
| **이미지-텍스트 쌍** | Conceptual Captions, LAION | 크로스모달 표현 학습 |

---

### 🧩 GLIP 성능 결과

#### 1. **제로샷 vs 파인튜닝 성능 비교**

| 모델 | Backbone | 사전학습 데이터 | 제로샷 COCO | 파인튜닝 COCO |
|------|----------|----------------|-------------|--------------|
| **기존 모델들** | | | | |
| Faster R-CNN | RN50-FPN | - | - | 40.2 |
| Faster R-CNN | RN101-FPN | - | - | 42.0 |
| DyHead-T | Swin-T | - | - | 49.7 |
| DyHead-L | Swin-L | - | - | 58.4 |
| **GLIP 모델들** | | | | |
| GLIP-T | Swin-T | O365 | **42.9** | **52.9** |
| GLIP-T | Swin-T | O365 | **44.9** | **53.8** |
| GLIP-T | Swin-T | O365+GoldG | **46.7** | **55.1** |
| GLIP-L | Swin-L | FourODs+GoldG+Cap24M | **49.8** | **60.8** |

**🚀 놀라운 결과**: GLIP-T 제로샷 성능이 기존 Faster R-CNN **파인튜닝 성능을 능가**!

#### 2. **다양한 데이터셋 제로샷 성능**

| 모델 | COCO AP | LVIS AP | ODinW (13개 데이터셋 평균) |
|------|---------|---------|---------------------------|
| CLIP + 탐지 헤드 | 12.1 | 8.3 | 15.7 |
| GLIP-T | 44.9 | 26.9 | 44.9 |
| GLIP-L | 49.8 | 31.8 | 51.4 |

#### 3. **퓨샷 학습**

| 샷 수 | COCO AP | LVIS AP |
|-------|---------|---------|
| 1샷 | 35.8 | 22.1 |
| 5샷 | 41.2 | 27.4 |
| 10샷 | 43.6 | 29.8 |

#### 4. **구문 그라운딩 결과**

| 데이터셋 | Recall@1 | Recall@5 | Recall@10 |
|----------|----------|----------|-----------|
| Flickr30K | 82.5 | 92.8 | 95.1 |
| RefCOCO | 78.9 | 87.6 | 91.2 |
| RefCOCO+ | 71.4 | 82.3 | 86.9 |

---

## 🧠 마무리 생각

> Yolo 쓰면서 Closed-set은 정말 불편하다고 생각했었는데!  
> 이렇게 SOTA OVOD인 GLIP은 참 대단한것 같습니다!!  

📝 GLIP 연구를 통해 배운 점:

- **언어는 컴퓨터 비전 과제에서 강력한 인터페이스**
- **다양한 데이터의 대규모 사전 학습**이 일반화에 중요
- **통합 프레임워크**가 분리된 모델보다 더 나은 성능 달성 가능

❗ 이 연구는 AI의 미래가 **멀티모달 이해**에 있음을 보여줍니다,  
비전과 언어가 매끄럽게 함께 작동하는 그런 미래 말이지요!

--- 