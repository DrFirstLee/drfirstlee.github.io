---
layout: post
title: "📝Understanding FG-CLip - FG-Clip 알아보기?!!"
author: [DrFirst]
date: 2025-06-11 07:00:00 +0900
categories: [AI, Research]
tags: [FG-CLIP, Fine Grained,  ICML, ICML 2025]
sitemap :
  changefreq : monthly
  priority : 0.8
---


---

## 🧠 (한국어) FG-CLIP 알아보기!  
_🔍 더 세세한 프롬포트도 가능한, 발전된 CLIP!!_

![Image]()

> 논문: [FG-CLIP: Fine-Grained Visual and Textual Alignment](https://arxiv.org/pdf/2505.05071)  
> 발표: ICML 2025   (Xie, Chunyu, et al.)  
> 코드: [360CVGroup/FG-CLIP](https://github.com/360CVGroup/FG-CLIP)  
> 코멘트: 


---


### 🧠 FG-CLIP 등장의 배경

- 기존 **CLIP**은 멀티모달 작업에서는 뛰어나지만,  
  **짧고 개략적인 캡션**에 의존하여 **세밀한 이해(fine-grained understanding)**가 부족함.  

#### 기존에 존재하던 CLIP 후속의 연구들!  

##### 🧠 1. CLIPSelf (ICLR 2024)
- **목표**: CLIP의 이미지 표현에 **자기지도 학습(self-supervised)**을 추가하여 성능 향상.  
- **핵심 기법**:  
  - 기존 CLIP의 구조 유지.  
  - 이미지 특징에서 **자기 예측(pretext task)**을 통해 정제된 표현 학습.  
- **장점**: 라벨 없이도 더 **정교하고 일반화된 시각 표현** 획득 가능.  
- **한계**: 텍스트 정보를 활용하지 않기 때문에 **시각-언어 연계 성능 향상에는 제한**이 있음.  
---

##### 🎯 2. FineCLIP (NeurIPS 2024)  
- **목표**: CLIP의 coarse한 텍스트에 기반한 한계를 극복하고, **fine-grained 시각-언어 표현** 강화.  
- **핵심 기법**:  
  - 객체 검출기를 이용해 **region-level 정보**와 CLIP 임베딩 정렬.  
  - 다단계 정렬 학습 (객체, 문장, 이미지 수준).  
- **장점**: **세부 객체 인식** 및 세밀한 텍스트 매핑 성능 강화.  
- **한계**: **객체 검출 성능과 주석 품질에 의존**하여 도메인 확장성과 일반화에 제약이 있음.  

---

##### 🔥 3. LLaVA (Large Language and Vision Assistant) (NeurIPS 2023)  
- **목표**: GPT 기반 LLM에 **시각 정보 해석 능력**을 부여한 멀티모달 어시스턴트 개발.  
- **핵심 기법**:  
  - CLIP Vision Encoder + LLM (예: Vicuna) 연결.  
  - **이미지 ↔ 자연어 대화**가 가능한 멀티모달 프롬프트 처리.  
- **장점**:  
  - **대화형 비전 이해 시스템** 구축 가능.  
  - ChatGPT 유사한 UX 제공 + 이미지 인식 기능 통합.  
- **한계**: **고품질 이미지-텍스트 alignment 데이터 의존**도가 높고, 실제 시각 reasoning 능력은 제한적임.  

---

##### 🧾 4. LongCLIP (ECCV 2024)  
- **목표**: CLIP의 짧은 텍스트 중심 한계를 극복하고, **장문 캡션 기반의 정교한 시각-언어 정렬** 실현.  
- **핵심 기법**:  
  - **대규모 장문 이미지-캡션 쌍**을 활용한 학습.  
  - CLIP의 텍스트 인코더를 장문 적합 구조로 확장.  
- **장점**:  
  - **스토리, 설명 중심 이미지** 이해에서 우수한 성능 발휘.  
  - Zero-shot 인식 및 문맥 이해 능력 향상.  
- **한계**: **장문 캡션 생성의 품질 편차**로 인해 정렬 학습의 노이즈가 발생할 수 있음.  



### 🚀 제안: FG-CLIP
세 가지 주요 혁신을 통해 세밀한 이해 성능 향상:

1. **장문 캡션 데이터 생성**
   - 대규모 멀티모달 모델을 활용해 **16억 장문 이미지-캡션 쌍** 생성  
   → 글로벌 의미 정보 포착

2. **고품질 영역별 주석 데이터셋**
   - **1,200만 개 이미지**  
   - **4,000만 개의 bounding box + 상세 캡션**  
   → 맥락 풍부한 정밀 표현 학습

3. **어려운 부정 샘플 도입**
   - **1,000만 개 fine-grained hard negatives** 포함  
   → 미세 의미 차이 구분 능력 향상


## 📊 주요 결과
- 새로운 데이터셋 **FineHARD** 구축
- 다양한 downstream task에서 기존 CLIP 및 SOTA 성능 초월:
  - Fine-grained 이해
  - Open-vocabulary 객체 탐지
  - 이미지-텍스트 검색
  - 일반 멀티모달 벤치마크