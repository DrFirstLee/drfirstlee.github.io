---
layout: post
title: "📝 Understanding LORA- LORA 알아보기?!!"
author: [DrFirst]
date: 2025-06-09 07:00:00 +0900
categories: [AI, Research]
tags: [LORA, fine-tuning, ICLR, ICLR 2022, Low-Rank Adaptation, Parameter Efficiency]
sitemap :
  changefreq : monthly
  priority : 0.8
---


---

## 🧠 (한국어) LISA: 추론 기반 세그멘테이션의 새로운 지평  
_🔍 복잡한 언어 지시를 이해하고, 이미지에서 해당 영역을 분할하는 혁신적인 모델!_

> 논문: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) 
> 발표: ICLR 2022   (Edward J. Hu et al. - Microsoft Research)  
> 코드: [microsoft/LORA](https://github.com/microsoft/LoRA)  
> 코멘트: LLM의 언어 이해 능력을 시각 분할에 접목한 획기적인 접근!


---

### 📌 요약

엄청 좋은 LLM 을 조금 수정하고싶을떄!!  
기존 방법들은 LLM만들때와 유사한 인프라를 가지고 미세 조정(fine-tuning)을 해야했습니다!!  
왜냐하면 기존 방식은 `full fine-tuning 방식`으로,  
수십억 개의 파라미터를 업데이트해야 했기 떄문입니다!  

하지만 LoRA는! 이런 문제를 해결하기위해 등장한 미세조정 기법으로!   
**훨씬 적은 파라미터만 추가 학습**하면서도 비슷하거나 더 좋은 성능을 달성합니다.

> 🎯 핵심 아이디어:  
> 👉 "기존 모델 파라미터는 그대로 고정하고, **저랭크 행렬(Low-Rank Matrices)**만 학습한다!"

---

### 🧠 LORA 등장의 배경

---

#### 📌 문제의식: 대규모 LLM의 한계

- 최근 언어 모델(GPT 등)은 수십억~수천억 개의 파라미터를 가지며, 이를 **전체 파인튜닝(fine-tuning)** 하는 것은 매우 비효율적  
- 태스크마다 별도로 모델 파라미터를 학습해야 하며, 이는 원본 파라미터 \( \Phi_0 \) 와 **동일한 크기**이기 때문에:
  - 💾 **저장 공간**: 태스크 수만큼 GPT-3 수준의 모델을 별도로 저장해야 함
  - 🚀 **배포/운영**: 모델 전환 비용이 커지고 실시간 서비스에 부적합
  - 💸 **학습 자원**: GPU 메모리와 연산비용이 과도하게 증가

---

#### 💡 기존 접근 방식의 한계

1. **어댑터 레이어 (Adapter Layers)**  
   - Transformer 블록 사이에 작은 병목 네트워크(bottleneck)를 삽입하여 적은 수의 파라미터만 학습
   - ✅ 장점: 적은 파라미터 학습   
   - ❌ 단점:
     - 어댑터 연산은 **순차적으로 수행**되므로 **추론 지연(latency)** 이 발생  
     - 실시간 온라인 환경(예: 배치 크기 1)에선 성능 저하 뚜렷  
     - 모델 병렬화(sharding) 환경에서 **통신 비용 증가**  

2. **프롬프트 기반 조정 (Prompt Tuning / Prefix Tuning)**  
   - 입력 토큰 앞에 학습 가능한 프롬프트를 삽입하여 조정
   - ✅ 장점: 모델 구조 변경 없음  
   - ❌ 단점:
     - 최적화가 **불안정**하고 성능이 **비선형적으로 변화**  
     - 프롬프트가 입력 길이를 차지해 **처리 가능한 시퀀스 길이 감소**  

---

#### 🚀 LoRA의 핵심 동기

- 위의 방식들은 효율성을 제공하지만, 실용성과 성능 간 **트레이드오프가 존재**
- **LoRA (Low-Rank Adaptation)** 는 다음의 관찰에서 출발함:
  - 대형 모델의 파인튜닝 시, 실제로 변경되는 파라미터의 변화는 **저차원 공간**에 존재함
- 따라서,
  - 전체 가중치 대신 **변화량(∆W)을 저랭크 행렬 \( A, B \) 로 분해**하여 학습
  - 사전학습된 가중치는 **고정(freeze)** 하여 효율적인 업데이트 가능
  - 결과적으로 **메모리·계산 자원 절감 + 성능 유지 + 추론 지연 없음**

---

### 🏗️ 방법론: Low-Rank Adaptation (LoRA)

#### 💡 기본 아이디어

모델의 일부 weight 행렬 `W`를 직접 업데이트하는 대신,  
아래과 같이 **저랭크 행렬의 곱으로 대체**  

```
W' = W + \Delta W = W + BA
```

- `A ∈ ℝ^{r×d}`  
- `B ∈ ℝ^{d×r}`  
- `r ≪ d`: 즉, 저랭크(rank-r) 구조  
- `W`는 고정(frozen), `A`, `B`만 학습

이렇게 하면 **훈련 파라미터 수와 연산량을 대폭 줄이면서도** 성능을 유지할 수 있음

---

### ⚙️ 적용 방식

LoRA는 주로 **Transformer 구조 내 Linear Layer**에 적용됩니다.

- Self-Attention의 Query, Key, Value projection에 적용  
- Feed-Forward Layer에도 적용 가능  
- 학습 후에는 `BA`를 `W`에 합쳐 Inference 시에는 원래 모델처럼 사용 가능

---

### 📈 실험 결과

- GPT, BERT, RoBERTa 등 다양한 모델에서 효과 확인  
- Full fine-tuning 대비 거의 동일한 성능 + 훨씬 적은 파라미터 학습
- 예: GPT-2 355M 기준, **1% 미만 파라미터만 업데이트**하여 SQuAD 성능 거의 동일

---

### 🧪 장점 요약

| 항목 | LoRA 방식 |
|------|-----------|
| ✅ 파라미터 효율 | 기존 대비 수백 배 적은 파라미터 |
| ✅ 메모리 절감 | 기존 대비 메모리 사용량 감소 |
| ✅ 성능 유지 | 거의 동일하거나 더 나은 성능 |
| ✅ 모듈성 | 다양한 pre-trained 모델에 쉽게 추가 가능 |

---

### 🔮 결론

LoRA는 **효율적인 LLM 적응을 위한 혁신적인 방법**입니다.  
특히 **모델 경량화, 빠른 실험, 분산 학습, 개인화** 등 다양한 활용 사례에 적합합니다.

> "LoRA는 LLM 미세조정의 새로운 패러다임을 제시하며,  
> 향후 다양한 Efficient Tuning 연구의 기반이 되었다."

---

### 📚 참고

- 공식 논문: [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)  
- 구현: [LoRA in HuggingFace PEFT](https://github.com/huggingface/peft)  
- 대표 활용: Alpaca, Vicuna, KoAlpaca, etc.
