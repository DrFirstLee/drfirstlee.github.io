---
layout: post
title: "ğŸ“ Understanding LORA- LORA ì•Œì•„ë³´ê¸°?!!"
author: [DrFirst]
date: 2025-06-09 07:00:00 +0900
categories: [AI, Research]
tags: [LORA, fine-tuning, ICLR, ICLR 2022, Low-Rank Adaptation, Parameter Efficiency]
sitemap :
  changefreq : monthly
  priority : 0.8
---


---

## ğŸ§  (í•œêµ­ì–´) LISA: ì¶”ë¡  ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ì˜ ìƒˆë¡œìš´ ì§€í‰  
_ğŸ” ë³µì¡í•œ ì–¸ì–´ ì§€ì‹œë¥¼ ì´í•´í•˜ê³ , ì´ë¯¸ì§€ì—ì„œ í•´ë‹¹ ì˜ì—­ì„ ë¶„í• í•˜ëŠ” í˜ì‹ ì ì¸ ëª¨ë¸!_

> ë…¼ë¬¸: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) 
> ë°œí‘œ: ICLR 2022   (Edward J. Hu et al. - Microsoft Research)  
> ì½”ë“œ: [microsoft/LORA](https://github.com/microsoft/LoRA)  
> ì½”ë©˜íŠ¸: LLMì˜ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥ì„ ì‹œê° ë¶„í• ì— ì ‘ëª©í•œ íšê¸°ì ì¸ ì ‘ê·¼!


---

### ğŸ“Œ ìš”ì•½

ì—„ì²­ ì¢‹ì€ LLM ì„ ì¡°ê¸ˆ ìˆ˜ì •í•˜ê³ ì‹¶ì„ë–„!!  
ê¸°ì¡´ ë°©ë²•ë“¤ì€ LLMë§Œë“¤ë•Œì™€ ìœ ì‚¬í•œ ì¸í”„ë¼ë¥¼ ê°€ì§€ê³  ë¯¸ì„¸ ì¡°ì •(fine-tuning)ì„ í•´ì•¼í–ˆìŠµë‹ˆë‹¤!!  
ì™œëƒí•˜ë©´ ê¸°ì¡´ ë°©ì‹ì€ `full fine-tuning ë°©ì‹`ìœ¼ë¡œ,  
ìˆ˜ì‹­ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•´ì•¼ í–ˆê¸° ë–„ë¬¸ì…ë‹ˆë‹¤!  

í•˜ì§€ë§Œ LoRAëŠ”! ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸°ìœ„í•´ ë“±ì¥í•œ ë¯¸ì„¸ì¡°ì • ê¸°ë²•ìœ¼ë¡œ!   
**í›¨ì”¬ ì ì€ íŒŒë¼ë¯¸í„°ë§Œ ì¶”ê°€ í•™ìŠµ**í•˜ë©´ì„œë„ ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.

> ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´:  
> ğŸ‘‰ "ê¸°ì¡´ ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” ê·¸ëŒ€ë¡œ ê³ ì •í•˜ê³ , **ì €ë­í¬ í–‰ë ¬(Low-Rank Matrices)**ë§Œ í•™ìŠµí•œë‹¤!"

---

### ğŸ§  LORA ë“±ì¥ì˜ ë°°ê²½

---

#### ğŸ“Œ ë¬¸ì œì˜ì‹: ëŒ€ê·œëª¨ LLMì˜ í•œê³„

- ìµœê·¼ ì–¸ì–´ ëª¨ë¸(GPT ë“±)ì€ ìˆ˜ì‹­ì–µ~ìˆ˜ì²œì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ë©°, ì´ë¥¼ **ì „ì²´ íŒŒì¸íŠœë‹(fine-tuning)** í•˜ëŠ” ê²ƒì€ ë§¤ìš° ë¹„íš¨ìœ¨ì   
- íƒœìŠ¤í¬ë§ˆë‹¤ ë³„ë„ë¡œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•´ì•¼ í•˜ë©°, ì´ëŠ” ì›ë³¸ íŒŒë¼ë¯¸í„° \( \Phi_0 \) ì™€ **ë™ì¼í•œ í¬ê¸°**ì´ê¸° ë•Œë¬¸ì—:
  - ğŸ’¾ **ì €ì¥ ê³µê°„**: íƒœìŠ¤í¬ ìˆ˜ë§Œí¼ GPT-3 ìˆ˜ì¤€ì˜ ëª¨ë¸ì„ ë³„ë„ë¡œ ì €ì¥í•´ì•¼ í•¨
  - ğŸš€ **ë°°í¬/ìš´ì˜**: ëª¨ë¸ ì „í™˜ ë¹„ìš©ì´ ì»¤ì§€ê³  ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ì— ë¶€ì í•©
  - ğŸ’¸ **í•™ìŠµ ìì›**: GPU ë©”ëª¨ë¦¬ì™€ ì—°ì‚°ë¹„ìš©ì´ ê³¼ë„í•˜ê²Œ ì¦ê°€

---

#### ğŸ’¡ ê¸°ì¡´ ì ‘ê·¼ ë°©ì‹ì˜ í•œê³„

1. **ì–´ëŒ‘í„° ë ˆì´ì–´ (Adapter Layers)**  
   - Transformer ë¸”ë¡ ì‚¬ì´ì— ì‘ì€ ë³‘ëª© ë„¤íŠ¸ì›Œí¬(bottleneck)ë¥¼ ì‚½ì…í•˜ì—¬ ì ì€ ìˆ˜ì˜ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ
   - âœ… ì¥ì : ì ì€ íŒŒë¼ë¯¸í„° í•™ìŠµ   
   - âŒ ë‹¨ì :
     - ì–´ëŒ‘í„° ì—°ì‚°ì€ **ìˆœì°¨ì ìœ¼ë¡œ ìˆ˜í–‰**ë˜ë¯€ë¡œ **ì¶”ë¡  ì§€ì—°(latency)** ì´ ë°œìƒ  
     - ì‹¤ì‹œê°„ ì˜¨ë¼ì¸ í™˜ê²½(ì˜ˆ: ë°°ì¹˜ í¬ê¸° 1)ì—ì„  ì„±ëŠ¥ ì €í•˜ ëšœë ·  
     - ëª¨ë¸ ë³‘ë ¬í™”(sharding) í™˜ê²½ì—ì„œ **í†µì‹  ë¹„ìš© ì¦ê°€**  

2. **í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì¡°ì • (Prompt Tuning / Prefix Tuning)**  
   - ì…ë ¥ í† í° ì•ì— í•™ìŠµ ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚½ì…í•˜ì—¬ ì¡°ì •
   - âœ… ì¥ì : ëª¨ë¸ êµ¬ì¡° ë³€ê²½ ì—†ìŒ  
   - âŒ ë‹¨ì :
     - ìµœì í™”ê°€ **ë¶ˆì•ˆì •**í•˜ê³  ì„±ëŠ¥ì´ **ë¹„ì„ í˜•ì ìœ¼ë¡œ ë³€í™”**  
     - í”„ë¡¬í”„íŠ¸ê°€ ì…ë ¥ ê¸¸ì´ë¥¼ ì°¨ì§€í•´ **ì²˜ë¦¬ ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤ ê¸¸ì´ ê°ì†Œ**  

---

#### ğŸš€ LoRAì˜ í•µì‹¬ ë™ê¸°

- ìœ„ì˜ ë°©ì‹ë“¤ì€ íš¨ìœ¨ì„±ì„ ì œê³µí•˜ì§€ë§Œ, ì‹¤ìš©ì„±ê³¼ ì„±ëŠ¥ ê°„ **íŠ¸ë ˆì´ë“œì˜¤í”„ê°€ ì¡´ì¬**
- **LoRA (Low-Rank Adaptation)** ëŠ” ë‹¤ìŒì˜ ê´€ì°°ì—ì„œ ì¶œë°œí•¨:
  - ëŒ€í˜• ëª¨ë¸ì˜ íŒŒì¸íŠœë‹ ì‹œ, ì‹¤ì œë¡œ ë³€ê²½ë˜ëŠ” íŒŒë¼ë¯¸í„°ì˜ ë³€í™”ëŠ” **ì €ì°¨ì› ê³µê°„**ì— ì¡´ì¬í•¨
- ë”°ë¼ì„œ,
  - ì „ì²´ ê°€ì¤‘ì¹˜ ëŒ€ì‹  **ë³€í™”ëŸ‰(âˆ†W)ì„ ì €ë­í¬ í–‰ë ¬ \( A, B \) ë¡œ ë¶„í•´**í•˜ì—¬ í•™ìŠµ
  - ì‚¬ì „í•™ìŠµëœ ê°€ì¤‘ì¹˜ëŠ” **ê³ ì •(freeze)** í•˜ì—¬ íš¨ìœ¨ì ì¸ ì—…ë°ì´íŠ¸ ê°€ëŠ¥
  - ê²°ê³¼ì ìœ¼ë¡œ **ë©”ëª¨ë¦¬Â·ê³„ì‚° ìì› ì ˆê° + ì„±ëŠ¥ ìœ ì§€ + ì¶”ë¡  ì§€ì—° ì—†ìŒ**

---

### ğŸ—ï¸ ë°©ë²•ë¡ : Low-Rank Adaptation (LoRA)

#### ğŸ’¡ ê¸°ë³¸ ì•„ì´ë””ì–´

ëª¨ë¸ì˜ ì¼ë¶€ weight í–‰ë ¬ `W`ë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸í•˜ëŠ” ëŒ€ì‹ ,  
ì•„ë˜ê³¼ ê°™ì´ **ì €ë­í¬ í–‰ë ¬ì˜ ê³±ìœ¼ë¡œ ëŒ€ì²´**  

```
W' = W + \Delta W = W + BA
```

- `A âˆˆ â„^{rÃ—d}`  
- `B âˆˆ â„^{dÃ—r}`  
- `r â‰ª d`: ì¦‰, ì €ë­í¬(rank-r) êµ¬ì¡°  
- `W`ëŠ” ê³ ì •(frozen), `A`, `B`ë§Œ í•™ìŠµ

ì´ë ‡ê²Œ í•˜ë©´ **í›ˆë ¨ íŒŒë¼ë¯¸í„° ìˆ˜ì™€ ì—°ì‚°ëŸ‰ì„ ëŒ€í­ ì¤„ì´ë©´ì„œë„** ì„±ëŠ¥ì„ ìœ ì§€í•  ìˆ˜ ìˆìŒ

---

### âš™ï¸ ì ìš© ë°©ì‹

LoRAëŠ” ì£¼ë¡œ **Transformer êµ¬ì¡° ë‚´ Linear Layer**ì— ì ìš©ë©ë‹ˆë‹¤.

- Self-Attentionì˜ Query, Key, Value projectionì— ì ìš©  
- Feed-Forward Layerì—ë„ ì ìš© ê°€ëŠ¥  
- í•™ìŠµ í›„ì—ëŠ” `BA`ë¥¼ `W`ì— í•©ì³ Inference ì‹œì—ëŠ” ì›ë˜ ëª¨ë¸ì²˜ëŸ¼ ì‚¬ìš© ê°€ëŠ¥

---

### ğŸ“ˆ ì‹¤í—˜ ê²°ê³¼

- GPT, BERT, RoBERTa ë“± ë‹¤ì–‘í•œ ëª¨ë¸ì—ì„œ íš¨ê³¼ í™•ì¸  
- Full fine-tuning ëŒ€ë¹„ ê±°ì˜ ë™ì¼í•œ ì„±ëŠ¥ + í›¨ì”¬ ì ì€ íŒŒë¼ë¯¸í„° í•™ìŠµ
- ì˜ˆ: GPT-2 355M ê¸°ì¤€, **1% ë¯¸ë§Œ íŒŒë¼ë¯¸í„°ë§Œ ì—…ë°ì´íŠ¸**í•˜ì—¬ SQuAD ì„±ëŠ¥ ê±°ì˜ ë™ì¼

---

### ğŸ§ª ì¥ì  ìš”ì•½

| í•­ëª© | LoRA ë°©ì‹ |
|------|-----------|
| âœ… íŒŒë¼ë¯¸í„° íš¨ìœ¨ | ê¸°ì¡´ ëŒ€ë¹„ ìˆ˜ë°± ë°° ì ì€ íŒŒë¼ë¯¸í„° |
| âœ… ë©”ëª¨ë¦¬ ì ˆê° | ê¸°ì¡´ ëŒ€ë¹„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ |
| âœ… ì„±ëŠ¥ ìœ ì§€ | ê±°ì˜ ë™ì¼í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ |
| âœ… ëª¨ë“ˆì„± | ë‹¤ì–‘í•œ pre-trained ëª¨ë¸ì— ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥ |

---

### ğŸ”® ê²°ë¡ 

LoRAëŠ” **íš¨ìœ¨ì ì¸ LLM ì ì‘ì„ ìœ„í•œ í˜ì‹ ì ì¸ ë°©ë²•**ì…ë‹ˆë‹¤.  
íŠ¹íˆ **ëª¨ë¸ ê²½ëŸ‰í™”, ë¹ ë¥¸ ì‹¤í—˜, ë¶„ì‚° í•™ìŠµ, ê°œì¸í™”** ë“± ë‹¤ì–‘í•œ í™œìš© ì‚¬ë¡€ì— ì í•©í•©ë‹ˆë‹¤.

> "LoRAëŠ” LLM ë¯¸ì„¸ì¡°ì •ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•˜ë©°,  
> í–¥í›„ ë‹¤ì–‘í•œ Efficient Tuning ì—°êµ¬ì˜ ê¸°ë°˜ì´ ë˜ì—ˆë‹¤."

---

### ğŸ“š ì°¸ê³ 

- ê³µì‹ ë…¼ë¬¸: [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)  
- êµ¬í˜„: [LoRA in HuggingFace PEFT](https://github.com/huggingface/peft)  
- ëŒ€í‘œ í™œìš©: Alpaca, Vicuna, KoAlpaca, etc.
