---
layout: post
title: "π“ LoRA: Low-Rank Fine-Tuning for Large Language Models - Understanding LORA- LORA μ•μ•„λ³΄κΈ°?!!"
author: [DrFirst]
date: 2025-06-09 07:00:00 +0900
categories: [AI, Research]
tags: [LORA, fine-tuning, ICLR, ICLR 2022, Low-Rank Adaptation, Parameter Efficiency]
sitemap :
  changefreq : monthly
  priority : 0.8
---


---

## π§  LoRA: Low-Rank Fine-Tuning for Large Language Models  
_π” Lightweight and Fast! A New Paradigm for Efficient Fine-Tuning_

> **Paper**: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)  
> **Conference**: ICLR 2022 (Edward J. Hu et al., Microsoft Research)  
> **Code**: [microsoft/LoRA](https://github.com/microsoft/LoRA)  
> **Comment**: A groundbreaking method that brings language model efficiency into new domains!

---

### π“ Summary

Want to fine-tune a large LLM... but without the massive GPU cost?

Traditionally, fine-tuning meant retraining **every parameter** in the model β€” which could mean **billions of weights**, just like training from scratch.

**LoRA** solves this by enabling effective fine-tuning while learning **only a tiny fraction of the parameters**, often achieving **comparable or even better performance**.

> π― Core Idea:  
> π‘‰ "Keep the original model weights frozen. Train only a small, lightweight module (low-rank matrices) added alongside."

---

### π§  Why LoRA?

#### π“ The Challenge of Full Fine-Tuning in LLMs

- Modern LLMs like GPT-3 have hundreds of billions of parameters
- Fine-tuning every parameter is:
  - π’Ύ **Storage-heavy**: Each task needs a full model copy  
  - π€ **Deployment-unfriendly**: Task switching is slow and heavy  
  - π’Έ **Expensive**: Requires huge compute and memory

---

#### π’΅ Limitations of Previous Methods

1. **Adapter Layers**  
   - Inserts bottleneck networks into the Transformer blocks  
   - β… Efficient in parameter count  
   - β But adds **latency**, especially problematic in online inference or sharded deployments

2. **Prompt/Prefix Tuning**  
   - Adds trainable tokens to the input sequence  
   - β… Keeps the model architecture unchanged  
   - β Suffers from **optimization instability**, and reduces the **usable sequence length**

---

#### π€ Motivation Behind LoRA

LoRA is based on the observation that **parameter updates during fine-tuning lie in a low-dimensional space**.

Thus:
- Instead of updating full weight matrices,
- LoRA learns a low-rank update:  
  \[
  \Delta W = B A
  \]
- Only matrices **A and B** are trainable; the base model is **frozen**

β… Result:  
**Less memory, fewer FLOPs, and no inference slowdown!**

---

### π—οΈ How LoRA Works

#### π’΅ Low-Rank Update Parameterization

```math
W' = W + \Delta W = W + B A
```
---

- `A β β„^{rΓ—d}` : initialized with random Gaussian  
- `B β β„^{dΓ—r}` : initialized with zeros  
- `r β‰ d` β†’ low-rank structure  
- The base weight `W` is frozen β€” only `A` and `B` are trained.

This setup allows **dramatic reduction in trainable parameters and FLOPs**, while maintaining speed and performance!

### π¤” But how small can `r` go?

Smaller `r` means less resource usage β€” but is it still effective?

This was explored in **Section 7.2: What is the Optimal Rank r for LoRA?**

> β… Result: Even **`r = 1`** yields surprisingly strong performance!  
> β… LoRA with `r = 8` and `r = 64` was compared using vector subspace similarity, and the overlap was high!

![Subspace Overlap](https://github.com/user-attachments/assets/2a17d563-8087-498d-9ced-044d8131013b)

---

### β–¶οΈ Forward Pass Modification

- Original: `h = Wβ‚€ Γ— x`  
- With LoRA: `h = Wβ‚€ Γ— x + B A Γ— x`

β†’ The two outputs are added element-wise (same dimensions).  
β†’ This allows LoRA to introduce updates **without altering architecture**.

---

### π§  How LoRA is Applied to Transformers

#### π”§ Target Weight Matrices

- **In Self-Attention Modules**:
  - \( W_q \): Query
  - \( W_k \): Key
  - \( W_v \): Value
  - \( W_o \): Output

- **In MLP Modules**: two Dense layers

> In experiments, W_q, W_k, W_v  are treated as unified square matrices  
> (even though in practice they are divided across attention heads)

> Most commonly, LoRA is applied to Wq and Wv.  
> See Section 7.2 for ablations on rank selection and subspace behavior:

![LoRA Rank Ablation](https://github.com/user-attachments/assets/113caa55-a1d6-419e-a729-cc9d5ec02e6a)

---

#### β™οΈ Training Strategy

- Only attention weight matrices are trained with LoRA  
- MLP, LayerNorm, and bias parameters are **frozen**

β†’ Simple and highly parameter-efficient.

---

### β… Practical Benefits of LoRA

- **Memory Efficiency**:
  - GPT-3 175B full fine-tuning: **1.2TB**
  - LoRA fine-tuning: **350GB**

- **Checkpoint Size Reduction**:
  - With `r = 4`, training only Q/V β†’ **350GB β†’ 35MB** (~10,000Γ— smaller)

- **Trainable on modest hardware**
  - Avoids I/O bottlenecks

- **Low-cost Task Switching**
  - Just swap LoRA modules instead of the entire model

- **25% Faster Training**
  - Most parameters are frozen β€” gradients are computed only for low-rank matrices

---

### β οΈ Limitations

- If you **merge `B A` into `W`** to avoid runtime overhead:
  - Itβ€™s difficult to batch tasks with different LoRA modules
- However, when latency is not critical:
  - You can keep LoRA unmerged and **dynamically swap modules per sample**

---

### π€ LoRA in Empirical Evaluation

This work compares LoRA against several fine-tuning methods:

- **Full Fine-Tuning (FT)**  
  Trains **all parameters**. Standard method but memory-heavy.

- **BitFit (Bias-only Tuning)**  
  Trains only **bias vectors** β€” very light, but limited capacity.

- **Prefix Tuning (PreEmbed)**  
  Adds trainable tokens to the input β€” only embeddings are trained.

- **Prefix Layer Tuning (PreLayer)**  
  Learns **intermediate activations** at each layer β€” more expressive.

- **Adapter Tuning**  
  Adds small MLP "adapters" to each layer β€” multiple variants (AdapterH, AdapterL, etc.)

- **LoRA (Low-Rank Adaptation)**  
  Adds parallel low-rank matrices to attention weights β€” maintains full inference speed  
  while dramatically reducing memory and parameter size.

---

### π“ Result?

> **LoRA achieves great performance while training far fewer parameters!**

![Performance Graph](https://github.com/user-attachments/assets/3824b21c-a1cf-44ef-aef6-f7673c8dc483)

- On the **GLUE benchmark (NLU)**, LoRA matches or outperforms full FT on RoBERTa/DeBERTa
- On **GPT-2 generation tasks (WikiSQL, SAMSum)**, LoRA outperforms Prefix Tuning
- On **GPT-3 175B**, LoRA trains on **350GB VRAM** β€” while full FT would be infeasible

---

### π”® Conclusion

**LoRA is a breakthrough method for fine-tuning large Transformer models** β€” from LLMs to ViTs to DETR.

It enables:
- β΅ Lightweight adaptation  
- π§ Rapid experimentation  
- π Efficient deployment  
- π¤– Personalized AI at scale



---

## π§  (ν•κµ­μ–΄) LORA : LLMμ„ μ„ν• μ €λ­ν¬ νμΈνλ‹ κΈ°λ²•  
_π” κ°€λ³κ³  λΉ λ¥΄κ²!! Fine-tuningμ μƒλ΅μ΄ λ°©λ²•λ΅  μ μ‹!_

![Image](https://github.com/user-attachments/assets/b151d896-4c10-420f-ad97-0e14262f07ee)

> λ…Όλ¬Έ: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)  
> λ°ν‘: ICLR 2022   (Edward J. Hu et al. - Microsoft Research)  
> μ½”λ“: [microsoft/LORA](https://github.com/microsoft/LoRA)  
> μ½”λ©νΈ: LLMμ μ–Έμ–΄ μ΄ν•΄ λ¥λ ¥μ„ μ‹κ° λ¶„ν• μ— μ ‘λ©ν• νκΈ°μ μΈ μ ‘κ·Ό!


---

### π“ μ”μ•½

μ—„μ²­ μΆ‹μ€ LLM μ„ μ΅°κΈ μμ •ν•κ³ μ‹¶μ„λ–„!!  
κΈ°μ΅΄ λ°©λ²•λ“¤μ€ LLMλ§λ“¤λ•μ™€ μ μ‚¬ν• μΈν”„λΌλ¥Ό κ°€μ§€κ³  λ―Έμ„Έ μ΅°μ •(fine-tuning)μ„ ν•΄μ•Όν–μµλ‹λ‹¤!!  
μ™λƒν•λ©΄ κΈ°μ΅΄ λ°©μ‹μ€ `full fine-tuning λ°©μ‹`μΌλ΅,  
μμ‹­μ–µ κ°μ νλΌλ―Έν„°λ¥Ό μ—…λ°μ΄νΈν•΄μ•Ό ν–κΈ° λ–„λ¬Έμ…λ‹λ‹¤!  

ν•μ§€λ§ LoRAλ”! μ΄λ° λ¬Έμ λ¥Ό ν•΄κ²°ν•κΈ°μ„ν•΄ λ“±μ¥ν• λ―Έμ„Έμ΅°μ • κΈ°λ²•μΌλ΅!   
**ν›¨μ”¬ μ μ€ νλΌλ―Έν„°λ§ μ¶”κ°€ ν•™μµ**ν•λ©΄μ„λ„ λΉ„μ·ν•κ±°λ‚ λ” μΆ‹μ€ μ„±λ¥μ„ λ‹¬μ„±ν•©λ‹λ‹¤.

> π― ν•µμ‹¬ μ•„μ΄λ””μ–΄!!  
> π‘‰ "κΈ°μ΅΄ λ¨λΈ κ°€μ¤‘μΉλ” κ³ μ •ν•κ³ , **λ’·λ¶€λ¶„μ— μ¶”κ°€λ λ‹¤λ¥Έ λ¶€λ¶„**(μ €λ­ν¬ ν–‰λ ¬, Low-Rank Matrices)λ§ ν•™μµν•λ‹¤!"

---

### π§  LORA λ“±μ¥μ λ°°κ²½


#### π“ λ¬Έμ μμ‹: λ€κ·λ¨ LLMμ ν•κ³„

- μµκ·Ό μ–Έμ–΄ λ¨λΈ(GPT λ“±)μ€ μμ‹­μ–µ~μμ²μ–µ κ°μ νλΌλ―Έν„°λ¥Ό κ°€μ§€λ©°, μ΄λ¥Ό **μ „μ²΄ νμΈνλ‹(fine-tuning)** ν•λ” κ²ƒμ€ λ§¤μ° λΉ„ν¨μ¨μ   
- νƒμ¤ν¬λ§λ‹¤ λ³„λ„λ΅ λ¨λΈ νλΌλ―Έν„°λ¥Ό ν•™μµν•΄μ•Ό ν•λ©°, μ΄λ” μ›λ³Έ νλΌλ―Έν„° \( \Phi_0 \) μ™€ **λ™μΌν• ν¬κΈ°**μ΄κΈ° λ•λ¬Έμ—:
  - π’Ύ **μ €μ¥ κ³µκ°„**: νƒμ¤ν¬ μλ§νΌ GPT-3 μμ¤€μ λ¨λΈμ„ λ³„λ„λ΅ μ €μ¥ν•΄μ•Ό ν•¨
  - π€ **λ°°ν¬/μ΄μ**: λ¨λΈ μ „ν™ λΉ„μ©μ΄ μ»¤μ§€κ³  μ‹¤μ‹κ°„ μ„λΉ„μ¤μ— λ¶€μ ν•©
  - π’Έ **ν•™μµ μμ›**: GPU λ©”λ¨λ¦¬μ™€ μ—°μ‚°λΉ„μ©μ΄ κ³Όλ„ν•κ² μ¦κ°€

---

#### π’΅ κΈ°μ΅΄ μ ‘κ·Ό λ°©μ‹μ ν•κ³„

1. **μ–΄λ‘ν„° λ μ΄μ–΄ (Adapter Layers)**  
   - Transformer λΈ”λ΅ μ‚¬μ΄μ— μ‘μ€ λ³‘λ© λ„¤νΈμ›ν¬(bottleneck)λ¥Ό μ‚½μ…ν•μ—¬ μ μ€ μμ νλΌλ―Έν„°λ§ ν•™μµ
   - β… μ¥μ : μ μ€ νλΌλ―Έν„° ν•™μµ   
   - β λ‹¨μ :
     - μ–΄λ‘ν„° μ—°μ‚°μ€ **μμ°¨μ μΌλ΅ μν–‰**λλ―€λ΅ **μ¶”λ΅  μ§€μ—°(latency)** μ΄ λ°μƒ  
     - μ‹¤μ‹κ°„ μ¨λΌμΈ ν™κ²½(μ: λ°°μΉ ν¬κΈ° 1)μ—μ„  μ„±λ¥ μ €ν• λλ ·  
     - λ¨λΈ λ³‘λ ¬ν™”(sharding) ν™κ²½μ—μ„ **ν†µμ‹  λΉ„μ© μ¦κ°€**  

2. **ν”„λ΅¬ν”„νΈ κΈ°λ° μ΅°μ • (Prompt Tuning / Prefix Tuning)**  
   - μ…λ ¥ ν† ν° μ•μ— ν•™μµ κ°€λ¥ν• ν”„λ΅¬ν”„νΈλ¥Ό μ‚½μ…ν•μ—¬ μ΅°μ •
   - β… μ¥μ : λ¨λΈ κµ¬μ΅° λ³€κ²½ μ—†μ  
   - β λ‹¨μ :
     - μµμ ν™”κ°€ **λ¶μ•μ •**ν•κ³  μ„±λ¥μ΄ **λΉ„μ„ ν•μ μΌλ΅ λ³€ν™”**  
     - ν”„λ΅¬ν”„νΈκ°€ μ…λ ¥ κΈΈμ΄λ¥Ό μ°¨μ§€ν•΄ **μ²λ¦¬ κ°€λ¥ν• μ‹ν€€μ¤ κΈΈμ΄ κ°μ†**  

---

#### π€ LoRAμ ν•µμ‹¬ λ™κΈ°

- μ„μ λ°©μ‹λ“¤μ€ ν¨μ¨μ„±μ„ μ κ³µν•μ§€λ§, μ‹¤μ©μ„±κ³Ό μ„±λ¥ κ°„ **νΈλ μ΄λ“μ¤ν”„κ°€ μ΅΄μ¬**
- **LoRA (Low-Rank Adaptation)** λ” λ‹¤μμ κ΄€μ°°μ—μ„ μ¶λ°ν•¨:
  - λ€ν• λ¨λΈμ νμΈνλ‹ μ‹, μ‹¤μ λ΅ λ³€κ²½λλ” νλΌλ―Έν„°μ λ³€ν™”λ” **μ €μ°¨μ› κ³µκ°„**μ— μ΅΄μ¬ν•¨
- λ”°λΌμ„,
  - μ „μ²΄ κ°€μ¤‘μΉ λ€μ‹  **λ³€ν™”λ‰(β†W)μ„ μ €λ­ν¬ ν–‰λ ¬ \( A, B \) λ΅ λ¶„ν•΄**ν•μ—¬ ν•™μµ
  - μ‚¬μ „ν•™μµλ κ°€μ¤‘μΉλ” **κ³ μ •(freeze)** ν•μ—¬ ν¨μ¨μ μΈ μ—…λ°μ΄νΈ κ°€λ¥
  - κ²°κ³Όμ μΌλ΅ **λ©”λ¨λ¦¬Β·κ³„μ‚° μμ› μ κ° + μ„±λ¥ μ μ§€ + μ¶”λ΅  μ§€μ—° μ—†μ**

---

### π—οΈ λ°©λ²•λ΅ : Low-Rank Adaptation (LoRA)

#### π’΅ Low-Rank-Parametrized Update Matrices (μ €λ­ν¬μ ν–‰λ ¬ μ—…λ°μ΄νΈ)

![Image](https://github.com/user-attachments/assets/96e90b1a-c457-4ee2-9b4a-c75dd12f7fe9)

λ¨λΈμ weight ν–‰λ ¬ `W`λ¥Ό μ§μ ‘ μ—…λ°μ΄νΈν•λ” λ€μ‹ ,  
μ•„λκ³Ό κ°™μ΄ **μ €λ­ν¬ ν–‰λ ¬μ κ³±μΌλ΅ λ€μ²΄**  

```
W' = W + Ξ”W = W + BA
```

- `A β β„^{rΓ—d}`   : μ •κ·λ¶„ν¬λ΅ μ΄κΈ°ν™”    
- `B β β„^{dΓ—r}`   : μ²μμ—” 0μΌλ΅ μ„¤μ •  
- `r β‰ d`: μ¦‰, μ €λ­ν¬(rank-r) κµ¬μ΅°  
- `W`λ” κ³ μ •(frozen), `A`, `B`λ§ ν•™μµ

μ΄λ ‡κ² ν•¨μΌλ΅μ¨!  
**ν›λ ¨ νλΌλ―Έν„° μμ™€ μ—°μ‚°λ‰μ„ λ€ν­ μ¤„μ΄κ³ ** μ†λ„&μ„±λ¥λ„ μ μ§€!!  

μ¶”κ°€λ΅! κ·ΈλΌ μ–΄λ–¤ μ°¨μ›(r)μΌλ΅ λ‚®μ¶”λ”κ²ƒ κΉμ§€ κ°€λ¥ν• κΉ?  
λ‚®μΌλ©΄ λ‚®μ„μλ΅ λ¦¬μ†μ¤λ” μ κ²λ“¤μ§€λ§ ν•™μµμ΄ λ κΉ κ±±μ •λκΈ°μ—!~  
ν•΄λ‹Ή κ³ λ―Όλ„ μ—°κµ¬μ λ’·λ¶€λ¶„  `7.2 WHAT IS THE OPTIMAL RANK r FOR LORA?`  μ— λ‚μ™”μµλ‹λ‹¤!!
κ²°λ΅ λ§ λ§ν•λ©΄ rμ΄ 1μΌλ–„λ„ κ½¤ μ„±λ¥μ΄ κ΄μ°®μ•λ°μ”!  
> λν• r=8 μ΄λ‘ r=64μΌλ–„μ λ²΅ν„°λ¥Ό κµ¬ν•΄μ„ μ–Όλ§λ‚ κ²ΉμΉλ”μ§€λ¥Ό μ‹κ°ν™”ν–λ”λ°, λ§μ΄ κ²ΉμΉλ”κ²ƒμ„ ν™•μΈν–λ€μ”!  
![Image](https://github.com/user-attachments/assets/2a17d563-8087-498d-9ced-044d8131013b) 


#### β–¶οΈ Forward Pass μμ • (κ²°κ³Όκ°‘ μμΈ΅ λ°©λ²• μμ •)  

- κΈ°μ΅΄:  `h = W_0 * x`
- LoRA μ μ© ν›„: ` h = W_0 * x + BA * x`  
  οΈβ†’ λ™μΌ μ…λ ¥μ— λ€ν•΄ λ‘ μ¶λ ¥ κ³„μ‚° ν›„ **μΆν‘λ³„ ν•©μ‚°**  
  (`W_0 * x` μ™€ `BA * x`λ” κ°™μ€ μ°¨μ›μ λ²΅ν„°λ΅ λ”ν•κΈ°κ°€ κ°€λ¥μ“°!)

---

### Transformerμ— LoRAλ¥Ό μ μ©ν•λ©΄@?  


#### π”§ μ μ© λ€μƒ

- **Self-Attention λ¨λ“** λ‚΄ κ°€μ¤‘μΉ ν–‰λ ¬:
  - \( W_q \): Query
  - \( W_k \): Key
  - \( W_v \): Value
  - \( W_o \): Output

- **MLP λ¨λ“**μ—λ” Dense Layer 2κ° μ΅΄μ¬

> μ‹¤ν—μ—μ„λ” W_q, W_k, W_v λ“¤μ„ λ‹¨μΌ ν–‰λ ¬λ΅ μ·¨κΈ‰  
> (μ‹¤μ λ΅λ” μ—¬λ¬ attention headλ΅ λ¶„ν• λμ§€λ§,, λ‹¨μν™”λ¥Ό μ„ν•μ—¬!!)  
> LoRA λ…Όλ¬Έμ—μ„λ” μ‹¤ν—μ μΌλ΅ λ‹¤μ–‘ν• μ΅°ν•©μ„ ν…μ¤νΈν–κ³ , Wqμ™€ Wvμ— μ μ©ν•λ”κ²ƒμ΄ λ€ν‘μ μ„!  

> `7.2 WHAT IS THE OPTIMAL RANK r FOR LORA?` μ—μ„ ν•΄λ‹Ή μ‹¤ν—λ‚΄μ©μ„ λ³Όμ μμ§€μ”~~  
![Image](https://github.com/user-attachments/assets/113caa55-a1d6-419e-a729-cc9d5ec02e6a)

---

#### β™οΈ μ‹¤ν— μ „λµ

- **Attention weightsλ§ LoRAλ΅ ν•™μµ**  
- **MLP, LayerNorm, Biasλ” λ¨λ‘ κ³ μ •(freeze)**  
β†’ κ°„λ‹¨ν•κ³  νλΌλ―Έν„° ν¨μ¨μ 

---

#### β… LoRAμ μ‹¤μ©μ  μ΄μ 

- **λ©”λ¨λ¦¬ μ κ°**:  
  - GPT-3 175B κΈ°μ¤€ VRAM μ‚¬μ©λ‰  
    β†’ μ „μ²΄ νμΈνλ‹: 1.2TB β†’ LoRA: 350GB  
- **μ²΄ν¬ν¬μΈνΈ ν¬κΈ° κ°μ†**:  
  - \( r = 4 \), Q/V projectionλ§ ν•™μµ μ‹  
  - 350GB β†’ 35MB (μ•½ 10,000λ°° μ¶•μ†)
- **μ μ€ GPUλ΅λ„ ν•™μµ κ°€λ¥**  
  - I/O λ³‘λ© μ™„ν™”
- **νƒμ¤ν¬ μ „ν™ λΉ„μ©β†“**  
  - μ „μ²΄ λ¨λΈ κµμ²΄ λ€μ‹  LoRA λ¨λ“λ§ κµμ²΄
- **ν•™μµ μ†λ„ 25% ν–¥μƒ**  
  - λ€λ¶€λ¶„μ νλΌλ―Έν„°λ” gradient κ³„μ‚° λ¶ν•„μ”

---

#### β οΈ ν•κ³„

- μ¶”λ΅  μ†λ„ μ μ§€λ¥Ό μ„ν•΄ \( A, B \)λ¥Ό \( W \)μ— **λ³‘ν•©(merge)** ν•  κ²½μ°:
  - μ„λ΅ λ‹¤λ¥Έ νƒμ¤ν¬μ© \( A, B \)λ¥Ό ν• λ²μ— **λ°°μΉ μ²λ¦¬ν•κΈ° μ–΄λ ¤μ›€**
- λ‹¨, **μ§€μ—°μ΄ μ¤‘μ”ν•μ§€ μ•μ€ κ²½μ°**:
  - λ³‘ν•©ν•μ§€ μ•κ³  **μƒν”λ§λ‹¤ λ‹¤λ¥Έ LoRA λ¨λ“** λ™μ  μ„ νƒ κ°€λ¥


---

### π€ LORAμ μ„±λ¥ μ‹¤ν—!!

μ΄ μ—°κµ¬λ” λ‹¤μ–‘ν• νμΈνλ‹(fine-tuning) κΈ°λ²•λ“¤κ³Ό μ„±λ¥μ„ λΉ„κµν–μµλ‹λ‹¤!  
λΉ„κµ λ€μƒμΌλ΅λ” μ „ν†µμ μΈ **Full Fine-Tuning (FT)** μ„ λΉ„λ΅―ν•΄ λ‹¤μκ³Ό κ°™μ€ λ°©λ²•λ“¤μ΄ μμµλ‹λ‹¤:

- **Full Fine-Tuning (FT)**  
  λ¨λΈμ **λ¨λ“  νλΌλ―Έν„°λ¥Ό μ—…λ°μ΄νΈ**ν•λ” λ°©μ‹. κ°€μ¥ μΌλ°μ μ΄μ§€λ§, νλΌλ―Έν„° μκ°€ λ§μ•„ **λ©”λ¨λ¦¬/μ—°μ‚° λΉ„μ©μ΄ νΌ**.

- **BitFit (Bias-only Tuning)**  
  μ¤μ§ **bias ν•­λ§ ν•™μµ**ν•λ” λ°©μ‹. λ§¤μ° κ°€λ³μ§€λ§ ν‘ν„λ ¥μ€ μ ν•μ μΌ μ μμ.

- **Prefix Tuning (PreEmbed)**  
  μ…λ ¥ μ•(λλ” μ¤‘κ°„)μ— **νΉμ ν† ν°μ„ μ‚½μ…**ν•κ³ , μ΄λ“¤μ μ„λ² λ”©λ§ ν•™μµ. λ¨λΈ κµ¬μ΅°λ¥Ό μ μ§€ν•λ©΄μ„ μ μ‘ κ°€λ¥.

- **Prefix Layer Tuning (PreLayer)**  
  λ‹¨μ μ„λ² λ”©μ΄ μ•„λ‹λΌ, **κ° Transformer μΈµμ hidden activation μμ²΄λ¥Ό ν•™μµ**. λ” κ°•λ ¥ν• ν‘ν„λ ¥μ„ κ°€μ§.

- **Adapter Tuning**  
  Transformer λ‚΄λ¶€μ— **μ‘μ€ MLP κµ¬μ΅°μ μ–΄λ‘ν„° λ μ΄μ–΄**λ¥Ό μ‚½μ…ν•μ—¬ μΌλ¶€ νλΌλ―Έν„°λ§ ν•™μµ. λ‹¤μ–‘ν• λ³€ν•(AdapterH, AdapterL, AdapterP λ“±)μ΄ μμ.

- **LoRA (Low-Rank Adaptation)**  
  κΈ°μ΅΄ κ°€μ¤‘μΉ ν–‰λ ¬μ— **μ €λ­ν¬ ν–‰λ ¬ (B, A)**λ¥Ό λ³‘λ ¬λ΅ μ¶”κ°€ν•μ—¬ μΌλ¶€λ§ ν•™μµ. **μ¶”λ΅  μ†λ„ μ €ν• μ—†μ΄**, μ„±λ¥μ„ μ μ§€ν•λ©΄μ„ **νλΌλ―Έν„° μμ™€ λ©”λ¨λ¦¬ λΉ„μ©μ„ ν¬κ² μ¤„μ„**.



κ·Έλ¦¬κ³  κ²°κ³Όλ”~!

> LORAλ” μ μ€ νλΌλ―Έν„° ν•™μµμ„ ν†µν•΄ μΆ‹μ€ ν¨κ³Όλ¥Ό λ‚Έκ²ƒμ„ λ³Όμ μμ§€μ”~!  

![Image](https://github.com/user-attachments/assets/3824b21c-a1cf-44ef-aef6-f7673c8dc483)

- **GLUE benchmark (NLU κ³Όμ )** μ—μ„λ” RoBERTaμ™€ DeBERTa κΈ°λ° μ‹¤ν—μ—μ„ LoRAκ°€ Full FTμ™€ **λΉ„μ·ν•κ±°λ‚ λ” μΆ‹μ€ μ„±λ¥**μ„ λ‹¬μ„±
- **GPT-2 κΈ°λ° μƒμ„± κ³Όμ  (WikiSQL, SAMSum)** μ—μ„λ„ Prefix Tuningλ³΄λ‹¤ LoRAκ°€ **λ” λ†’μ€ BLEU/ROUGE μ„±λ¥**μ„ κΈ°λ΅
- **GPT-3 175B**μ—μ„λ” Full FTκ°€ λ¶κ°€λ¥ν• ν™κ²½μ—μ„λ„ **350GB VRAMμΌλ΅ ν•™μµ κ°€λ¥**ν•κ³ , κΈ°μ΅΄ κ²°κ³Όμ™€ μ μ‚¬ν• μ„±λ¥ ν™•λ³΄


---


### π”® κ²°λ΅ 

LoRAλ” **Transformer λ¨λΈλ“¤ (LLM, VIT, DETR λ“±λ“±)μ„ Fine-tuning ν•κΈ°μ„ν• νμ‹ μ μΈ λ°©λ²•**μ…λ‹λ‹¤!!  
μ΄ λ•λ¶„μ— μ¶”ν›„ μ—°κµ¬μ—μ„ **λ¨λΈ κ²½λ‰ν™”, λΉ λ¥Έ μ‹¤ν—, λ¶„μ‚° ν•™μµ, κ°μΈν™”** λ“±μ— λ‹¤μ–‘ν•κ² ν™μ©λκ³  μμµλ‹λ‹¤.  

