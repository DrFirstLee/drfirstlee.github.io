---
layout: post
title: "🎥 LAVAD: Training-Free Video Anomaly Detection with LLMs"
author: [DrFirst]
date: 2025-09-08 09:00:00 +0900
categories: [AI, Research]
tags: [Training-Free, Video Anomaly Detection, LLM, VLM, CVPR 2024, Surveillance, Zero-Shot, CVPR]
sitemap:
  changefreq: monthly
  priority: 0.8
---

### 🎥 (한국어) LAVAD: 학습 없이 영상 이상 감지까지, LLM이 해낸다!  
> LA-VAD = LAnguage-based Video Anomaly Detection. 즉, 언어모델 기반의 비디오 이상탐지!!  

![Image](https://github.com/user-attachments/assets/ece43f08-e9e6-4335-88e3-14cc76757320)  

* **제목**: Harnessing Large Language Models for Training-free Video Anomaly Detection  
* **학회**: CVPR 2024  
* **코드/체크포인트**: [GitHub – LAVAD](https://github.com/lucazanella/lavad), [Poroject](https://lucazanella.github.io/lavad/)    
* **핵심 키워드**: `Training-Free`, `Video Anomaly Detection`, `LLM`, `VLM`, `Zero-Shot`  
* **요약**: 사전 학습된 Vision-Language Model(VLM)과 Large Language Model(LLM)을 결합해, **추가 학습 없이(training-free)** 영상 내 이상 행동을 탐지하는 최초의 접근!  

---

### 🚀 LAVAD 핵심 요약

> 한 줄 요약: **“훈련된 데이터셋 없어도, LLM과 VLM으로 비디오 속 이상 상황을 잡아낸다!”**

1) **Training-Free Zero-Shot VAD**  
- 기존 VAD는 one-class 학습이나 unsupervised 학습이 필요했음  
- LAVAD는 **추가 훈련 전혀 없이**, 사전 학습된 모델만으로 이상 감지 수행  

2) **VLM 기반 캡션 생성**  
- 입력 영상 프레임을 VLM이 설명 → **텍스트 캡션** 생성  
- 예: “A person is walking”, “A person is fighting”  

3) **LLM 활용 이상 감지**  
- LLM이 캡션 시퀀스를 받아, **정상 vs 이상 행동** 분류  
- 시간적 맥락 이해 + prompt 기반 reasoning으로 anomaly score 계산  

4) **정제 단계 (Noise Reduction)**  
- 캡션 노이즈 → cross-modal similarity로 제거  
- frame-level anomaly score → temporal smoothing으로 안정화  

---

### 🔍 기존 연구의 흐름  

- **Video Anomaly Detection(VAD)**: 주로 대규모 정상 영상으로 one-class 훈련, 또는 weakly supervised 방식 활용  
  - 비디오는 이미지가 계속 이어짐!  
  - 전통적인 VAD는 각 Frame별 이미지 I에 대해 anomalous Score를 구함  
  - 이를 통해서 데이터 Label은 (V,y) 형식으로 구성됨 (비디오, label)  
  - **Fully-supervised** : y 는 [0,0,0,1,0...] 과 같이 프레임별 결과 제시  
  - **Weakly-supervised** : y 는 0 혹은 1로 비디오 전체에 대한 판단만 제시  
  - **One-Class** : y는 정상인것만 있음 (정상인것만 다 학습해서, 못보던 상황이면 이상으로!!)  
  - 위 방식들의 한계: 데이터셋 구하기 힘들 뿐만 아니라 새로운 환경/도메인에 적용하려면 항상 **다시 학습 필요**  

- LLM의 발전 : ChatGPT로 시작되어 여러 LLM이 발전함. VLM도 좋은 성능!!  

#### 그럼! 그냥 바로 VLM과 LLM을 가져다가 VAD에 써보면 어떨까??

- LLM에게 캡션을 주고, 정상/이상을 판단하는 Classification이 아닌, 이상 점수(anomaly score) 측성을 요청 - 점수는 11단계(0.0~1.0)로 나뉨  
- Equation(1) - **ΦLLM​(PC​∘PF​∘ΦC​(I))**  

  - P_C : 상황 맥락 프롬포트  
    - example : "“If you were a law enforcement agency, how would you rate the scene described on a scale from 0 to 1, with 0 representing a standard scene and 1 denoting a scene with suspicious activities?"
  - P_F : LLM에게 일정 형식의 output을 요청하는 프롬포트  
    - _"Please provide the response in the form of a Python list … choose only one number from [0, 0.1, …, 1.0]."_
  -  ◦  : 텍스트 연결
  - ΦC : VLM에서 나온 캡션

- 이 Equation (1) 의 방법으로 하면 잘될까!?  
  - VLM(x축) 과 LLM(Bar 색상)을 바꿔가며 테스트해봄, 결과는 random 보다는 좋지만 SOTA에 한참 못 미친다!  
  ![Image](https://github.com/user-attachments/assets/07df231c-4504-40e3-b354-19ef259fb829)

  - 왜일까?  
    a. Frame-level로 하면 잡음이 많아!  
    b. 영상의 전체 맥락을 파악 못해!  

---

### 🧱 LAVAD 구조 (Architecture)

![Image](https://github.com/user-attachments/assets/39d524a9-f9f1-4af4-b610-671762106382)

- 다섯가지 주요 요소로 구성된다!!  
  1. ΦC : VLM에서 나온 캡션. 이미지를 텍스트로.   
  2. ΦLLM : LLM. 텍스트를 텍스틀.  
  3. E_I : 이미지 인코더. 이미지를 벡터로.  
  4. E_T : 텍스트 인코더. 텍스트를 벡터로.  
  5. E_V : 비디오 인코더. 비디오를 벡터로.  
  + D_C : 영상 전체 프래임의 캡션의 집합  
  + C^_i : i 프레임에서 전후로 캡션 받아서 모은것  
  + S : 맥락정보가 반영된 캡션  

- 이제 아래의 3개 요소가 앞서 보았던 Unsupervised의 문제점을 해결해준다!  
  i) Image-Text Caption Cleaning: E_I와 E_T를 통핸 이미지 텍스트 캡션 정제  
  ii) LLM-based Anomaly Scoring: ΦLLM을 통해 시간적 정보를 인코딩하여 이상 점수 산출  
  iii) Video-Text Score Refinement: E_V와 E_T를 사용한 비디오-텍스트 유사도를 기반으로 한 이상 점수 보정  

#### i) Image-Text Caption Cleaning  

- 비디오에서 프레임별로 ΦC를 활용, 캡선(C)을 만듬 -> 앞서 본것처럼 noisy 하겠지?  
- 이 잡음 제거를 위해 전체 프레임의 캡션을 가져와서, E_T 로 벡터를 뽑고  
- E_I로 산출된 해당 프레임 이미지와 가장 cosine similarity가 높은 하나의 캡션을 뽑는다.  
- 참고: 실시간 영상분석이 아니라 테스트 비디오 전체를 이미 가지고 있다”는 offline VAD 상황임. 그래서 전체 캡션 집합 사용이 가능함!  
- 결국 전체 프레임에 대해서 정제된 캡션집합  C_i 를 확보함  

#### ii) LLM-based Anomaly Scoring  
-  C_i 가 장면정보가 있지만, 상황에 대한 정보는 없다!  
- 그래서 LLM으로 요약한다. 현 프래임 전후의 T초를 window로해서, 현 상황에서의 상황정보를 받는다.(C^_i)    
- 프롬포트는!! *“Please summarize what happened in few sentences, based on the following temporal description of a scene. Do not include any unnecessary details or descriptions.”*  
- 이를 통해서 현프레임 i 에서의 묘사를한 S_i 를 다시 LLM에 넣어서 anomaly score(a)를 구한다.  

#### iii) Video-Text Score Refinement  
- 이제, 프레임별 점수 a가 나왔지만, 이는 LLM을 통한거야.  
- 비디오 인코더 E_V를 통해서 i 프레임 중심의 영상을 인코딩하고,  
- 텍스트 인코더 E_T를 통해서 S_i를 인코딩한 값으로 다시 한번 점수 a~_i를 구한다.  

![Image](https://github.com/user-attachments/assets/f1f81389-c1c5-404c-8e9a-1fbf60a70be6)

- 그래서! i 프레임의 전후 비디오인코딩값이랑 전후의 텍스트 인코딩 유사도를 구해서,  
- 해당 유사도를 가중평균해서 a~_i 들을 더한다!! 이를 통해 최종 점수를 산출!!

---

### 🧪 실험 및 결과 분석   

#### 🧪 실험 요건  

- 실험 세부사항!! 
  - VLM은 **BLIP-2** 사용  
  - LLM은 **Llama-2-13b-chat** 사용  
  - 멀티모달 인코더(이미지,텍스트,비디오)는 **ImageBind** 사용, 시간 window는 10초!    
  - 

- 기존의 VAD용 데이터셋인 **UCF-Crime** and **XD-Violence** 활용, AUC-ROC 지표로 평가    
  - UCF-Crime : 1900개의 실제상황 감시카메라 비디오, 13가지의 이상클래스가 존재. AUC-ROC + Average Precision(AP) 둘 다 평가    
  - XD-Violence : 4754 비디오. 6개의 이상클래스 존재.  

#### 🧪 실험 결과  

![Image](https://github.com/user-attachments/assets/23533f4d-8ea0-403e-b920-a6915432c470)

- Training-Free중에서는 최고다!!  
  - 다만, Supervised에 비해서는 안좋네!  

#### 🧪 Ablation 분석  

![Image](https://github.com/user-attachments/assets/679a349d-ef8e-4f61-a9d4-7abb63845351)

- A. 아래 세가지 요소는 모두 중요했다!!  
  i) Image-Text Caption Cleaning  
  ii) LLM-based Anomaly Scoring  
  iii) Video-Text Score Refinement  

- B. 프롬포트가 중요할까?에 대한 실험  
  - 1번쨰 줄은 제일 기본 : *How would you rate the scene described on a scale from 0 to 1, with 0 representing a standard scene and 1 denoting a scene with suspicious activities?*   
  - 2번쨰 줄은 *“or potentially criminal activities* 추가  
  - 3번쨰 줄은 *“If you were a law enforcement agency,”* 추가  
  - 4번째 줄은 모두 추가!!
  - 신기하게도 3번쨰가 성능이 제일 좋았다! 아마도 너무 제약이 많으면 한계가 있을수도..    

- C. window K에 대한 분석  
- **노이즈 정제 단계** 없으면 성능 급락  
- **LLM reasoning**이 anomaly score 품질에 큰 기여  
- 단순 키워드 매칭 방식 대비 월등히 높은 정확도  

---

### ✅ 결론  

- **LAVAD**는 최초의 **Training-Free Video Anomaly Detection** 프레임워크  
- VLM과 LLM을 조합해, **훈련 데이터 없이도 실시간 이상 행동 탐지** 가능  
- 감시, 보안, 안전-critical 시스템 등에서 **즉각적 활용 가능성**  
- 앞으로 **더 강력한 멀티모달 LLM**이 등장하면 성능·확장성은 더욱 커질 전망!  


### 내 추가생각!!  
- 실시간 영상에 대한 분석도 해면 더 좋겠네!!  
- fps가 엄청 안나오곘는걸  
- 재미있는  TF연구네!  