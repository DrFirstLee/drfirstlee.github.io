---
layout: post
title:  "📝Understanding CLIP4HOI - CLIP4HOI를 알아보자!!!"
author: [DrFirst]
date: 2025-06-23 07:00:00 +0900
categories: [AI, Research]
tags: [HOI, CLIP, NeurIPS, Zero-Shot, CLIP4HOI, Training-Free]
sitemap :
  changefreq : monthly
  priority : 0.8
---


---

### 🧠 (한국어) CLIP4HOI 알아보기?!!  
_🔍 CLIP에 간단히 손만 얹었을뿐!!!_  

![manhwa]()

> 논문: [CLIP4HOI: Towards Adapting CLIP for Practical Zero-Shot HOI Detection](https://papers.nips.cc/paper_files/paper/2023/file/8fd5bc08e744fe0dfe798c61d1575a22-Paper-Conference.pdf)  
> 발표:  2024 (Mao, Yunyao, et al.)  
> 코드: [ChelsieLei/EZ-HOI](https://github.com/ChelsieLei/EZ-HOI)  

---

---

### 🔎 핵심 요약

- 💡 **CLIP**의 Zero-shot 능력을 HOI(Task: Human-Object Interaction)로 그대로 활용!
- ❌ 별도 학습 없이! → **Training-free**
- ✅ 템플릿 기반 텍스트 설명 + 이미지 → CLIP으로 바로 similarity 계산!

---

### 🏗️ 어떻게 작동할까?

CLIP4HOI는 두 가지 주요 단계로 HOI를 추론합니다:

1. **사람-객체 탐지 (Detection)**  
   - Off-the-shelf detector (예: DETR 등)로 이미지 내 사람과 객체 감지  
   - 가능한 모든 ⟨human, object⟩ 쌍 생성  

2. **HOI 추론 (Recognition)**  
   - 각 HOI 클래스에 대해 `"a person [verb] a [object]"` 형태로 텍스트 문장 생성  
   - CLIP의 텍스트 인코더로 문장 임베딩  
   - 해당 ⟨human, object⟩ 쌍의 이미지 특징(CLIP visual encoder)과 텍스트 임베딩 간 cosine similarity 측정  
   - 가장 유사한 클래스가 예측 결과!

---

### ✨ 왜 특별할까?

| 항목 | 설명 |
|------|------|
| **학습 없음** | 학습 없이 즉시 inference 가능 (training-free) |
| **템플릿 문장** | 모든 HOI class를 `"a person [verb] a [object]"` 구조로 자동 생성 |
| **Zero-Shot 대응** | 학습에 없던 HOI class (unseen)도 text 표현만 있으면 처리 가능 |
| **간단한 구조** | 복잡한 구조 없이 CLIP 하나로 inference 완성 |

---

### ⚠️ 한계점

- 🙈 **문장 표현 한계**  
  단일 템플릿 기반 문장은 복잡한 상호작용 표현에 약할 수 있음  

- 💬 **문맥 정보 부족**  
  사람-객체 쌍 간의 위치, 방향, 시선 등 **상황(context)** 정보 반영이 부족

- 📉 **동일 verb + object 조합에서 오류 가능**  
  예: `"pulling tie"` vs `"wearing tie"` → 이미지에서 정확히 구분하기 어려운 경우 발생

---

### ✅ 마무리 요약

CLIP4HOI는 복잡한 학습 없이도 바로 사용할 수 있는 **Training-Free HOI 탐지 방법**으로,  
기존 Zero-shot 설정에서 매우 간단하면서도 강력한 baseline을 제시합니다.

> 📌 **CLIP은 결국 텍스트+이미지 정렬 전문가!**  
> HOI처럼 사람과 객체 간의 상호작용을 표현할 수 있는 좋은 문장만 있다면,  
> 바로 inference가 가능한 실용적인 접근법!
