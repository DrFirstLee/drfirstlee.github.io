---
layout: post
title:  "📝Understanding CLIP4HOI - CLIP4HOI를 알아보자!!!"
author: [DrFirst]
date: 2025-06-23 07:00:00 +0900
categories: [AI, Research]
tags: [HOI, CLIP, NeurIPS, Zero-Shot, CLIP4HOI, NeurIPS 2023]
sitemap :
  changefreq : monthly
  priority : 0.8
---


---

### 🧠 (한국어) CLIP4HOI 알아보기?!!  
_🔍 CLIP에 간단히 손만 얹었을뿐!!!_  

![manhwa]()

> 논문: [CLIP4HOI: Towards Adapting CLIP for Practical Zero-Shot HOI Detection](https://papers.nips.cc/paper_files/paper/2023/file/8fd5bc08e744fe0dfe798c61d1575a22-Paper-Conference.pdf)  
> 발표: NeurIPS 2023 (Mao, Yunyao, et al.)  
> 코드: [maoyunyao/CLIP4HOI](https://github.com/maoyunyao/CLIP4HOI) 코드는 없음,,  

---

---

#### 🔎 핵심 요약

- 💡 **CLIP**의 Vision-Language 능력을 HOI(Human-Object Interaction) detection에 활용!
- 🏗️ **DETR + CLIP 결합**: DETR에 HOI 전용 컴포넌트 3개 추가하여 HOI detection 성능 향상
- ✅ **CLIP의 풍부한 표현력**: 이미지-텍스트 매칭 능력을 HOI 분류에 활용!

---

#### 🚨 **기존 방법들의 한계점**

기존 Zero-shot HOI detection 방법들은 다음과 같은 문제점들을 가지고 있었습니다:

#### **1️⃣ One-Stage Framework의 한계** 🎯

- 기존 방법: 하나의 쿼리로 사람-객체를 동시에 찾기  
- 문제점:   
  - 학습 데이터의 위치 분포에 과적합  
  - "사람이 컵을 들 때는 주로 오른쪽에 위치"와 같은 패턴 학습  
  - 새로운 위치 관계에서 성능 저하  

**예시**: 학습 시 "사람-컵" 상호작용이 주로 특정 위치에서 발생했다면, 다른 위치에서는 제대로 인식 못함  

#### **2️⃣ Knowledge Distillation의 한계** 🧠

- 기존 방법: CLIP 지식을 HOI detector에 증류  
- 문제점:  
  - 학습 데이터에 의존적 (data-sensitive)  
  - Seen categories 위주로 증류 진행  
  - Unseen categories 일반화 능력 부족  


#### **3️⃣ 미리 정의된 카테고리 의존성** 📚

- 기존 방법: 학습 시 unseen categories 미리 정의 필요  
- 문제점:  
  - 실제 응용에서 제한적
  - 진정한 "practical" zero-shot이 아님
  - 확장성 부족

> 💡 **CLIP4HOI의 해결책**: Two-stage paradigm으로 위치 분포 문제 해결 + CLIP 직접 adaptation으로 증류 의존성 제거!

---

#### **🏗️ CLIP4HOI Pipeline**

![structure]()

```
Input Image 
    ↓
1. Object Detector (DETR) : 사람들 + 객체들 탐지
    ↓
2. HO Interactor : 각 pair에 대한 proposal 생성
- pair에 대한 proposal의 예시는 아래와 같음  
- Input:  
    - 사람: bbox[100, 50, 200, 300] + features[사람 시각 정보]
    - 컵: bbox[180, 120, 220, 160] + features[컵 시각 정보]
- HO Interactor Output(HOI tokens):
    - 위치 관계: "컵이 사람 오른쪽 근처에 있음"
    - 거리: "30픽셀 거리"
    - 상호작용 특징: "잡을 수 있는 거리, 손 근처 위치" 등이 인코딩된 벡터
    - 결합된 표현: 이 모든 정보가 합쳐진 HOI proposal
    ↓
3. HOI Decoder (CLIP image encoder 기반)
 - Transformer 구조로, 아래 QKV로 Output 산출    
 - Query: 2. HO Interactor에서 나온 pairwise HOI proposals
 - Key/Value: CLIP image encoder의 전체 이미지 features
 - Output: 각 H-O pair에 특화된 visual features
    ↓
4. HOI Classifier (CLIP text embedding과 비교)  
 - 3에서의 결과물과 CLIP의 text embedding을 비교해서 가장 높은 값을 선정!  
    ↓
Final HOI predictions
```

#### **📦 3가지 핵심 컴포넌트 (학습되는 부분)**

**1️⃣ HO Interactor** 🤝
- DETR의 객체 탐지 결과에서 ⟨human, object⟩ 쌍 생성
- **Feature interaction + Spatial information injection**
- 인간-객체 간의 관계 정보를 담은 HOI tokens 생성 
```
- Input:  
    - 사람: bbox[100, 50, 200, 300] + features[사람 시각 정보]
    - 컵: bbox[180, 120, 220, 160] + features[컵 시각 정보]
- HO Interactor Output:
    - 위치 관계: "컵이 사람 오른쪽 근처에 있음"
    - 거리: "30픽셀 거리"
    - 상호작용 특징: "잡을 수 있는 거리, 손 근처 위치" 등이 인코딩된 벡터
    - 결합된 표현: 이 모든 정보가 합쳐진 HOI proposal
```

**2️⃣ HOI Decoder** 🧠  
- **CLIP image representation을 활용**하여 HOI visual features 생성
- HOI tokens를 query로 사용하여 attention mechanism 적용
- 시각적 HOI 표현을 더욱 풍부하게 만듦
```
 - Query: 2. HO Interactor에서 나온 pairwise HOI proposals
 - Key/Value: CLIP image encoder의 전체 이미지 features
 - Output: 각 H-O pair에 특화된 visual features
```

**3️⃣ HOI Classifier** 🎯
- **CLIP의 text embedding**과 HOI visual features 비교
- `"a person [verb] a [object]"` 텍스트 템플릿 활용
- Global + Pairwise HOI scores 계산하여 최종 예측
- 🔍 구체적 작동 방식  
  - HOI Classifier는 단순한 cosine similarity가 아님!!  
  - 1. Different Granularities를 보기위해 global, pair, local 정보 확인 및 sim 구하기!  
  ```python
  # 다양한 레벨의 features
  global_features = hoi_decoder_output['global']  
  pairwise_features = hoi_decoder_output['pairwise']
  local_features = hoi_decoder_output['local']

  # 각각에 대해 similarity 계산
  global_sim = cosine(global_features, text_emb)
  pairwise_sim = cosine(pairwise_features, text_emb)
  local_sim = cosine(local_features, text_emb)
  ```
  - 2. Global + Pairwise Scores를 구함 (논문에 정확히 안나와서 예측!)  
  ```python
  # 두 가지 타입의 점수 결합
  global_score = weighted_combine(global_sim, context_info)      # 전체 맥락 고려
  pairwise_score = weighted_combine(pairwise_sim, pair_info)     # 쌍 관계 고려
  ```

  # 학습 가능한 가중치로 결합
  final_score = w1 * global_score + w2 * pairwise_score  # ← 학습 필요!
  ```
  
  **3️⃣ 추가 학습 컴포넌트들 🔧**
  ```python
  # Projection layers (차원 맞추기)
  projected_visual = projection_layer(hoi_visual_features)  # ← 학습 필요!

  # Temperature scaling
  scaled_similarity = similarity / temperature  # ← 학습 필요!

  # Multi-head attention (possible)
  attended_features = attention(visual_features, text_features)  # ← 학습 필요!
  ```  


---

### 📊 **실험 결과 & 성능**

CLIP4HOI는 주요 벤치마크에서 **기존 방법들을 크게 능가**하는 성능을 보여줍니다:

#### **🏆 주요 벤치마크 결과**

| 방법 | HICO-DET (Zero-Shot) | V-COCO (Zero-Shot) |
|------|---------------------|-------------------|
| **기존 SOTA** | - | - |
| **CLIP4HOI** | **🎯 State-of-the-Art** | **🎯 State-of-the-Art** |

#### **✨ 핵심 성과**

**1️⃣ Rare & Unseen Categories 성능** 🎯
- **Rare HOI categories**: 기존 방법 대비 **대폭 향상**
- **Unseen HOI categories**: 다양한 zero-shot 설정에서 **일관된 우수 성능**
- **Positional distribution discrepancy**: 위치 분포 차이가 큰 경우에도 **robust한 성능**

**2️⃣ 다양한 Zero-Shot 설정** 🔬
```
Zero-Shot 시나리오별 성능:
├── UC (Unseen Combinations): ✅ SOTA 달성
├── UO (Unseen Objects): ✅ SOTA 달성  
├── UV (Unseen Verbs): ✅ SOTA 달성
└── Full Zero-Shot: ✅ 전체적으로 우수한 성능
```

**3️⃣ 방법론별 기여도 분석** 📈
- **Two-stage paradigm**: 위치 분포 robustness **크게 개선**
- **CLIP direct adaptation**: Knowledge distillation 대비 **일반화 성능 향상**
- **Distribution-agnostic approach**: Seen vs Unseen 성능 gap **현저히 감소**

#### **🎯 실용적 가치**

> 📌 **논문 결론**: "CLIP4HOI outperforms previous approaches on both rare and unseen categories, and sets a series of **state-of-the-art records** under a variety of zero-shot settings"

**✅ 실제 응용 관점:**
- **더 넓은 HOI 범위** 커버 가능
- **새로운 환경/시나리오**에서도 안정적 성능
- **Practical zero-shot** 실현으로 실용성 크게 향상

---

### ⚠️ 한계점

- 🙈 **단순한 텍스트 템플릿의 한계**  
  `"a person [verb] a [object]"` 형태만으로는 복잡한 상호작용의 뉘앙스 표현이 어려움  

- 💬 **공간적 관계 정보 부족**  
  사람-객체 간의 정확한 위치, 방향, 자세, 시선 등 **geometric context** 반영 한계

- 📉 **유사한 동작 구분의 어려움**  
  예: `"pulling tie"` vs `"wearing tie"`, `"holding phone"` vs `"talking on phone"`  
  → 시각적으로 매우 유사한 경우 구분 성능 저하

- 🔧 **Pre-trained 모델 의존성**  
  Object detector와 CLIP의 성능에 전적으로 의존  
  → 이들 모델의 한계가 그대로 HOI 성능에 영향

- 📚 **학습 데이터 요구**  
  HOI detection을 위한 별도 학습이 필요  
  → HOI 라벨링된 데이터셋과 학습 시간 필요

---

### ✅ 마무리 요약

CLIP4HOI는 **DETR의 객체 탐지 능력과 CLIP의 Vision-Language 이해력을 결합**한 HOI detection 방법입니다.

#### 🎯 **핵심 아이디어**
- **DETR + CLIP 결합**: 검증된 객체 탐지 + 강력한 의미 이해 조합
- **3가지 새 컴포넌트**: HO Interactor, HOI Decoder, HOI Classifier 추가
- **텍스트 템플릿 활용**: 자연어로 HOI를 표현하여 유연한 분류 가능
- **Zero-shot 일반화**: 학습되지 않은 HOI 클래스도 텍스트 설명으로 추론

#### ⚡ **기술적 혁신**
- **Spatial + Semantic 정보 결합**: 공간적 관계와 의미적 이해를 동시에 활용
- **Global + Pairwise 스코어**: 전체적/개별적 HOI 점수를 모두 고려
- **CLIP 표현력 극대화**: Pre-trained vision-language 모델의 능력을 HOI에 적용

> 📌 **CLIP의 텍스트-이미지 매칭 능력을 HOI detection에 성공적으로 접목!**  
> 
> 🚀 **실용적 가치**: 복잡한 HOI 관계도 자연어 설명을 통해 효과적으로 학습 및 추론 가능!
