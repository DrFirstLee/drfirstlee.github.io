---
layout: post
title: "ğŸ§© Segment Anything, Even Occluded (SAMEO): ê°€ë ¤ì§„ ë¶€ë¶„ê¹Œì§€ ì„¸ê·¸ë©˜íŠ¸í•˜ëŠ” SAM í™•ì¥"
author: [DrFirst]
date: 2025-08-28 10:00:00 +0900
categories: [AI, Research]
tags: [SAM, Amodal Segmentation, EfficientSAM, CVPR, CVPR 2025]
sitemap:
  changefreq: monthly
  priority: 0.8
---

---

### ğŸ§© (í•œêµ­ì–´) SAMEO : ê°€ë ¤ì§„ ê°ì²´ê¹Œì§€ í•œ ë²ˆì— segmentation!!  

![Image](https://github.com/user-attachments/assets/f0ded136-d325-482a-a24b-1b1e2e67adce)

* **ì œëª©**: [Segment Anything, Even Occluded (SAMEO)](https://arxiv.org/abs/2503.06261)  
* **í•™íšŒ**: CVPR 2025  
* **í”„ë¡œì íŠ¸/ë°ëª¨**: [Project Page](https://weient.github.io/sameo.github.io/) Â· [CVF OpenAccess PDF](https://openaccess.thecvf.com/content/CVPR2025/papers/Tai_Segment_Anything_Even_Occluded_CVPR_2025_paper.pdf)  
* **í•µì‹¬ í‚¤ì›Œë“œ**: `Amodal Instance Segmentation`, `Segment Anything`, `EfficientSAM`, `Detector+Mask Decoupling`, `Amodal-LVIS`  
* **ìš”ì•½**: SAMEOëŠ” **ë³´ì´ì§€ ì•ŠëŠ”(ê°€ë ¤ì§„) ë¶€ë¶„ê¹Œì§€ Segmentation**í•˜ê¸° ìœ„í•´, ë‹¤ë¥¸ SOTA ê°ì±„íƒì§€ê¸°ë¡œ ë¨¼ì € bboxí•˜ë©´! SAM í™œìš©í•´ì„œ bboxëœ ë¶€ë¶„ + ê°€ë ¤ì§„ ë¶€ë¶„ segmentë¥¼ ì°¾ëŠ”ë‹¤!  

---

### ğŸ§  ì£¼ìš” ê¸°ì—¬

1. **SAMEO í”„ë ˆì„ì›Œí¬ ì œì•ˆ**  
   ì•„ëª¨ë‹¬ ë¶„í• ì„ **(1) ê°ì²´ ê²€ì¶œ + (2) ë§ˆìŠ¤í¬ ë³µì›**ì˜ ë‘ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ê³ , **SAM(EfficientSAM)**ì„ **í”ŒëŸ¬ê·¸í˜• ë§ˆìŠ¤í¬ ë””ì½”ë”**ë¡œ í™œìš©í•´ **ê°€ë ¤ì§„ í˜•íƒœê¹Œì§€ ë³µì›**í•©ë‹ˆë‹¤. ê²€ì¶œê¸°ëŠ” êµì²´ ê°€ëŠ¥í•˜ì—¬ ë‹¤ì–‘í•œ ë°±ë³¸ê³¼ ê²°í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. :contentReference[oaicite:2]{index=2}  

2. **Amodal-LVIS ëŒ€ê·œëª¨ í•©ì„± ë°ì´í„°ì…‹(â‰ˆ30ë§Œ ì´ë¯¸ì§€)**  
   LVIS/LVVISë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì•„ëª¨ë‹¬ ì£¼ì„ì„ í•©ì„±**í•œ **Amodal-LVIS**ë¥¼ ì†Œê°œí•˜ì—¬, ì•„ëª¨ë‹¬ ë¶„í•  ì—°êµ¬ì˜ **í•™ìŠµ ë°ì´í„° ë³‘ëª©**ì„ ì™„í™”í–ˆìŠµë‹ˆë‹¤. :contentReference[oaicite:3]{index=3}  

3. **ì œë¡œìƒ· ì¼ë°˜í™”**  
   COCOA-cls, D2SA ë“± ë²¤ì¹˜ë§ˆí¬ì—ì„œ **í•™ìŠµë˜ì§€ ì•Šì€ ìƒí™©ì—ë„ ê°•í•œ ì œë¡œìƒ· ì„±ëŠ¥**ì„ ë³´ì—¬ì¤Œ!!  

4. **ì‹¤ìš©ì  í™œìš©ì„±**  
   ê¸°ì¡´ **ëª¨ë‹¬ ê²€ì¶œê¸°(ì˜¤í”ˆ/í´ë¡œì¦ˆì…‹ ë¶ˆë¬¸)**ì™€ ê²°í•© ê°€ëŠ¥í•˜ê³ , **SAM ê¸°ë°˜ ì£¼ì„ ë„êµ¬**ì²˜ëŸ¼ ë¶„í• +ë¼ë²¨ë§ íŒŒì´í”„ë¼ì¸ì—ë„ ì‘ìš©í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. :contentReference[oaicite:5]{index=5}


---

### ğŸ” ì—°êµ¬ ë°°ê²½

- Amodal segmentation(ì•„ëª¨ë‹¬ ë¶„í• )ì´ë€!! **ë³´ì´ëŠ” ì˜ì—­(Modal) + ê°€ë ¤ì§„ ì˜ì—­(Occluded)**ì„ ëª¨ë‘ ë³µì›í•´ì„œ segmentation í•˜ëŠ”ê²ƒ!!  

- Instance Segmentationì˜ ê¸°ì¡´ ë°©ë²•ë“¤ì€ **ê°ì±„ íƒì§€Â·ë¶„í• ì„ í•œêº¼ë²ˆì— í•™ìŠµ**í•´ ìœ ì—°ì„±ì´ ì ê³ , ëŒ€ê·œëª¨ í•™ìŠµ ë°ì´í„°ë„ ë¶€ì¡±í•œ ë‹¨ì !!

- Segment Anything ì€ ëª¨ë“  ê°ì²´ë¥¼ "ì˜" ë¶„í• í•˜ëŠ” íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì´ì—ˆìœ¼ë©° ì´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê°œì„ í•œ EfficientSAM ë„ ìˆì—ˆìŒ!  

- ê¸°ì¡´ Amodal datasetìœ¼ë¡œëŠ” COCOë¡œ ë¶€í„° ìœ ë˜í•œ COCOA, ë°ì´í„°ì…‹ì€ COCOA/D2SA/COCOA-cls ì™¸ì— KINS, DYCE, MUVA, MP3D-Amodal, WALT, KITTI-360-APS ë“±ì´ ìˆì§€ë§Œ, ëª¨ë‘ ë‹¨ì ì´ìˆì—ˆë‹¤  

   - **DYCE / MP3D-Amodal (í•©ì„±Â·ì‹¤ë‚´ 3D ë©”ì‰¬ ê¸°ë°˜)**ì˜ ê²½ìš° **ê±´ì¶• ìš”ì†Œê°€ í™”ë©´ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€**í•´ í•™ìŠµì— ë¹„íš¨ìœ¨ì ì´ì—ˆìœ¼ë©°, **ê°€ì‹œ ë¶€ë¶„ì´ ê·¹íˆ ì‘ì€ ê°ì²´**(visible < ì „ì²´ì˜ ê·¹ì†Œ ë¹„ìœ¨)ê°€ ë‹¤ìˆ˜ë¡œ í•™ìŠµ ì‹ í˜¸ ì•½í•¨  

   - **WALT (íƒ€ì„ë©ìŠ¤Â·êµí†µ ì¥ë©´ í•©ì„±)**: ë°•ìŠ¤ êµì°¨ë¥¼ ì´ìš©í•œ ê°ì²´ ì¬í•©ì„± ê³¼ì •ì—ì„œ **ë¹„í˜„ì‹¤ì ì¸(ìì—°ìŠ¤ëŸ½ì§€ ì•Šì€) ê°€ë¦¼** ë°œìƒí•˜ì˜€ê³ , ë ˆì´ì–´ ìˆœì°¨ ë°°ì¹˜ë¡œ **ê¹Šì´Â·ê°€ë¦¼ ê´€ê³„ ì™œê³¡**ì˜ ë¬¸ì œ  

   - **COCOA ë“± í´ë˜ìŠ¤ ì£¼ì„ í¬í•¨ ë°ì´í„°ì…‹**: **stuff(ë°°ê²½) í´ë˜ìŠ¤** ë‹¤ìˆ˜ í¬í•¨ â†’ ì•„ëª¨ë‹¬ *ì¸ìŠ¤í„´ìŠ¤* ë¶„í•  ëª©í‘œì™€ **ë¶€í•©í•˜ì§€ ì•ŠëŠ” ë¼ë²¨** í˜¼ì¬, ì˜ë¯¸ ìˆëŠ” â€˜ë¬¼ì²´â€™ ì¤‘ì‹¬ì˜ í•™ìŠµì— **ì¡ìŒ ì¦ê°€**  



---

### ğŸ“˜ SAMEO êµ¬ì¡° (Architecture)!!  

![Image](https://github.com/user-attachments/assets/d1e207f3-73db-4d8b-9b58-268c9e8ed29c)  

- **Front-end Detector**: ê¸°ì¡´(ë˜ëŠ” ì„ í˜¸í•˜ëŠ”) ê²€ì¶œê¸°ê°€ BBOX ê°’ì„ ë¥¼ ì˜ˆì¸¡ ë° ì „ë‹¬   
- **Back-end SAMEO (Mask Decoder)**: BBOX ê°’ì„ ë°”íƒ•ìœ¼ë¡œ **EfficientSAM** ë°©ì‹ìœ¼ë¡œ segmentation, ë‹¤ë§Œ ì´ë¯¸ì§€ ì¸ì½”ë”Â·í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”ëŠ” ë™ê²°í•˜ê³  **Mask Decoderë§Œ ë¯¸ì„¸í•™ìŠµ**
   - Input : Original Image + BBox (from detector)  
   - ì´ë•Œ bboxì˜ ê²°ê³¼ëŠ” modal, amodal bboxë¥¼ 5:5ë¡œ ëª¨ë‘ í™œìš©!!    

--- 
### ğŸ”§ í•™ìŠµì „ë ¥ : Lossì˜ êµ¬ì„±  

![Image](https://github.com/user-attachments/assets/b4cc0dbb-c5c2-4be8-917b-1c44a0806535)

0) ìš”ì•½    
  - Dice â†’ **ê²¹ì¹¨ ìµœëŒ€í™”**  
  - Focal â†’ **ì–´ë ¤ìš´ í”½ì…€ ê°•ì¡°**  
  - IoU L1 â†’ **í’ˆì§ˆ ì ìˆ˜ ë³´ì •**(ì‹ ë¢°ë„ í•™ìŠµ)


1) Dice Loss(3)  â€” ê²½ê³„Â·ê²¹ì¹¨ ì¤‘ì‹¬
- **ëª©ì **: ì˜ˆì¸¡ ë§ˆìŠ¤í¬ `MÌ‚`ì™€ ì •ë‹µ ë§ˆìŠ¤í¬ `M_gt`ì˜ **ê²¹ì¹¨(Overlap)** ìµœëŒ€í™”
- **ê³„ì‚°ë°©ë²•ë²•**:  
  \[
  frac{2\,|MÌ‚ \cap M_{gt}|}
  \]
  - ë¶„ì: êµì§‘í•©(ê²¹ì¹˜ëŠ” í”½ì…€ í•©)  
  - ë¶„ëª¨: ë‘ ë§ˆìŠ¤í¬ì˜ í”½ì…€ í•©
    \[
  \{|MÌ‚| + |M_{gt}|}
  \]
- **íŠ¹ì§•**: ë¶ˆê· í˜• í´ë˜ìŠ¤(ê°ì²´ê°€ ì‘ì„ ë•Œ)ì—ì„œ **ì•ˆì •ì **. ê²½ê³„ í’ˆì§ˆ ê°œì„ ì— ë„ì›€.

2) Focal Loss (4) â€” ì–´ë ¤ìš´ í”½ì…€ì— ê°€ì¤‘ì¹˜
- **ëª©ì **: ì´ë¯¸ ì˜ ë§ì¶˜(ì‰¬ìš´) í”½ì…€ì˜ ê¸°ì—¬ë¥¼ ì¤„ì´ê³ , **ì–´ë ¤ìš´ í”½ì…€**ì— í•™ìŠµ ì§‘ì¤‘
- **ì •ì˜**:  
  - \(p_t\): íƒ€ê¹ƒ í´ë˜ìŠ¤(í¬ê·¸ë¼ìš´ë“œ/ë°±ê·¸ë¼ìš´ë“œ)ì— ëŒ€í•œ **ì˜ˆì¸¡ í™•ë¥ **
  - \(\gamma\)â†‘ â†’ ì‰¬ìš´ ìƒ˜í”Œ ì–µì œâ†‘, ì–´ë ¤ìš´ ìƒ˜í”Œ ê°•ì¡°â†‘  (Î³=2ë¡œ ì„¤ì •)
- **íŠ¹ì§•**: í”½ì…€ ë‹¨ìœ„ì˜ **ë‚œì´ë„ ì¡°ì ˆ**ë¡œ ë¯¸ì„¸í•œ ì˜ì—­/ê°€ë ¤ì§„ ë¶€ë¶„ í•™ìŠµì— ìœ ë¦¬.

3) IoU ì˜ˆì¸¡ L1 Loss (Î»=0.05) â€” í’ˆì§ˆ ì ìˆ˜ ë³´ì •
- **ëª©ì **: ë””ì½”ë”ê°€ ë‚´ëŠ” **ë§ˆìŠ¤í¬ í’ˆì§ˆ ì¶”ì •ì¹˜**(ì˜ˆ: IoU í—¤ë“œì˜ \(\hat{\rho}\))ë¥¼ **ì‹¤ì œ IoU**ì— ê°€ê¹ê²Œ í•™ìŠµ
- **íŠ¹ì§•**: ëª¨ë¸ì´ **ìì‹ ì˜ ë§ˆìŠ¤í¬ í’ˆì§ˆì„ ìŠ¤ìŠ¤ë¡œ í‰ê°€**í•˜ë„ë¡ ë§Œë“¤ì–´,  
  í›„ë³´ ë§ˆìŠ¤í¬ ì¤‘ **ì‹ ë¢°ë„ ê¸°ë°˜ ì„ íƒ/í›„ì²˜ë¦¬**ì— í™œìš© ê°€ëŠ¥.
- **ê°€ì¤‘ì¹˜**: ì „ì²´ ë¡œìŠ¤ì—ì„œ **Î» = 0.05**ë¡œ ê°€ë³ê²Œ ë°˜ì˜.

---

### ğŸ“š Amodal-LVIS ë°ì´í„°ì…‹!!  

![Image](https://github.com/user-attachments/assets/a1c8bb07-f101-4f90-b580-75220100e290)

- ë³¸ ì—°êµ¬ì—ì„œëŠ” Amodal segmentation ëª¨ë¸ ì™¸ì—ë„ í•™ìŠµ ë°ì´í„°ì…‹ì„ ì œì‹œí•¨!!  
- ì´ ë°ì´í„°ì…‹ì€ í•©ì„±ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ë°ì´í„°ì…‹ìœ¼ë¡œ, 3ë‹¨ê³„ì˜ ê³¼ì •ì„ ê±¸ì²´ ì œì‘ë¨!!  
- **ì´ 100ë§Œ ì´ë¯¸ì§€ / 200ë§Œê°œì˜ ì£¼ì„** ê·œëª¨ë¡œ êµ¬ì„±ë¨  

#### ğŸ”„ ìƒì„± íŒŒì´í”„ë¼ì¸

1) Complete Object Collection (ì™„ì „ ê°ì²´ ìˆ˜ì§‘)
- **SAMEO**ë¡œ **LVIS/LVVIS** ì¸ìŠ¤í„´ìŠ¤ì— **ì•„ëª¨ë‹¬ ë§ˆìŠ¤í¬ ì˜ì‚¬ë¼ë²¨** ìƒì„±  
- ì˜ˆì¸¡ëœ ì•„ëª¨ë‹¬ ë§ˆìŠ¤í¬ì™€ **GT ëª¨ë‹¬ ë§ˆìŠ¤í¬**ë¥¼ ë¹„êµí•´ **ì™„ì „íˆ ë³´ì´ëŠ”(ê°€ë ¤ì§€ì§€ ì•Šì€)** ê°ì²´ë§Œ **ì„ ë³„**  
  > ê²°ê³¼: **ì™„ì „ ê°ì²´ í’€(pool)** í™•ë³´

2) Synthetic Occlusion Generation (í•©ì„± ê°€ë¦¼)
- í’€ì—ì„œ **ê°ì²´ë¥¼ ë¬´ì‘ìœ„ í˜ì–´ë§**í•˜ì—¬ ë™ì¼ ì¥ë©´ì— **í•©ì„± ë°°ì¹˜**
- **ë¹„ìœ¨ ìœ ì§€ + í¬ê¸° ì •ê·œí™”**ë¡œ **ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤ì¼€ì¼** ë³´ì¥
- **Bounding box**ë¡œ **ìƒëŒ€ ìœ„ì¹˜/ê°€ë¦¼ ë¹„ìœ¨**ì„ ì œì–´ â†’ **ê°€ë¦¼ ë‚œì´ë„ ì»¤ë¦¬í˜ëŸ¼** êµ¬ì„± ê°€ëŠ¥

3) Dual Annotation Mechanism (ì´ì¤‘ ì£¼ì„) : ì•ˆê°€ë¦° ì‚¬ì§„ê³¼ ê°€ë¦°ì‚¬ì§„ ì œì‹œ!!  
- ì‹¤í—˜ì ìœ¼ë¡œ **ê°€ë ¤ì§„ ì‚¬ë¡€ë§Œ í•™ìŠµ**í•˜ë©´ ëª¨ë¸ì´ **ê³¼ë„í•œ ê°€ë¦¼ ì˜ˆì¸¡**ì„ í•˜ê²Œ ë¨  
- ì´ë¥¼ ë§‰ê¸° ìœ„í•´ **ê° ì¸ìŠ¤í„´ìŠ¤ì— ëŒ€í•´:**
  - **ì›ë³¸(ë¹„ê°€ë ¤ì§„) ì´ë¯¸ì§€/ë§ˆìŠ¤í¬**
  - **í•©ì„±(ê°€ë ¤ì§„) ì´ë¯¸ì§€/ë§ˆìŠ¤í¬**  
  **ë‘ ë²„ì „ì„ ëª¨ë‘ ì œê³µ** â†’ **í¸í–¥ ê°ì†Œ + ì¼ë°˜í™” í–¥ìƒ**


---

### ğŸ§ª Ablation & ê²°ê³¼ ë¶„ì„  

- Ablation1 : bbox ì œê³µì‹œ ê°€ë¦¼ ì˜ˆì¸¡í•˜ëŠ”ê±°ë§Œ(amodal), ê°€ë¦¼ë¬´ì‹œí•˜ëŠ”ê±°ë§Œ(modal), ë°˜ë°˜ ì„ ë¹„êµ! ë°˜ë°˜ì´ ì œì¼ íš¨ê³¼ê°€ ì¢‹ì•˜ë‹¤!

- Ablation2 : ê°€ë ¤ì§„ ì‚¬ì§„ìœ¼ë¡œë§Œ í•™ìŠµí•˜ë‹ˆ! ì˜¤íˆë ¤ ëª…í™•íˆ ê°ì±„ê²Œ ë‚˜ì˜´ì—ë„ ì˜ëª» Segmentation í•˜ëŠ”ê²½ìš°ê°€ ë°œìƒí–ˆë‹¤!  

- ê²°ê³¼ëŠ”?

![Image](https://github.com/user-attachments/assets/d74139db-6a8e-459b-96b1-177217424986)

1) ì •ëŸ‰ ê²°ê³¼ (Quantitative)
- COCOA-cls, D2SA, MUVA **ê° ë°ì´í„°ì…‹ì˜ trainâ†’test**ë¡œ í‰ê°€.
- **í”„ëŸ°íŠ¸ì—”ë“œ(ê²€ì¶œê¸°/ë¶„í• ê¸°) ì¢…ë¥˜ì™€ ë¬´ê´€**í•˜ê²Œ, SAMEOë¥¼ ë¶™ì´ë©´ **AISFormer ëŒ€ë¹„ APÂ·AR ì „ë°˜ ìƒìŠ¹**.
- **ëª¨ë‹¬/ì•„ëª¨ë‹¬ ì¶œë ¥ ëª¨ë‘** SAMEOê°€ **ì•„ëª¨ë‹¬ ë§ˆìŠ¤í¬ë¡œ ì •ì œ**í•´ **ìœ ì‚¬í•œ ê³ ì„±ëŠ¥** ë‹¬ì„±(í”„ë¡¬í”„íŠ¸ íƒ€ì… ë¶ˆë¬¸).

2) ì •ì„± ê²°ê³¼ (Qualitative)
- ë³µì¡í•œ ì¤‘ì²©(ë³‘/ìš©ê¸° ë‹¤ì¤‘ ê°ì²´), **ì‹¬í•œ ê°€ë¦¼**(ì¥ì• ë¬¼ ë’¤ ì¸ë¬¼), ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬Â·ìì„¸ì—ì„œ
  - **ê²½ê³„ê°€ ë” ë‚ ì¹´ë¡œìš´ ì•„ëª¨ë‹¬ ë§ˆìŠ¤í¬**, 
  - **ê°€ë ¤ì§„ ë¶€ë¶„ ì¶”ë¡ ì˜ í•©ë¦¬ì„±**ì´ ê°œì„  â†’ **ë² ì´ìŠ¤ë¼ì¸(AISFormer) ëŒ€ë¹„ í’ˆì§ˆ ìš°ìœ„** í™•ì¸.

3) ì œë¡œìƒ· ì„±ëŠ¥ (Zero-shot)
- í•™ìŠµ: **ìì²´ ì»¬ë ‰ì…˜ + Amodal-LVIS**(ë‹¨, **COCOA-cls/D2SA ì œì™¸**)ë¡œ ì§„í–‰, ë°°ì¹˜ëŠ” **ë¡œê·¸ ë¹„ìœ¨ ìƒ˜í”Œë§**.
- í‰ê°€: ì œì™¸í•œ ë‘ ë°ì´í„°ì…‹ì—ì„œ **í”„ëŸ°íŠ¸ì—”ë“œ ë‹¤ì–‘í•˜ê²Œ** ê²°í•©í•´ ì œë¡œìƒ· ì„±ëŠ¥ ì¸¡ì •.
- ê²°ê³¼: **COCOA-cls(+RTMDet)ì—ì„œ +13.8 AP**, **D2SA(+CO-DETR)ì—ì„œ +8.7 AP** ë“± **SOTA ë‹¬ì„±**, EfficientSAMì„ **ì•„ëª¨ë‹¬ë¡œ ì„±ê³µ ì ì‘**í•˜ë©´ì„œ **ì œë¡œìƒ· ì¼ë°˜í™”** ìœ ì§€.


---

### ğŸ§© ê²°ë¡ 

- ì—¬ëŸ¬ Object Detectorì™€ ê²°í•©í•˜ì—¬, bboxë‚´ì˜ ê°€ë ¤ì§„ ë¶€ë¶„ê¹Œì§€ segmentation í• ìˆ˜ ìˆëŠ” SOTA ì•Œê³ ë¦¬ì¦˜!!  
- ê²Œë‹¤ê°€ ë°ì´í„°ì…‹ ê³µê°œê¹Œì§€! ë–™í!!  


---

### ğŸ§© (English) SAMEO: Segment occluded objects in one shot!!

![Image](https://github.com/user-attachments/assets/f0ded136-d325-482a-a24b-1b1e2e67adce)

* **Title**: [Segment Anything, Even Occluded (SAMEO)](https://arxiv.org/abs/2503.06261)  
* **Conference**: CVPR 2025  
* **Project/Demo**: [Project Page](https://weient.github.io/sameo.github.io/) Â· [CVF OpenAccess PDF](https://openaccess.thecvf.com/content/CVPR2025/papers/Tai_Segment_Anything_Even_Occluded_CVPR_2025_paper.pdf)  
* **Keywords**: `Amodal Instance Segmentation`, `Segment Anything`, `EfficientSAM`, `Detector+Mask Decoupling`, `Amodal-LVIS`  
* **Summary**: To **segment even occluded regions**, SAMEO first takes **bboxes** from another SOTA object detector, then **uses SAM** to recover **both the boxed area and the occluded parts**!

---

### ğŸ§  Key Contributions

1. **SAMEO Framework**  
   Decomposes amodal segmentation into **(1) object detection + (2) mask reconstruction** and uses **SAM (EfficientSAM)** as a **plug-in mask decoder** to **recover occluded shapes**. The detector is swappable and can be paired with various backbones. :contentReference[oaicite:2]{index=2}  

2. **Amodal-LVIS: Large-Scale Synthetic Dataset (â‰ˆ300K images)**  
   Introduces **Amodal-LVIS**, synthesized from LVIS/LVVIS with **amodal annotations**, alleviating the **training data bottleneck** for amodal segmentation. :contentReference[oaicite:3]{index=3}  

3. **Zero-shot Generalization**  
   Shows **strong zero-shot performance** on benchmarks like COCOA-cls and D2SA!!

4. **Practical Utility**  
   Compatible with existing **modal detectors (open-/closed-set)** and applicable to segmentation + labeling pipelines like **SAM-based annotation tools**. :contentReference[oaicite:5]{index=5}

---

### ğŸ” Background

- **Amodal segmentation** aims to segment **both visible (modal) and occluded regions**, reconstructing the **full object**.

- Many instance segmentation methods **jointly train detection and segmentation**, which reduces flexibility and faces **limited large-scale training data**.

- **Segment Anything** is a foundation model that segments â€œanythingâ€ well; **EfficientSAM** improves practicality with a lighter design.

- Existing amodal datasets include **COCOA / D2SA / COCOA-cls**, and also **KINS, DYCE, MUVA, MP3D-Amodal, WALT, KITTI-360-APS**â€”but each has drawbacks:

  - **DYCE / MP3D-Amodal (synthetic indoor, 3D mesh-based)**: **Architectural elements** (walls/floors/ceilings) dominate the frame â†’ inefficient signals; many samples where the **visible part is extremely small**, weakening supervision.

  - **WALT (time-lapse / traffic synthesis)**: Layered compositing can cause **unnatural occlusions** and **distorted depth/occlusion relationships**.

  - **COCOA and similar with class annotations**: Many **stuff (background) classes** â†’ labels not aligned with **amodal *instance* segmentation**, adding noise instead of object-centric learning.

---

### ğŸ“˜ SAMEO Architecture!!

![Image](https://github.com/user-attachments/assets/d1e207f3-73db-4d8b-9b58-268c9e8ed29c)

- **Front-end Detector**: Your existing (or preferred) detector **predicts and passes BBoxes**.  
- **Back-end SAMEO (Mask Decoder)**: Given BBoxes, performs segmentation in the **EfficientSAM** way; **freeze the image encoder & prompt encoder** and **finetune only the mask decoder**.
  - **Input**: Original Image + BBox (from detector)  
  - **Training**: Use **modal and amodal boxes at a 50:50 ratio**!!

--- 
### ğŸ”§ Training Strategy: Loss Composition

![Image](https://github.com/user-attachments/assets/b4cc0dbb-c5c2-4be8-917b-1c44a0806535)

**0) Summary**  
- **Dice** â†’ **maximize overlap**  
- **Focal** â†’ **focus on hard pixels**  
- **IoU L1** â†’ **quality score calibration** (learn reliability)

**1) Dice Loss (Eq. 3) â€” Overlap/Boundary-focused**
- **Goal**: Maximize overlap between predicted mask `MÌ‚` and ground-truth mask `M_gt`
- **Definition**:  
  \[
  \mathcal{L}_{\text{Dice}} = 1 - \frac{2\,|MÌ‚ \cap M_{gt}|}{|MÌ‚| + |M_{gt}|}
  \]
  - Numerator: intersection (overlapping pixels)  
  - Denominator: sum of pixels in both masks
- **Note**: Stable under class imbalance (small objects); improves boundary quality.

**2) Focal Loss (Eq. 4) â€” Emphasize hard pixels**
- **Goal**: Down-weight easy pixels and **focus on hard ones**
- **Definition**:  
  \[
  \mathcal{L}_{\text{Focal}} = - (1 - p_t)^{\gamma}\,\log(p_t),\quad \gamma=2
  \]
  - \(p_t\): predicted probability of the target class (FG/BG)
  - Larger \(\gamma\) â†’ stronger suppression of easy samples, more focus on hard samples
- **Note**: Helps on fine/occluded regions.

**3) IoU Prediction L1 Loss (Î»=0.05) â€” Score Calibration**
- **Goal**: Make the decoderâ€™s **predicted IoU \(\hat{\rho}\)** close to the **true IoU**
- **Use**: Enables **confidence refinement** and **reliable ranking** among candidate masks.
- **Weight**: Use a small coefficient **Î» = 0.05** in the total loss.

---

### ğŸ“š Amodal-LVIS Dataset!!

![Image](https://github.com/user-attachments/assets/a1c8bb07-f101-4f90-b580-75220100e290)

- In addition to the amodal model, this work also presents a **training dataset**!  
- Itâ€™s a **synthetic** dataset created through a **3-stage pipeline**.  
- **Total size**: **~1M images / ~2M annotations**

#### ğŸ”„ Generation Pipeline

1) **Complete Object Collection**  
- Use **SAMEO** to generate **pseudo amodal masks** for LVIS/LVVIS instances.  
- Compare predicted amodal masks with **GT modal masks** to **select fully visible (unoccluded) objects**.  
  > Outcome: a **pool of complete objects**.

2) **Synthetic Occlusion Generation**  
- **Randomly pair** objects from the pool and **compose** them into the same scene.  
- Preserve **aspect ratios** with **size normalization** for natural scale.  
- Use **bounding boxes** to control **relative positions/occlusion ratios** â†’ enables **occlusion curriculum**.

3) **Dual Annotation Mechanism**: provide **both unoccluded and occluded** versions!  
- Training **only** on occluded cases leads to **over-occlusion predictions**.  
- For each instance, provide:
  - **Original (unoccluded) image/mask**
  - **Synthesized (occluded) image/mask**  
  â†’ **Reduces bias** and **improves generalization**.

---

### ğŸ§ª Ablation & Results

- **Ablation 1**: With bbox prompts, compare **amodal-only**, **modal-only**, and **50:50 mixed**. The **mixed** setup performs best overall!  

- **Ablation 2**: Training **only on occluded images** leads to **incorrect segmentation** even when the target object is clearly indicated by the bbox!

- **Results?**

![Image](https://github.com/user-attachments/assets/d74139db-6a8e-459b-96b1-177217424986)

**1) Quantitative**
- Evaluate **trainâ†’test** on **COCOA-cls, D2SA, MUVA**.
- **Regardless of the front-end type**, attaching SAMEO yields **AP/AR gains over AISFormer**.
- Whether the front-end outputs **modal or amodal** masks, SAMEO **refines** them into **strong amodal performance** (prompt-type agnostic).

**2) Qualitative**
- In challenging casesâ€”complex overlaps (bottles/containers), **heavy occlusions** (people behind barriers), diverse categories/posesâ€”  
  - **Sharper amodal boundaries**,  
  - **More reasonable occlusion inference** than the baseline (AISFormer).

**3) Zero-shot**
- Training: **Our collection + Amodal-LVIS** (excluding **COCOA-cls/D2SA**), with **log-proportional dataset sampling per batch**.
- Evaluation: Zero-shot on the two held-out datasets with **various front-ends**.
- Results: **+13.8 AP on COCOA-cls (with RTMDet)**, **+8.7 AP on D2SA (with CO-DETR)** â†’ **SOTA**, successfully **adapts EfficientSAM to amodal** while preserving **zero-shot generalization**.

---

### ğŸ§© Conclusion

- A **SOTA plug-in** that works with various object detectors to **segment both visible and occluded regions** within the bbox!  
- And they **release a dataset** as wellâ€”thanks!!
