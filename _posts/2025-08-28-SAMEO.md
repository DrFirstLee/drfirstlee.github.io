---
layout: post
title: "🧩 Segment Anything, Even Occluded (SAMEO): 가려진 부분까지 세그멘트하는 SAM 확장"
author: [DrFirst]
date: 2025-08-28 10:00:00 +0900
categories: [AI, Research]
tags: [SAM, Amodal Segmentation, EfficientSAM, CVPR, CVPR 2025]
sitemap:
  changefreq: monthly
  priority: 0.8
---

---

### 🧩 (한국어) SAMEO : 가려진 객체까지 한 번에 segmentation!!  

![sameo_teaser](https://weient.github.io/sameo.github.io/assets/teaser.jpg)

* **제목**: [Segment Anything, Even Occluded (SAMEO)](https://arxiv.org/abs/2503.06261)  
* **학회**: CVPR 2025  
* **프로젝트/데모**: [Project Page](https://weient.github.io/sameo.github.io/) · [CVF OpenAccess PDF](https://openaccess.thecvf.com/content/CVPR2025/papers/Tai_Segment_Anything_Even_Occluded_CVPR_2025_paper.pdf)  
* **핵심 키워드**: `Amodal Instance Segmentation`, `Segment Anything`, `EfficientSAM`, `Detector+Mask Decoupling`, `Amodal-LVIS`  
* **요약**: SAMEO는 **보이지 않는(가려진) 부분까지 Segmentation**하기 위해, 다른 SOTA 객채탐지기로 먼저 bbox하면! SAM 활용해서 bbox된 부분 + 가려진 부분 segment를 찾는다!  

---

### 🧠 주요 기여

1. **SAMEO 프레임워크 제안**  
   아모달 분할을 **(1) 객체 검출 + (2) 마스크 복원**의 두 단계로 나누고, **SAM(EfficientSAM)**을 **플러그형 마스크 디코더**로 활용해 **가려진 형태까지 복원**합니다. 검출기는 교체 가능하여 다양한 백본과 결합할 수 있습니다. :contentReference[oaicite:2]{index=2}  

2. **Amodal-LVIS 대규모 합성 데이터셋(≈30만 이미지)**  
   LVIS/LVVIS를 바탕으로 **아모달 주석을 합성**한 **Amodal-LVIS**를 소개하여, 아모달 분할 연구의 **학습 데이터 병목**을 완화했습니다. :contentReference[oaicite:3]{index=3}  

3. **제로샷 일반화**  
   COCOA-cls, D2SA 등 벤치마크에서 **학습되지 않은 상황에도 강한 제로샷 성능**을 보여줌!!  

4. **실용적 활용성**  
   기존 **모달 검출기(오픈/클로즈셋 불문)**와 결합 가능하고, **SAM 기반 주석 도구**처럼 분할+라벨링 파이프라인에도 응용할 수 있음을 시사합니다. :contentReference[oaicite:5]{index=5}


---

### 🔍 연구 배경

- Amodal segmentation(아모달 분할)이란!! **보이는 영역(Modal) + 가려진 영역(Occluded)**을 모두 복원해서 segmentation 하는것!!  

- Instance Segmentation의 기존 방법들은 **객채 탐지·분할을 한꺼번에 학습**해 유연성이 적고, 대규모 학습 데이터도 부족한 단점!!

- Segment Anything 은 모든 객체를 "잘" 분할하는 파운데이션 모델이었으며 이를 효율적으로 개선한 EfficientSAM 도 있었음!  

- 기존 Amodal dataset으로는 COCO로 부터 유래한 COCOA, 데이터셋은 COCOA/D2SA/COCOA-cls 외에 KINS, DYCE, MUVA, MP3D-Amodal, WALT, KITTI-360-APS 등이 있지만, 모두 단점이있었다  

   - **DYCE / MP3D-Amodal (합성·실내 3D 메쉬 기반)**의 경우 **건축 요소가 화면 대부분을 차지**해 학습에 비효율적이었으며, **가시 부분이 극히 작은 객체**(visible < 전체의 극소 비율)가 다수로 학습 신호 약함  

   - **WALT (타임랩스·교통 장면 합성)**: 박스 교차를 이용한 객체 재합성 과정에서 **비현실적인(자연스럽지 않은) 가림** 발생하였고, 레이어 순차 배치로 **깊이·가림 관계 왜곡**의 문제  

   - **COCOA 등 클래스 주석 포함 데이터셋**: **stuff(배경) 클래스** 다수 포함 → 아모달 *인스턴스* 분할 목표와 **부합하지 않는 라벨** 혼재, 의미 있는 ‘물체’ 중심의 학습에 **잡음 증가**  



---

### 📘 SAMEO 구조 (Architecture)!!  

![Image](https://github.com/user-attachments/assets/d1e207f3-73db-4d8b-9b58-268c9e8ed29c)  

- **Front-end Detector**: 기존(또는 선호하는) 검출기가 BBOX 값을 를 예측 및 전달   
- **Back-end SAMEO (Mask Decoder)**: BBOX 값을 바탕으로 **EfficientSAM** 방식으로 segmentation, 다만 이미지 인코더·프롬프트 인코더는 동결하고 **Mask Decoder만 미세학습**
   - Input : Original Image + BBox (from detector)  
   - 이때 bbox의 결과는 modal, amodal bbox를 5:5로 모두 활용!!    

--- 
### 🔧 학습전력 : Loss의 구성  

![Image](https://github.com/user-attachments/assets/b4cc0dbb-c5c2-4be8-917b-1c44a0806535)

0) 요약    
  - Dice → **겹침 최대화**  
  - Focal → **어려운 픽셀 강조**  
  - IoU L1 → **품질 점수 보정**(신뢰도 학습)


1) Dice Loss(3)  — 경계·겹침 중심
- **목적**: 예측 마스크 `M̂`와 정답 마스크 `M_gt`의 **겹침(Overlap)** 최대화
- **계산방법법**:  
  \[
  frac{2\,|M̂ \cap M_{gt}|}
  \]
  - 분자: 교집합(겹치는 픽셀 합)  
  - 분모: 두 마스크의 픽셀 합
    \[
  \{|M̂| + |M_{gt}|}
  \]
- **특징**: 불균형 클래스(객체가 작을 때)에서 **안정적**. 경계 품질 개선에 도움.

2) Focal Loss (4) — 어려운 픽셀에 가중치
- **목적**: 이미 잘 맞춘(쉬운) 픽셀의 기여를 줄이고, **어려운 픽셀**에 학습 집중
- **정의**:  
  - \(p_t\): 타깃 클래스(포그라운드/백그라운드)에 대한 **예측 확률**
  - \(\gamma\)↑ → 쉬운 샘플 억제↑, 어려운 샘플 강조↑  (γ=2로 설정)
- **특징**: 픽셀 단위의 **난이도 조절**로 미세한 영역/가려진 부분 학습에 유리.

3) IoU 예측 L1 Loss (λ=0.05) — 품질 점수 보정
- **목적**: 디코더가 내는 **마스크 품질 추정치**(예: IoU 헤드의 \(\hat{\rho}\))를 **실제 IoU**에 가깝게 학습
- **특징**: 모델이 **자신의 마스크 품질을 스스로 평가**하도록 만들어,  
  후보 마스크 중 **신뢰도 기반 선택/후처리**에 활용 가능.
- **가중치**: 전체 로스에서 **λ = 0.05**로 가볍게 반영.

---

### 📚 Amodal-LVIS 데이터셋!!  

![Image](https://github.com/user-attachments/assets/a1c8bb07-f101-4f90-b580-75220100e290)

- 본 연구에서는 Amodal segmentation 모델 외에도 학습 데이터셋을 제시함!!  
- 이 데이터셋은 합성으로 만들어진 데이터셋으로, 3단계의 과정을 걸체 제작됨!!  
- **총 100만 이미지 / 200만개의 주석** 규모로 구성됨  

#### 🔄 생성 파이프라인

1) Complete Object Collection (완전 객체 수집)
- **SAMEO**로 **LVIS/LVVIS** 인스턴스에 **아모달 마스크 의사라벨** 생성  
- 예측된 아모달 마스크와 **GT 모달 마스크**를 비교해 **완전히 보이는(가려지지 않은)** 객체만 **선별**  
  > 결과: **완전 객체 풀(pool)** 확보

2) Synthetic Occlusion Generation (합성 가림)
- 풀에서 **객체를 무작위 페어링**하여 동일 장면에 **합성 배치**
- **비율 유지 + 크기 정규화**로 **자연스러운 스케일** 보장
- **Bounding box**로 **상대 위치/가림 비율**을 제어 → **가림 난이도 커리큘럼** 구성 가능

3) Dual Annotation Mechanism (이중 주석) : 안가린 사진과 가린사진 제시!!  
- 실험적으로 **가려진 사례만 학습**하면 모델이 **과도한 가림 예측**을 하게 됨  
- 이를 막기 위해 **각 인스턴스에 대해:**
  - **원본(비가려진) 이미지/마스크**
  - **합성(가려진) 이미지/마스크**  
  **두 버전을 모두 제공** → **편향 감소 + 일반화 향상**


---

### 🧪 Ablation & 결과 분석  

- Ablation1 : bbox 제공시 가림 예측하는거만(amodal), 가림무시하는거만(modal), 반반 을 비교! 반반이 제일 효과가 좋았다!

- Ablation2 : 가려진 사진으로만 학습하니! 오히려 명확히 객채게 나옴에도 잘못 Segmentation 하는경우가 발생했다!  

- 결과는?

![Image](https://github.com/user-attachments/assets/d74139db-6a8e-459b-96b1-177217424986)

1) 정량 결과 (Quantitative)
- COCOA-cls, D2SA, MUVA **각 데이터셋의 train→test**로 평가.
- **프런트엔드(검출기/분할기) 종류와 무관**하게, SAMEO를 붙이면 **AISFormer 대비 AP·AR 전반 상승**.
- **모달/아모달 출력 모두** SAMEO가 **아모달 마스크로 정제**해 **유사한 고성능** 달성(프롬프트 타입 불문).

2) 정성 결과 (Qualitative)
- 복잡한 중첩(병/용기 다중 객체), **심한 가림**(장애물 뒤 인물), 다양한 카테고리·자세에서
  - **경계가 더 날카로운 아모달 마스크**, 
  - **가려진 부분 추론의 합리성**이 개선 → **베이스라인(AISFormer) 대비 품질 우위** 확인.

3) 제로샷 성능 (Zero-shot)
- 학습: **자체 컬렉션 + Amodal-LVIS**(단, **COCOA-cls/D2SA 제외**)로 진행, 배치는 **로그 비율 샘플링**.
- 평가: 제외한 두 데이터셋에서 **프런트엔드 다양하게** 결합해 제로샷 성능 측정.
- 결과: **COCOA-cls(+RTMDet)에서 +13.8 AP**, **D2SA(+CO-DETR)에서 +8.7 AP** 등 **SOTA 달성**, EfficientSAM을 **아모달로 성공 적응**하면서 **제로샷 일반화** 유지.


---
