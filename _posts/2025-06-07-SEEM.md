---
layout: post
title: "ğŸ§  Understanding SEEM - SEEM(Segment Everything Everywhere All at Once) ì•Œì•„ë³´ê¸°!!"
author: [DrFirst]
date: 2025-06-07 07:00:00 +0900
categories: [AI, Research]
tags: [SEEM, computer vision, NeurIPS, NeruIPS 2023, Segmentation]
sitemap :
  changefreq : monthly
  priority : 0.8
---


---

## ğŸ§  SEEM: Segment Everything Everywhere All at Once

*ğŸ” A universal segmentation model that handles text, clicks, boxes, and more via multimodal prompts.*

> Paper: [SEEM: Segment Everything Everywhere All at Once](https://arxiv.org/abs/2405.20191)
> Conference: NeurIPS 2024 (Zou, Xueyan, et al.)
> Code: [UX-Decoder/Segment-Everything-Everywhere-All-At-Once](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)
> Comment: All-in-one segmentation with multi-modal prompting!

---

### ğŸ¯ Four Core Capabilities of SEEM

1. **ğŸ›ï¸ Versatility**

   * Unifies various spatial queries (clicks, boxes, scribbles, masks) into a single **visual prompt**
   * Can even handle referred regions from other images

2. **ğŸ”— Compositionality**

   * Learns a **joint visual-language embedding space** for interpreting combinations of text and visual prompts
   * Freely supports prompt composition

3. **ğŸ” Interactivity**

   * Uses **memory prompts** to retain previous segmentation information
   * Optimized for iterative interaction

4. **ğŸ§  Semantic-awareness**

   * Aligns text and mask labels in the same semantic space
   * Enables **open-vocabulary segmentation** (can identify unseen classes)

---

### ğŸ“š Background: Why Do We Need a Universal Segmentation Model?

Image segmentation is a fundamental task in computer vision, responsible for understanding objects at the pixel level. Traditional approaches such as **semantic**, **instance**, and **panoptic segmentation** have laid a strong foundation. But the current trend is moving toward **flexible and general-purpose segmentation models**.

#### ğŸ”„ Evolution of Segmentation

1. **Closed-set â†’ Open-vocabulary**

   * Instead of recognizing fixed classes, models now use **multimodal pretraining (e.g., CLIP)** to generalize to unseen categories.

2. **Generic â†’ Referring**

   * Text-guided segmentation is gaining traction as it offers a more intuitive interface for users.

3. **One-shot â†’ Interactive**

   * Users provide input iteratively (clicks, boxes, etc.) to refine results step by step.

Despite these advances, many models still rely on task-specific architectures and lack the flexibility to handle diverse inputs or task switching within one system.

---

#### ğŸ§  Meanwhile, Language Models Have Solved This

Language models like **GPT-3** and **T5** paved the way for unified interfaces by handling multiple NLP tasks with a single model through prompting.

However, segmentation models still face these limitations:

* Limited prompt types (text, box, click only)
* Outputs masks only, without semantic meaning
* Poor generalization to new prompt combinations or domains

---

#### ğŸš€ Enter SEEM

SEEM addresses all these challenges with:

* A single model that handles **all types of segmentation tasks**
* Integrated support for **text, visual, and memory prompts**
* Flexible **prompt composition** and interactive updates
* **Open-vocabulary capabilities** for semantic prediction

> âœ… Just like GPT understands text in context, **SEEM segments the world interactively and semantically.**

---

### ğŸ§  SEEM Model Architecture

SEEM builds on the encoder-decoder paradigm and accepts **textual (Pt)**, **visual (Pv)**, and **memory (Pm)** prompts to drive segmentation.

---

#### ğŸ“¦ 1. Overall Pipeline

![overal\_model](https://github.com/user-attachments/assets/31afd2c3-e9c1-4b75-8090-ee397fd0c84c)

```
Input image (I)  
â†“  
[Image Encoder] â†’ Feature map Z  
â†“  
[SEEM Decoder (Queries + Prompt Interaction)]  
â†“  
â†’ MaskPredictor â†’ Output mask M  
â†’ ConceptClassifier â†’ Semantic label C
```

---

#### ğŸ§± 2. Key Components

**(1) Image Encoder**

* Input: `I âˆˆ â„^{HÃ—WÃ—3}`
* Output: visual feature map `Z`
* Uses Vision Transformer variants

**(2) Prompts**

* `Pt`: Text prompts (natural language commands)
* `Pv`: Visual prompts (clicks, boxes, scribbles, masks, referred images)
* `Pm`: Memory prompts (track previous interaction results)

**(3) Learnable Queries (Qh)**

* Trainable tokens to query outputs (mask and class)
* Duplicated per task during training (generic, referring, interactive)

---

#### ğŸ”„ 3. Decoder Operations

![model\_detail](https://github.com/user-attachments/assets/464aa5eb-5c0e-449e-8885-611ca5ea8401)

(1) Query-prompt interaction

```
<Om_h, Oc_h> = Decoder(Qh ; <Pt, Pv, Pm> | Z)
```

* `Om_h`: Embedding for segmentation masks
* `Oc_h`: Embedding for semantic concepts

(2) Mask prediction

```
M = MaskPredictor(Om_h)
```

(3) Concept classification

```
C = ConceptClassifier(Oc_h)
```

---

#### ğŸŒ 4. Key Capabilities

##### ğŸ§© Versatile: Unify Diverse Inputs

* SEEM handles all non-text prompts as **visual prompts (`Pv`)** (clicks, boxes, scribbles, reference image)
* These inputs are projected into a **shared visual space** via a Visual Sampler
* Enables seamless composition with text inputs

##### ğŸ§  Compositional: Flexible Prompt Combinations

* Supports **mixing visual and text prompts** during inference
* Visual prompts align with `Om_h`, text prompts align with `Oc_h`
* Uses `IOUmask` (IoU between predicted and GT masks) for more accurate matching
* Trained to generalize to **unseen prompt combinations**

##### ğŸ”„ Interactive: Iterative Refinement

* Introduces `Pm` (memory prompts) to carry forward context
* `MaskedCrossAtt(Pm; Mp | Z)` updates memory based on previous mask
* Efficiently supports **multi-round segmentation** without re-encoding the image

##### ğŸ§  Semantic-aware: Meaningful Predictions

* Unlike class-agnostic models, SEEM predicts **semantic labels for masks**
* Thanks to alignment in **visual-language embedding space** (zero-shot capable)
* No semantic label training required for interactive tasks

---

### ğŸ§ª Experiments Summary

SEEM demonstrates strong performance across four segmentation settings with a **single unified model**.

---

#### ğŸ“‚ Datasets and Setup

* **Trained on**:

  * COCO2017 (Panoptic & Interactive)
  * RefCOCO, RefCOCO+, RefCOCOg (Referring)

* **Backbone**: FocalT, DaViT-d3/d5

* **Language Encoder**: UniCL / Florence

* **Metrics**: PQ, AP, mIoU, NoC\@85/90, 1-IoU, K-NoC\@90, Zero-shot VOS

---

#### ğŸ” Main Results

* **Generic**: +10 points on Panoptic PQ vs. Pix2Seqv2, SegGPT, etc.
* **Referring**: With visual prompt: +10.5 cIoU, +6.0 mIoU, +9.3 AP50 (Tiny model)
* **Interactive**: Better than SAM using 100x less data; supports diverse prompts
* **Video**: Zero-shot VOS (DAVIS17) + 1-click interactive DAVIS16

---

#### ğŸ“Š Summary Table

| Task Type                | SEEM Highlights                                                      |
| ------------------------ | -------------------------------------------------------------------- |
| Generic Segmentation     | +10 PQ over baselines                                                |
| Referring Segmentation   | +10.5 cIoU / +6.0 mIoU / +9.3 AP50 with visual prompt                |
| Interactive Segmentation | Outperforms SAM, supports text, box, click, scribble, polygon inputs |
| Video Segmentation       | Zero-shot DAVIS17, strong 1-click interactive performance on DAVIS16 |

---

### ğŸ“ Conclusion

Ultimately, SEEM behaves like a visual ChatGPT â€” segmenting from multimodal prompts and refining results interactively.

> **Segmentation with meaning** â€” not just masks, but concepts too!

![results1](https://github.com/user-attachments/assets/ad6e4fa6-905b-4505-a97f-4da7066b4e5c)

> Like SAM2, SEEM tracks objects across video frames without retraining!

![results2](https://github.com/user-attachments/assets/0fc400e2-74b2-4ee1-b8d8-c043abdccfa3)

---

Stay tuned for future experiments and hands-on tutorials!



---

## ğŸ§  (í•œêµ­ì–´) SEEM: Segment Everything Everywhere All at Once
_ğŸ” í…ìŠ¤íŠ¸, í´ë¦­, ë°•ìŠ¤ ë¬´ì—‡ì´ë“  OK! ë©€í‹°ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ë¡œ ì„¸ìƒì„ ë¶„í• í•˜ëŠ” ë²”ìš© ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸_


> ë…¼ë¬¸: [SEEM: Segment Everything Everywhere All at Once](https://arxiv.org/abs/2405.20191)
> ë°œí‘œ: NeurIPS 2024 (Zou, Xueyan, et al.)  
> ì½”ë“œ: [UX-Decoder/Segment-Everything-Everywhere-All-At-Once](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)  
> ì½”ë©˜íŠ¸: Multi-modal promptë¡œ ëª¨ë“ ê²ƒì„ í•œë²ˆì— segmentation!!

---

### ğŸ¯ SEEMì˜ 4ê°€ì§€ í•µì‹¬ì„±ëŠ¥!!!

1. **ğŸ›ï¸ ë‹¤ì¬ë‹¤ëŠ¥ì„± (Versatility)**  
   - í´ë¦­, ë°•ìŠ¤, ë‚™ì„œ, ë§ˆìŠ¤í¬ ë“± ë‹¤ì–‘í•œ ì§ˆì˜ë¥¼ í•˜ë‚˜ì˜ **ë¹„ì£¼ì–¼ í”„ë¡¬í”„íŠ¸**ë¡œ í†µí•©  
   - ì°¸ì¡° ì´ë¯¸ì§€ê¹Œì§€ í™œìš© ê°€ëŠ¥í•œ í™•ì¥ì„±

2. **ğŸ”— êµ¬ì„± ê°€ëŠ¥ì„± (Compositionality)**  
   - í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ë¥¼ í•¨ê»˜ í•´ì„í•  ìˆ˜ ìˆëŠ” **ê³µë™ ì‹œê°-ì–¸ì–´ ê³µê°„ í•™ìŠµ**  
   - í”„ë¡¬í”„íŠ¸ì˜ ììœ ë¡œìš´ ì¡°í•© ê°€ëŠ¥

3. **ğŸ” ìƒí˜¸ì‘ìš©ì„± (Interactivity)**  
   - **ë©”ëª¨ë¦¬ í”„ë¡¬í”„íŠ¸**ë¥¼ í†µí•´ ì´ì „ ë¶„í•  ì •ë³´ë¥¼ ê¸°ì–µ  
   - ì‚¬ìš©ìì™€ì˜ ë°˜ë³µì  ìƒí˜¸ì‘ìš©ì— ìµœì í™”

4. **ğŸ§  ì˜ë¯¸ ì¸ì‹ (Semantic-awareness)**  
   - í…ìŠ¤íŠ¸ì™€ ë§ˆìŠ¤í¬ ë¼ë²¨ì„ ê°™ì€ ì˜ë¯¸ ê³µê°„ì— ì¸ì½”ë”©  
   - **Open-vocabulary segmentation** ê°€ëŠ¥ (ìƒˆë¡œìš´ í´ë˜ìŠ¤ë„ ì¸ì‹)


### ğŸ“š SEEM ë“±ì¥ ë°°ê²½: ì™œ ë²”ìš© ì„¸ë¶„í™” ëª¨ë¸ì´ í•„ìš”í•œê°€?

ì´ë¯¸ì§€ ì„¸ë¶„í™”ëŠ” ì»´í“¨í„° ë¹„ì „ì˜ í•µì‹¬ ê³¼ì œë¡œ, í”½ì…€ ë‹¨ìœ„ ìˆ˜ì¤€ì—ì„œ ì‚¬ë¬¼ì„ ì‹ë³„í•˜ê³  êµ¬ì¡°í™”í•˜ëŠ” ì¤‘ìš”í•œ ì‘ì—…ì…ë‹ˆë‹¤.  
ê·¸ë™ì•ˆì€ **Semantic Segmentation**, **Instance Segmentation**, **Panoptic Segmentation** ë“± ë‹¤ì–‘í•œ ì ‘ê·¼ ë°©ì‹ì—ì„œ ì—°êµ¬ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.  
í•˜ì§€ë§Œ ìµœê·¼ ë¹„ì „ AIì˜ íë¦„ì€ ë‹¨ìˆœí•œ ì •í™•ë„ë¥¼ ë„˜ì–´, **ë” ìœ ì—°í•˜ê³  ë²”ìš©ì ì¸ ì„¸ë¶„í™” ëª¨ë¸**ì„ í–¥í•´ ê°€ê³  ìˆìŠµë‹ˆë‹¤.  

#### ğŸ”„ ì„¸ë¶„í™”ì˜ ì§„í™” ë°©í–¥

ìµœê·¼ ì„¸ë¶„í™” ì—°êµ¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì„¸ ê°€ì§€ ë°©í–¥ìœ¼ë¡œ ë¹ ë¥´ê²Œ í™•ì¥ë˜ê³  ìˆì–´ìš”!!  

1. **íì‡„í˜•ì—ì„œ ê°œë°©í˜• ì„¸ë¶„í™”ë¡œ (Closed-set â†’ Open-vocabulary)**  
   - ê¸°ì¡´ ëª¨ë¸ì€ ë¯¸ë¦¬ ì •ì˜ëœ í´ë˜ìŠ¤ë§Œ ì¸ì‹í–ˆì§€ë§Œ, ìµœê·¼ì—ëŠ” CLIP ê°™ì€ **ë©€í‹°ëª¨ë‹¬ ì‚¬ì „í•™ìŠµ ëª¨ë¸**ì„ í™œìš©í•´ **ìƒˆë¡œìš´ ê°œë…ê¹Œì§€ ì¸ì‹**í•˜ë ¤ëŠ” ì‹œë„ê°€ í™œë°œí•´ì§€ê³  ìˆìŠµë‹ˆë‹¤.

2. **ì¼ë°˜ ì„¸ë¶„í™”ì—ì„œ ì°¸ì¡° ê¸°ë°˜ ì„¸ë¶„í™”ë¡œ (Generic â†’ Referring)**  
   - í…ìŠ¤íŠ¸ ë¬¸êµ¬ë¡œ íŠ¹ì • ì˜ì—­ì„ ì§€ì‹œí•˜ê³  ë¶„í• í•˜ëŠ” **ì‚¬ìš©ì ì¹œí™”ì  ì¸í„°í˜ì´ìŠ¤**ê°€ ì£¼ëª©ë°›ê³  ìˆìœ¼ë©°, ì–¸ì–´ ê¸°ë°˜ ì§€ì‹œë¥¼ ì •í™•íˆ ë°˜ì˜í•˜ëŠ” ëª¨ë¸ì´ í•„ìš”í•´ì¡ŒìŠµë‹ˆë‹¤.

3. **ë‹¨ë°œ ì‹¤í–‰ì—ì„œ ìƒí˜¸ì‘ìš© ì„¸ë¶„í™”ë¡œ (One-shot â†’ Interactive)**  
   - í´ë¦­, ë°•ìŠ¤ ë“± ë‹¤ì–‘í•œ ì…ë ¥ì„ ë°˜ë³µ ì œê³µí•˜ë©° ê²°ê³¼ë¥¼ ì ì§„ì ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆëŠ” **ìƒí˜¸ì‘ìš© ê¸°ë°˜ ëª¨ë¸**ì´ ì¤‘ìš”í•´ì§€ê³  ìˆìŠµë‹ˆë‹¤.

ì´ëŸ¬í•œ ë°œì „ì€ ì„¸ë¶„í™” ëª¨ë¸ì„ ë³´ë‹¤ ì‹¤ìš©ì ìœ¼ë¡œ ë§Œë“¤ì—ˆì§€ë§Œ, ì—¬ì „íˆ **ê° ì‘ì—…ë§ˆë‹¤ ëª¨ë¸ êµ¬ì¡°ê°€ ë¶„ë¦¬ë˜ì–´ ìˆìœ¼ë©°**, ë‹¤ì–‘í•œ ì…ë ¥ ë°©ì‹ì´ë‚˜ ì‘ì—… ê°„ ì „í™˜ì— ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•˜ì§€ ëª»í•˜ëŠ” í•œê³„ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.

---

#### ğŸ§  í•˜ì§€ë§Œ, ì–¸ì–´ ëª¨ë¸ì€ ì´ë¯¸ í•´ê²°í–ˆë‹¤?

í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œëŠ” **GPT-3, T5** ë“± ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì´ ë“±ì¥í•˜ë©°, ë‹¤ì–‘í•œ ì–¸ì–´ ì‘ì—…ì„ í•˜ë‚˜ì˜ ì¸í„°í˜ì´ìŠ¤ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” **ë²”ìš© ì–¸ì–´ ëª¨ë¸ì˜ ì‹œëŒ€**ê°€ ì—´ë ¸ìŠµë‹ˆë‹¤.  
- í”„ë¡¬í”„íŠ¸ë§Œ ë°”ê¾¸ë©´ ì§ˆì˜ì‘ë‹µ, ë²ˆì—­, ìš”ì•½, ëŒ€í™” ë“± ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ê°€ í‘œì¤€ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ **ì‹œê° ì„¸ë¶„í™” ë¶„ì•¼ëŠ” ì—¬ì „íˆ** ë‹¤ìŒê³¼ ê°™ì€ ì œí•œì´ ìˆìŠµë‹ˆë‹¤:
- í´ë¦­, ë°•ìŠ¤, í…ìŠ¤íŠ¸ ë“± í”„ë¡¬í”„íŠ¸ê°€ **ì œí•œì **
- ëª¨ë¸ì´ **ì˜ë¯¸ ì—†ëŠ” ë§ˆìŠ¤í¬**ë§Œ ìƒì„± (ì˜ˆ: SAM)
- **ìƒˆë¡œìš´ ì‘ì—… ì¡°í•©ì´ë‚˜ ë„ë©”ì¸ì— ëŒ€í•œ ì¼ë°˜í™” ë¶€ì¡±**

---

#### ğŸš€ ê·¸ë˜ì„œ ë“±ì¥í•œ SEEM

ì´ëŸ¬í•œ ë°°ê²½ ì†ì—ì„œ ë“±ì¥í•œ ê²ƒì´ ë°”ë¡œ **SEEM**ì…ë‹ˆë‹¤.

SEEMì€ ê¸°ì¡´ì˜ ë¬¸ì œì ì„ ì •ë©´ìœ¼ë¡œ ëŒíŒŒí•˜ë©° ë‹¤ìŒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤:

- í•˜ë‚˜ì˜ ëª¨ë¸ì´ **ëª¨ë“  ì¢…ë¥˜ì˜ ì„¸ë¶„í™” ì‘ì—…**ì„ ì²˜ë¦¬
- í´ë¦­, ë°•ìŠ¤, ë§ˆìŠ¤í¬, í…ìŠ¤íŠ¸, ì°¸ì¡° ì´ë¯¸ì§€ ë“± **ëª¨ë“  í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•©**
- **í”„ë¡¬í”„íŠ¸ ê°„ ì¡°í•©**ì´ ììœ ë¡­ê³ , **ì´ì „ ì´ë ¥ê¹Œì§€ ê¸°ì–µí•˜ëŠ” ìƒí˜¸ì‘ìš©ì„±**
- **ì˜ë¯¸ ìˆëŠ” ë¼ë²¨**ê¹Œì§€ ì œê³µí•˜ëŠ” Open Vocabulary ëŒ€ì‘

SEEMì€ ë§ˆì¹˜ LLMì´ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ë£¨ë“¯, ì‹œê° ì„¸ë¶„í™”ì—ì„œ **ì§„ì •í•œ ë²”ìš© ì¸í„°í˜ì´ìŠ¤**ë¥¼ ì‹¤í˜„í•˜ë ¤ëŠ” ê°•ë ¥í•œ ì‹œë„ì…ë‹ˆë‹¤.


> âœ… SEEMì€ "í…ìŠ¤íŠ¸ë¡œ ì§€ì‹œí•˜ê³ , í´ë¦­ìœ¼ë¡œ ìˆ˜ì •í•˜ë©°, ì´ì „ íˆìŠ¤í† ë¦¬ê¹Œì§€ ê¸°ì–µí•˜ëŠ”"  
> ì§„ì •í•œ **ë²”ìš© ì„¸ë¶„í™” í”„ë ˆì„ì›Œí¬**ì…ë‹ˆë‹¤.


### ğŸ§  SEEM ëª¨ë¸ì˜ êµ¬ì¡°

SEEMì€ ì „í†µì ì¸ **Encoder-Decoder** êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©´ì„œ,  
**í…ìŠ¤íŠ¸(Text), ì‹œê°(Visual), ë©”ëª¨ë¦¬(Memory) í”„ë¡¬í”„íŠ¸**ë¥¼ ëª¨ë‘ ìˆ˜ìš©í•  ìˆ˜ ìˆëŠ”  
ë²”ìš©ì ì¸ ì„¸ë¶„í™” ëª¨ë¸ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

---

#### ğŸ“¦ 1. ì „ì²´ ì²˜ë¦¬ íë¦„

![overal_model](https://github.com/user-attachments/assets/31afd2c3-e9c1-4b75-8090-ee397fd0c84c)

```
ì…ë ¥ ì´ë¯¸ì§€ (I)  
â†“  
[Image Encoder] â†’ ì´ë¯¸ì§€ íŠ¹ì§• Z ì¶”ì¶œ  
â†“  
[SEEM Decoder (ì¿¼ë¦¬ + í”„ë¡¬í”„íŠ¸ ìƒí˜¸ì‘ìš©)]  
â†“  
â†’ MaskPredictor â†’ ë§ˆìŠ¤í¬ ì¶œë ¥ M  
â†’ ConceptClassifier â†’ ì˜ë¯¸ ë¼ë²¨ ì¶œë ¥ C
```

ìµœì¢…ì ìœ¼ë¡œ ì…ë ¥ì´ë¯¸ì§€(I)ì™€ ë‹¤ì–‘í•œ í˜•ì‹ì˜ í”„ë¡¬í¬íŠ¸(P) ë¥¼ ë°›ì•„,  
Decoderë¥¼ í†µí•´ segmantation Mask(M)ê³¼ í•´ë‹¹ ë§ˆìŠ¤í¬ì˜ ì˜ë¯¸(C) ì¶œë ¥í•˜ê²Œë©ë‹ˆë‹¤!!

---

#### ğŸ§± 2. êµ¬ì„± ìš”ì†Œ ì„¤ëª…

**(1) Image Encoder**  
- ì…ë ¥ ì´ë¯¸ì§€ `I âˆˆ â„^{HÃ—WÃ—3}`  
- ì‹œê° íŠ¹ì§• ë²¡í„° `Z`ë¥¼ ì¶”ì¶œ  
- Vision Transformer ê³„ì—´ êµ¬ì¡° ì‚¬ìš© ê°€ëŠ¥

**(2) Prompts (í”„ë¡¬í”„íŠ¸ ìœ í˜•)**  
- `Pt` : Text Prompt (ìì—°ì–´ ëª…ë ¹)  
- `Pv` : Visual Prompt (point, box, scribble, mask, referred region)  
- `Pm` : Memory Prompt (ê³¼ê±° ì„¸ë¶„í™” ì •ë³´ë¥¼ ì €ì¥)

**(3) Learnable Queries (Qh)**  
- ë§ˆìŠ¤í¬ ë° í´ë˜ìŠ¤ ì¶œë ¥ì„ ìœ„í•œ í•™ìŠµ ê°€ëŠ¥í•œ ì¿¼ë¦¬  
- í•™ìŠµ ì‹œì—ëŠ” ì¼ë°˜/ì°¸ì¡°/ìƒí˜¸ì‘ìš© ì„¸ë¶„í™”ì— ë”°ë¼ `Qh`ê°€ ë³µì œë¨

---

#### ğŸ”„ 3. ë””ì½”ë” ì‘ë™ ë°©ì‹

![model_detail](https://github.com/user-attachments/assets/464aa5eb-5c0e-449e-8885-611ca5ea8401)

(1) ì¿¼ë¦¬-í”„ë¡¬í”„íŠ¸ ìƒí˜¸ì‘ìš©  
```
âŸ¨Om_h, Oc_hâŸ© = Decoder(Qh ; âŸ¨Pt, Pv, PmâŸ© | Z)
```

- Om_h : Maskì— ëŒ€í•œ embedding
- Oc_h : Class ì„¤ëª…ì— ëŒ€í•œ embedding

(2) ë§ˆìŠ¤í¬ ì˜ˆì¸¡  
```
M = MaskPredictor(Om_h)
```

(3) ì˜ë¯¸ í´ë˜ìŠ¤ ì˜ˆì¸¡  
```
C = ConceptClassifier(Oc_h)
```

---

#### ğŸŒ 4. ì£¼ìš” íŠ¹ì„±

---

##### ğŸ§© Versatile: ë‹¤ì–‘í•œ ì…ë ¥ì„ í•˜ë‚˜ë¡œ í†µí•©

- SEEMì€ í´ë¦­, ë°•ìŠ¤, ë‚™ì„œ, ì°¸ì¡° ì´ë¯¸ì§€ ë“± **í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ëª¨ë“  ì…ë ¥**ì„ í•˜ë‚˜ì˜ ì‹œê° í”„ë¡¬í”„íŠ¸(`Pv`)ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
- ê¸°ì¡´ ë°©ì‹ë“¤ê³¼ ë‹¬ë¦¬, ê° ì…ë ¥ ìœ í˜•ë§ˆë‹¤ ë³„ë„ êµ¬ì¡°ë¥¼ ë‘ì§€ ì•Šê³  **Visual Sampler**ë¥¼ í†µí•´ **ëª¨ë“  ë¹„í…ìŠ¤íŠ¸ ì…ë ¥ì„ ë™ì¼í•œ í‘œí˜„ ê³µê°„ì— ì •ë ¬**í•©ë‹ˆë‹¤.
- ì´ ë•ë¶„ì— **í…ìŠ¤íŠ¸ + ì‹œê° í”„ë¡¬í”„íŠ¸**ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ì¡°í•©ë  ìˆ˜ ìˆìœ¼ë©°, ì‚¬ìš©ì ì˜ë„ë„ ë” ì •í™•í•˜ê²Œ ë°˜ì˜ë©ë‹ˆë‹¤.

---

##### ğŸ§  Compositional: í”„ë¡¬í”„íŠ¸ ì¡°í•©ì— ìœ ì—°í•˜ê²Œ ëŒ€ì‘

- ì‚¬ìš©ìëŠ” ì‹¤ì œë¡œ **í…ìŠ¤íŠ¸ì™€ ì‹œê° í”„ë¡¬í”„íŠ¸ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ê²½ìš°**ê°€ ë§ìŠµë‹ˆë‹¤.
- SEEMì€ ì„œë¡œ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ í”„ë¡¬í”„íŠ¸ê°€ í•¨ê»˜ ì œê³µë˜ë”ë¼ë„ ì´ë¥¼ **ì„œë¡œ ë‹¤ë¥¸ ì¶œë ¥(target)ì— ë§ì¶° ì •ë ¬**í•¨ìœ¼ë¡œì¨ í‘œí˜„ ê³µê°„ ê°„ ì°¨ì´ë¥¼ ê·¹ë³µí•©ë‹ˆë‹¤.
- êµ¬ì²´ì ìœ¼ë¡œ, ì‹œê° í”„ë¡¬í”„íŠ¸(`Pv`)ëŠ” ë§ˆìŠ¤í¬ ì„ë² ë”©(`Omâ‚•`)ê³¼, í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸(`Pt`)ëŠ” í´ë˜ìŠ¤ ì„ë² ë”©(`Ocâ‚•`)ê³¼ ê°ê° ì •ë ¬ë©ë‹ˆë‹¤.
- ì´ë•Œ ì •ë ¬ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” `IOUmask`ëŠ” **ì˜ˆì¸¡ëœ ë§ˆìŠ¤í¬ì™€ ì‹¤ì œ ë§ˆìŠ¤í¬ ê°„ì˜ ê²¹ì¹¨ ì •ë„(IoU: Intersection over Union)**ë¥¼ í™œìš©í•˜ì—¬,
  **ì–´ë–¤ í”„ë¡¬í”„íŠ¸ê°€ ì–´ë–¤ ì¶œë ¥ê³¼ ì˜ ë§ëŠ”ì§€ë¥¼ íŒë‹¨**í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.
- í•™ìŠµ í›„ì—ëŠ” **í”„ë¡¬í”„íŠ¸ê°€ ì—†ê±°ë‚˜**, **í•˜ë‚˜ë§Œ ì£¼ì–´ì§€ê±°ë‚˜**, ë˜ëŠ” **ë‘˜ ëª¨ë‘ ì£¼ì–´ì ¸ë„** í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë©°,  
  **í•™ìŠµ ì¤‘ ë³¸ ì  ì—†ëŠ” ì¡°í•©ì—ë„ ì¼ë°˜í™”**ë©ë‹ˆë‹¤.

---

##### ğŸ”„ Interactive: ë°˜ë³µ ìƒí˜¸ì‘ìš©ìœ¼ë¡œ ì ì§„ì  ì„¸ë¶„í™”

- SEEMì€ `Pm`ì´ë¼ëŠ” **ë©”ëª¨ë¦¬ í”„ë¡¬í”„íŠ¸(memory prompt)**ë¥¼ ë„ì…í•˜ì—¬, ì´ì „ ë§ˆìŠ¤í¬ ê²°ê³¼ë¥¼ í˜„ì¬ ì…ë ¥ì— ë°˜ì˜í•©ë‹ˆë‹¤.
- ì´ì „ ë§ˆìŠ¤í¬ì˜ ì •ë³´ëŠ” **ë§ˆìŠ¤í¬ ê¸°ë°˜ í¬ë¡œìŠ¤ ì–´í…ì…˜(Masked Cross Attention)**ì„ í†µí•´ íŠ¹ì • ì˜ì—­ ë‚´ì—ì„œë§Œ ë°˜ì˜ë˜ë©°, 
  ì´ë¥¼ í†µí•´ **ë°˜ë³µì ì¸ ì…ë ¥ì— ë”°ë¥¸ ì„¸ë¶„í™” ê²°ê³¼ì˜ ì ì§„ì  ê°œì„ **ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
- ë³„ë„ì˜ ì¶”ê°€ ë„¤íŠ¸ì›Œí¬ ì—†ì´ ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ì—¬ **íš¨ìœ¨ì„±ë„ ë›°ì–´ë‚¨**.

---

##### ğŸ§  Semantic-aware: ì˜ë¯¸ ìˆëŠ” ì„¸ë¶„í™” ê²°ê³¼ ì œê³µ

- ê¸°ì¡´ì˜ ìƒí˜¸ì‘ìš© ì„¸ë¶„í™” ëª¨ë¸ë“¤ì€ ë‹¨ìˆœíˆ ë§ˆìŠ¤í¬ë§Œ ìƒì„±í•˜ì§€ë§Œ,  
  SEEMì€ **ê° ë§ˆìŠ¤í¬ê°€ ë¬´ì—‡ì¸ì§€(semantic class)**ê¹Œì§€ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì‹œê° í”„ë¡¬í”„íŠ¸ì™€ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ **ê³µë™ ì‹œê°-ì˜ë¯¸ í‘œí˜„ ê³µê°„**ì— ì •ë ¬í•˜ì—¬, í•™ìŠµ ì‹œ ì˜ë¯¸ ë¼ë²¨ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë”ë¼ë„ **ì œë¡œìƒ·ìœ¼ë¡œ ì˜ë¯¸ë¥¼ ë¶„ë¥˜**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ë•ë¶„ì— SEEMì€ ë‹¨ìˆœí•œ ë¶„í• ì„ ë„˜ì–´, **"ë¬´ì—‡ì„ ë¶„í• í–ˆëŠ”ê°€"**ê¹Œì§€ ì„¤ëª… ê°€ëŠ¥í•œ ëª¨ë¸ì…ë‹ˆë‹¤.

---


### ğŸ§ª 4. ì‹¤í—˜ ìš”ì•½

SEEMì€ ë‹¤ì–‘í•œ ì„¸ë¶„í™” ì‘ì—…ì„ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ í†µí•©í•´ ì²˜ë¦¬í•˜ë©°,  
ë‹¤ìŒ ë„¤ ê°€ì§€ ì£¼ìš” ì‹¤í—˜ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

---

#### ğŸ“‚ ë°ì´í„°ì…‹ ë° ì„¤ì •

- **í•™ìŠµ ëŒ€ìƒ ì‘ì—…**:  
  - Panoptic Segmentation (COCO2017)  
  - Referring Segmentation (RefCOCO, RefCOCO+, RefCOCOg)  
  - Interactive Segmentation (COCO2017 ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜ í´ë¦­)

- **ëª¨ë¸ êµ¬ì„±**:  
  - Vision Backbone: FocalT, DaViT-d3/d5  
  - Language Encoder: UniCL ë˜ëŠ” Florence  
  - ë””ì½”ë”ëŠ” SEEM-Decoderë¡œ êµì²´

- **í‰ê°€ì§€í‘œ**:  
  - PQ (Panoptic Quality), AP (Average Precision), mIoU  
  - NoC@85 / NoC@90, 1-IoU, K-NoC@90  
  - Video: Zero-shot í‰ê°€ (DAVIS17, DAVIS16-Interactive)

---

#### ğŸ” ì£¼ìš” ì‹¤í—˜ ê²°ê³¼

- **Generic Segmentation**  
  - ê¸°ì¡´ ë²”ìš© ëª¨ë¸(Pix2Seqv2, Painter ë“±) ëŒ€ë¹„ Panoptic PQ +10í¬ì¸íŠ¸ í–¥ìƒ

- **Referring Segmentation**  
  - ì‹œê° í”„ë¡¬í”„íŠ¸ ì¶”ê°€ ì‹œ cIoU +10.5, mIoU +6.0, AP50 +9.3 í–¥ìƒ

- **Interactive Segmentation**  
  - SAMë³´ë‹¤ ì ì€ ë°ì´í„°ë¡œ ë” ë‚˜ì€ ì„±ëŠ¥  
  - ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸(í…ìŠ¤íŠ¸, í´ë¦­, ë°•ìŠ¤ ë“±) ì¡°í•© ê°€ëŠ¥

- **Video Object Segmentation**  
  - êµ¬ì¡° ë³€ê²½ ì—†ì´ zero-shot ìˆ˜í–‰  
  - DAVIS17ì—ì„œ fully-supervised ìˆ˜ì¤€ ì„±ëŠ¥  
  - DAVIS16-Interactiveì—ì„œ ë‹¨ì¼ í´ë¦­ìœ¼ë¡œ ê°•ë ¥í•œ ì„±ëŠ¥

---

#### ğŸ“Š ì„±ëŠ¥ ì¢…í•© ì •ë¦¬

| ì‘ì—… ìœ í˜•                 | SEEM ì„±ëŠ¥ ìš”ì•½                                                 |
|--------------------------|----------------------------------------------------------------|
| Generic Segmentation     | ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ panoptic ì„±ëŠ¥ +10í¬ì¸íŠ¸                         |
| Referring Segmentation   | ì‹œê° í”„ë¡¬í”„íŠ¸ ì¶”ê°€ ì‹œ cIoU +10.5, mIoU +6.0, AP50 +9.3 ì¦ê°€     |
| Interactive Segmentation | ì ì€ ë°ì´í„°ë¡œ SAM ëŠ¥ê°€, ë‹¤ì–‘í•œ ì…ë ¥ ì§€ì› (í…ìŠ¤íŠ¸, ë°•ìŠ¤ ë“±)     |
| Video Segmentation       | Zero-shotìœ¼ë¡œ DAVIS17/16 ìˆ˜ì¤€ ì„±ëŠ¥, ë³„ë„ êµ¬ì¡° ìˆ˜ì • ë¶ˆí•„ìš”      |


---


### ğŸ“ ê²°ë¡ 

ì´ë¥¼ í†µí•´ì„œ!! ê²°êµ­!! í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ì˜ ì—¬ëŸ¬ ì¸í’‹ ë“±ì„ í”„ë¡¬í¬íŠ¸ë¡œ ì‚¬ìš©í• ìˆ˜ ìˆìœ¼ë©°,  
ê³„ì† ëŒ€í™”ê°€ ê°€ëŠ¥í•œ chatGPTì²˜ëŸ¼ SEEMì—ì„œë„ ê¸°ì¡´ í”„ë¡¬í¬íŠ¸ì— ì´ì–´ì„œ segmentationì„ ì§„í–‰í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤!!
ë…¼ë¬¸ì—ëŠ” ì´ì— ëŒ€í•œ ë‹¤ì–‘í•œ ê²°ê³¼ ì´ë¯¸ì§€ë“¤ì„ ë³´ì—¬ì£¼ëŠ”ë°!  
ì •ë§ í¥ë¯¸ë¡­ë„¤ìš”!!  

> segmantation ì— ë”í•˜ì—¬ class ì˜ë¯¸ê¹Œì§€!! ë†€ëë„¤ìš”!!
![results1](https://github.com/user-attachments/assets/ad6e4fa6-905b-4505-a97f-4da7066b4e5c)

> ì•„ë˜ì—ì„œëŠ” SAM2ì²˜ëŸ¼ ë¹„ë””ì˜¤ì˜ í”„ë ˆì„ì„ ì¶”ì í•˜ë©° segmentation í•œë‹¤ëŠ” ê²ƒì´ ë†€ëìŠµë‹ˆë‹¤!
![results2](https://github.com/user-attachments/assets/0fc400e2-74b2-4ee1-b8d8-c043abdccfa3)


- ì €í¬ë„ ê³§ ì‹¤ìŠµì„ í†µí•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤!!
---

