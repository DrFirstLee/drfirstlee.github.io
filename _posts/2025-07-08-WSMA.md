---
layout: post
title: "🔍 WSMA: Multimodal Weak Supervision으로 Egocentric Affordance Grounding 혁신!"
author: [DrFirst]
date: 2025-07-08 07:00:00 +0900
categories: [AI, Research]
tags: [Computer Vision, Affordance, Weakly-Supervised, Multimodal, CLIP, AAAI, AAAI 2024, Robotics]
sitemap:
  changefreq: monthly
  priority: 0.8
---

### 🔍 (한국어) WSMA: Sentence도 학습해서!(Multimodal 약지도 학습)으로 Affordance Grounding 고도화!  

![Image](https://github.com/user-attachments/assets/dc32460d-8692-4d64-af83-e23ee21d3b2a)

* **제목**: [Weakly Supervised Multimodal Affordance Grounding for Egocentric Images](https://ojs.aaai.org/index.php/AAAI/article/view/28451)  
* **학회**: AAAI 2024  
* **코드/체크포인트**: [GitHub – WSMA](https://github.com/xulingjing88/WSMA)    
* **저자**: Lingjing Xu, Yang Gao, Wenfeng Song, Aimin Hao (Beihang Univ., BISTU)  
* **핵심 키워드**: `Affordance`, `Weakly-Supervised`, `Multimodal`, `CLIP`, `Egocentric`, `Robotics`  
* **요약**: WSMA는 **exocentric 이미지 + 텍스트 설명**에서 affordance 지식을 추출하고, 이를 **egocentric 이미지로 전이**하는 새로운 멀티모달 프레임워크. 픽셀 단위 주석 없이도 affordance 영역을 정확히 찾으며, 기존 SOTA보다 우수한 성능을 보임! 🚀  

---

### 🚀 연구 핵심 요약  

> 한 줄 요약: **“WSMA = Exocentric + Text → Egocentric 전이 → Weakly supervised로도 정확한 affordance 지역화 달성!”**

1) **연구 배경 (Affordance Grounding)**  
- 객체가 제공하는 **행동 가능성(action possibilities)** → 컵은 “마시기”, 칼날은 “자르기”  
- 문제: 기존 연구는 **Pixel 단위 어노테이션** 의존 → 비용↑, 오류↑  
- 현실적 학습: **이미지 수준 라벨(image-level labels)** 만으로 affordance 영역 학습 필요(Weakly supervised  )  

2) **WSMA 방법론**  
- **HOI-Transfer Module**: exocentric 이미지(사람-객체 상호작용)에서 affordance 지식 추출 → egocentric 이미지로 전이  
- **Pixel-Text Fusion Module**: affordance 텍스트(CLIP text encoder 활용)와 egocentric 이미지 특징 결합  
- **Weak Supervision**: CAM + Refined Module 기반 약지도 추론 → 세밀한 영역 분할  

3) **최종 출력**  
- egocentric 이미지에서 affordance heatmap 산출  
- 픽셀 단위 주석 없이도 **“잡기, 마시기, 자르기”** 등의 기능 영역 정확히 localize  

---

### 🔍 기존 연구의 한계와 차별점  

- Visual Affordance Grounding 연구
  - Affordance라 함은 Gibson에 의하여 정의되어서 Visual Affordance Grounding 차원에서 이어져왔다!  
  - 다만, 픽셀 단위 GT에 의존 → 비싸고 오류 많기에 여러 weakly supervised approaches 접근도 있었다.  
  - 몇 개의 점을 바탕으로한 학습이라던지, 비디오로부터의 학습법 등!!  
  - 가장 최근에는 이미지 level의 레이블링을 통한 weakly supervised approaches 가 있었음!!
  - 본 연구는 이에 더해서!! action에 대한 글자 정보를 활용한다!!

- Cross-view Knowledge Distillation
  - Knowledge distillation 은 딥러닝 기술에서 선생/학생모델을 두고 가르치는 기법.  
  - 반면, cross-view knowledge distillation은! 다른 관점에서의 지식을 전이하는데 집중함!!  
  - exo이미지의 지식을 ego 이미지로 전이하는 연구들이 있었음!!

- Vision-language Models  
  - 더이상 설명이 필요 없는 [CLIP!!](https://drfirstlee.github.io/posts/CLIP/)  
  - CLIP을 활용한 segmentation 등 다양한 연구가있다!  
  - 이번 연구는 CLIP을 통해 textual features를 추출할그다!  

---

### 🧱 WSMA 구조 (Architecture)  

![Image](https://github.com/user-attachments/assets/9ec990f0-18ed-432c-ae99-b288695dc07b)

- 3개의 주요 Branch로 구성 : Exocentric, Egocentric, and Text branches.  
- 4가지 Loss로 구성: `L_cls`, `L_clip`, `L_d`, `L_l_rela`  


1) **Egocentric Branch**  
  - Egocentric 이미지(I_g)를 DINO-ViT로 Feature 추출!  
  - DINO 추출시 마지막 2개 Layer의 출력을 결합, 해당 Feature를 2 layer의 MLP로 보낸다!  
  - 결국 이미지 Feature (f_g) 추출!!  

2) **Text Branch**  
  - affordance label (C)에 대하여 설명한 Affordance Text(T)를 단순히 텍스트로 하기엔 디자인적 문제가 있다.  
  - 이에, CoOp 방식에 의거, trainable prompts(V)를 통해 만든다.  
    - CoOp 방식이란? cut_with 을 그대로 넣을순 없고, a photo of knife to cut_with로 설계하긴 애매하니 [V1][V2][V3] 등등으로 하는것!!  
    - 이 Trainable Token이 이후 학습에서 조절되며 최적의 token으로 선정됨
  - 최종적으로는, learnable prompt 가 들어간 text를 바탕으로 CLIP에 넣어 text feature `f_t`를 뽑는다
  
3) **Pixel-Text Fusion Module**
  - Ego 이미지와 Text 정보를 잘 합치는 부분!   
  - 이미지(f_g), Text(f_t)의 align 해야한다!!  
    a. (Alignment1) 이미지정보(f`_g) 랑 텍스트정보(f_t)의 **Global align 정도를 평가** > L_clip  
      - f_g는 DINO로서 local 정보만 있다. 그래서 Global정보를 추가해준다!" `f'_g`
      - `f′_g = AttentionPool(Concat(Average(fg), fg)). `  
        - Average(fg) : 글로벌 정보   
        - AttentionPool로 가공 : local 패치와 global 사이 관계(유사도) 계산, 중요한 패치에 높은 weight를 주어 가중합된 새로운 벡터 f`_g 산출!!  
      -  `Z_clip = f_′g(0) · f_t(transpose), ′`
      - 이미지정보(f`_g) 랑 텍스트정보(f_t)가 얼마나 유사한지를 산출(Z_clip)  
      - 결국 Z_clip은 이미지정보(f`_g) 랑 텍스트정보(f_t)의 align 정도를 평가하며 이후 cross-entropy loss 용 *L_clip* 으로 활용댐  

​    b. (Alignment2) 텍스트와 각 이미지 위치(patch) 간의 **세부적(local) align 정도를 평가** : L_cls  
      - `f_att = f_t · [f'_g(1:)](transpose)`
      - `f_t`: 텍스트 정보
      - `f'_g(1:)` : 0번쨰는 전역정보니까 그거 뺴고 1번쨰부터  
      - 결국, `f_att`는 지역별 패치마다의 텍스트정보와의 유사도 계산한것!!  
      - 이젠, 텍스트 의미가 반영된 이미지 특징 맵(F_g)를 만든다!!
        - `F_g = f_g X f_att + f_g`
        - `f_g X f_att` : 텍스트 의미와 관련 있는 위치
        - `+ f_g` : 원래 이미지 특징을 보존
      - F_g가 3 × 3 convolutional layer 랑 FC 을 지나 구분점수 `c_ego`가 됨!!  
      - 결국 c_ego는 점수로서 cross-entropy loss `L_cls` 로 활용댐  


4) **Exocentric Branch**  
  - 1..i..n개의 Exo이미지에서, DINO-ViT 기반 feature 추출  
  - 이때 DINO의 마지막 2개 Layer의 feature를 추출 (f_b-1_i, f_b_i) 해서 concat 후 MLP 하여 이미지i에대한 피처 (f_i_x)를 만듬  
    - f_i_x = MLP(Concat(f_b-1_i, f_b_i))  
  - (AIM 모듈 적용) 을 통해서 F_i_x를 구함. F_i_x는 i번째 exocentric 이미지로부터 추출된, action의 특징!!  
  - F_i_x가 3 × 3 convolutional layer 랑 FC 을 지나 구분점수 `c_exo`가 됨!!  
  - c_exo는 점수로서 cross-entropy loss `L_cls` 로 활용댐  

5) **HOI-Transfer Module**  
- ego 이미지에서 추출된 F_g 랑 exo 이미지들로 부터 추출된 `F_i_x` 가 있다.  
- CAM을 활용해서 j-action에 대해서의 F_j_g, F_j_x 의 local knowledge transfer를 함.  
- `L_d`는 이때 ego와 exo 의 차이를 최소화하는 방향으로 학습됨.  
  - 가중치 `w_j`랑 `F_j_{g,x}` 를 곱하고 더해서 나오는 `Y_j_{g,x}`  
  - L_d = || Y_j_g - Y_j_x ||  
- `L_l_rela` 는 서로 다른 affordance 행동들이 실제로는 서로 연관되어 있을 수 있으니까, 모델이 그 관계를 인식하도록 돕는 loss이다.   
  ![rela](https://github.com/user-attachments/assets/bac3faae-3d9c-4b5d-8028-50641ba03fb1)  
  (7) `Y_g` 랑 `Y_x`를 flatten 하면, 각 action `C`에 대응되는 1차원벡터가 된다. 
  (8) 이를 각각 transpose한것을  곱해서 행렬로 만들면, Action 들간의 관계를 보여주는 2차원 행렬이 된다!!    
  (9) 이제 구해진 R_exo랑 R_ego은 각각 아래의 의미에 대응된다.   
    - `R_exo` : exo 시점에서 action 간의 관계.  
    - `R_ego` : ego 시점에서 action 간의 관계.  
    - 이  둘을 cosine 함수를 통해서 align 시킨다!!

6) 최종 inference는!?  
![inference](https://github.com/user-attachments/assets/4be090f3-2b04-4d8d-a441-ee707fcbb324)  
    1. ego 이미지를 DINO-ViT를 통과시키며, 마지막 2개 Layer의 추출한다   
    2. 추출값들을 concat 후 MLP 하여 이미지 에대한 피쳐로 만들고 local 정보만 있다기에 Global정보를 추가하는 attention pool을 거침   
    3. Learnable + class를 encoder에 넣어서 텍스트 피쳐를 만든다!!  
    4. Pixel-text attention에로, text 지식과 ego 이미지가 섞인 결과값을 만들고, FC layer를 지난다  
    5. 생성된 CAM과 ego의 attention matrix를 교차하며 최종 refined heatmap을 추출한다!  



---

### 🧪 실험 결과 및 ablation.   

#### Ablation Test.  

![ablation](https://github.com/user-attachments/assets/10dd4a01-8b31-48f9-b781-cc62b94dd26e)

- (UnSeen) HOI Transfer의 역할이 크고, 그 에 더해진 Pixel-text fusion이 작동한다!!  
- (Seen) 이미 상황을 알고있기에 상황설명이있는 Pixel-Text Fusion이 HOI  transfer 보다 중요하게 작동한다!!


#### 데이터셋 & 지표  
- **ADE20K (seen/unseen split)**  
- **HICO-IIF**  
- 평가 지표: **KLD ↓, SIM ↑, NSS ↑**  

#### 결과  

- ADE20K-unseen: **KLD 1.335, SIM 0.382, NSS 1.220**  
- ADE20K-seen: **KLD 1.176, SIM 0.416, NSS 1.247**  
- HICO-IIF: **KLD 1.465, SIM 0.358, NSS 1.012**  
- → LOCATE, Cross-view-AG 등 기존 모델 대비 성능 우수  

#### 정성적 비교 (Qualitative)  
- 컵의 입구, 칫솔의 끝 등 작은 affordance 부위를 정확히 localize  
- 배경 간섭 억제 및 unseen 객체에서도 일반화 잘 수행  

---

### 🧪 Ablation 분석  

- **모듈 제거 실험**:  
  - Ego branch만 사용 → 성능 저하  
  - HOI-Transfer 추가 → 성능 개선  
  - Pixel-Text Fusion 추가 → 성능 더 향상  
  - 두 모듈 모두 포함 시 최고 성능  
- **Loss 분석**:  
  - Cross-entropy (L_cls) + CLIP alignment (L_clip) + Distillation loss (L_d) + Relation loss (L_lrela) 조합이 가장 효과적  

---

## ✅ 결론  

- WSMA는 **Multimodal 약지도 프레임워크**로 egocentric affordance grounding을 크게 향상  
- 주요 기여:  
  1. **HOI-Transfer Module**로 exocentric affordance 지식 전이  
  2. **Pixel-Text Fusion Module**로 텍스트–이미지 결합  
  3. ADE20K, HICO-IIF에서 SOTA 수준 성능 달성  
- → 로봇 인지, 인간-로봇 상호작용(HOI), AR/VR 등 **실세계 응용**에 중요한 기여 🎯  

---
