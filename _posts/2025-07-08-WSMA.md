---
layout: post
title: "🔍 WSMA: Multimodal Weak Supervision으로 Egocentric Affordance Grounding 혁신!"
author: [DrFirst]
date: 2025-07-08 07:00:00 +0900
categories: [AI, Research]
tags: [Computer Vision, Affordance, Weakly-Supervised, Multimodal, CLIP, AAAI, AAAI 2024, Robotics]
sitemap:
  changefreq: monthly
  priority: 0.8
---

### 🔍 (한국어) WSMA: Multimodal 약지도 학습으로 Affordance Grounding 고도화!  

![Image](https://github.com/user-attachments/assets/wsma-aaai2024)

* **제목**: [Weakly Supervised Multimodal Affordance Grounding for Egocentric Images](https://ojs.aaai.org/index.php/AAAI/article/view/28451)  
* **학회**: AAAI 2024  
* **코드/체크포인트**: [GitHub – WSMA](https://github.com/xulingjing88/WSMA)    
* **저자**: Lingjing Xu, Yang Gao, Wenfeng Song, Aimin Hao (Beihang Univ., BISTU)  
* **핵심 키워드**: `Affordance`, `Weakly-Supervised`, `Multimodal`, `CLIP`, `Egocentric`, `Robotics`  
* **요약**: WSMA는 **exocentric 이미지 + 텍스트 설명**에서 affordance 지식을 추출하고, 이를 **egocentric 이미지로 전이**하는 새로운 멀티모달 프레임워크. 픽셀 단위 주석 없이도 affordance 영역을 정확히 찾으며, 기존 SOTA보다 우수한 성능을 보임! 🚀  

---

### 🚀 연구 핵심 요약  

> 한 줄 요약: **“WSMA = Exocentric + Text → Egocentric 전이 → Weakly supervised로도 정확한 affordance 지역화 달성!”**

1) **연구 배경 (Affordance Grounding)**  
- 객체가 제공하는 **행동 가능성(action possibilities)** → 컵은 “마시기”, 칼날은 “자르기”  
- 문제: 기존 연구는 **Pixel 단위 어노테이션** 의존 → 비용↑, 오류↑  
- 현실적 학습: **이미지 수준 라벨(image-level labels)** 만으로 affordance 영역 학습 필요(Weakly supervised  )  

2) **WSMA 방법론**  
- **HOI-Transfer Module**: exocentric 이미지(사람-객체 상호작용)에서 affordance 지식 추출 → egocentric 이미지로 전이  
- **Pixel-Text Fusion Module**: affordance 텍스트(CLIP text encoder 활용)와 egocentric 이미지 특징 결합  
- **Weak Supervision**: CAM + Refined Module 기반 약지도 추론 → 세밀한 영역 분할  

3) **최종 출력**  
- egocentric 이미지에서 affordance heatmap 산출  
- 픽셀 단위 주석 없이도 **“잡기, 마시기, 자르기”** 등의 기능 영역 정확히 localize  

---

### 🔍 기존 연구의 한계와 차별점  

- Visual Affordance Grounding 연구
  - Affordance라 함은 Gibson에 의하여 정의되어서 Visual Affordance Grounding 차원에서 이어져왔다!  
  - 다만, 픽셀 단위 GT에 의존 → 비싸고 오류 많기에 여러 weakly supervised approaches 접근도 있었다.  
  - 몇 개의 점을 바탕으로한 학습이라던지, 비디오로부터의 학습법 등!!  
  - 가장 최근에는 이미지 level의 레이블링을 통한 weakly supervised approaches 가 있었음!!
  - 본 연구는 이에 더해서!! action에 대한 글자 정보를 활용한다!!

- Cross-view Knowledge Distillation
  - Knowledge distillation 은 딥러닝 기술에서 선생/학생모델을 두고 가르치는 기법.  
  - 반면,  cross-view knowledge distillation은! 다른 관점에서의 지식을 전이하는데 집중함!!  
  - exo이미지의 지식을 ego 이미지로 전이하는 연구들이 있었음!!

- Vision-language Models  
  - 더이상 설명이 필요 없는 [CLIP!!](https://drfirstlee.github.io/posts/CLIP/)  
  - CLIP을 활용한 segmentation 등 다양한 연구가있다!  
  - 이번 연구는 CLIP을 통해 textual features를 추출할그다!  

---

### 🧱 WSMA 구조 (Architecture)  

![Image](https://github.com/user-attachments/assets/wsma-arch-2024)

- 3개의 주요 Branch로 구성 : Exocentric, Egocentric, and Text branches.  


1) **Egocentric Branch**  
  - Egocentric 이미지(I-g)를 DINO-ViT로 Feature 추출!  
  - 해당 Feature를 2 layer의 MLP로 보낸다!  
  - 결국 이미지 Feature (f_g) 추출!!  

2) **Text Branch**  
  -  affordance label (C)에 대하여 설명한 Affordance Text(T)를 CoOp 방식에 의거, trainable prompts(V)를 통해 만든다.  
  - 이를 CLIP에 넣어 Text Feature(f_t)를 뽑는다
  
3) **Pixel-Text Fusion Module**
  - Ego 이미지와 Text 정보를 잘 합치는 부분!  
  - 이미지(f_g), Text(f_t)의 align 해야한다!!
    a. (Alignment1) 이미지정보(f`_g) 랑 텍스트정보(f_t)의 **Global align 정도를 평가** > L_clip  
      - f_g는 DINO로서 local 정보만 있다. 그래서 Global정보를 추가해준다!
      - `f′_g = AttentionPool(Concat(Average(fg), fg)). `
      - Average(fg) : 글로벌 정보  
      - AttentionPool로 가공 : local 패치와 global 사이 관계(유사도) 계산, 중요한 패치에 높은 weight를 주어 가중합된 새로운 벡터 f`_g 산출!!  
      -  `Z_clip = f_′g(0) · f_t(transpose), ′`
      - 이미지정보(f`_g) 랑 텍스트정보(f_t)가 얼마나 유사한지를 산출(Z_clip)  
      - 결국 Z_clip은 이미지정보(f`_g) 랑 텍스트정보(f_t)의 align 정도를 평가하며 이후 cross-entropy loss 용 *L_clip* 으로 활용댐  

​    b. (Alignment2) 텍스트와 각 이미지 위치(patch) 간의 **세부적(local) align 정도를 평가** : L_cls  
      - `f_att = f_t · [f'_g(1:)](transpose)`
      - `f'_t`: 텍스트 정보
      - `f'_g(1:)` : 0번쨰는 전역정보니까 그거 뺴고 1번쨰부터  
      - 결국, `f_att`는 지역별 패치마다의 텍스트정보와의 유사도 계산한것!!  
      - 이젠, 텍스트 의미가 반영된 이미지 특징 맵(F_g)를 만든다!!
        - `F_g = f_g X f_att + f_g`
        - `f_g X f_att` : 텍스트 의미와 관련 있는 위치
        - `+ f_g` : 원래 이미지 특징을 보존
      - F_g가 3 × 3 convolutional layer 랑 FC 을 지나 구분점수 `c_ego`가 됨!!  
      - 결국 c_ego는 점수로서 cross-entropy loss `L_cls` 로 활용댐  


4) **Exocentric Branch**  
  - 1..i..n개의 Exo이미지에서, DINO-ViT 기반 feature 추출  
  - 이때 DINO의 마지막 2개 Layer의 feature를 추출 (f_b-1_i, f_b_i) 해서 concat 후 MLP 하여 이미지i에대한 피처 (f_i_x)를 만듬
  - (AIM 모듈 적용) 을 통해서 F_i_x를 구함. F_i_x는 i번째 exocentric 이미지로부터 추출된, action의 특징!!  
  - F_i_x가 3 × 3 convolutional layer 랑 FC 을 지나 구분점수 `c_exo`가 됨!!  
  - c_exo는 점수로서 cross-entropy loss `L_cls` 로 활용댐  

5) **HOI-Transfer Module**


---

### 🧪 실험 결과  

#### 데이터셋 & 지표  
- **ADE20K (seen/unseen split)**  
- **HICO-IIF**  
- 평가 지표: **KLD ↓, SIM ↑, NSS ↑**  

#### 결과  

- ADE20K-unseen: **KLD 1.335, SIM 0.382, NSS 1.220**  
- ADE20K-seen: **KLD 1.176, SIM 0.416, NSS 1.247**  
- HICO-IIF: **KLD 1.465, SIM 0.358, NSS 1.012**  
- → LOCATE, Cross-view-AG 등 기존 모델 대비 성능 우수  

#### 정성적 비교 (Qualitative)  
- 컵의 입구, 칫솔의 끝 등 작은 affordance 부위를 정확히 localize  
- 배경 간섭 억제 및 unseen 객체에서도 일반화 잘 수행  

---

### 🧪 Ablation 분석  

- **모듈 제거 실험**:  
  - Ego branch만 사용 → 성능 저하  
  - HOI-Transfer 추가 → 성능 개선  
  - Pixel-Text Fusion 추가 → 성능 더 향상  
  - 두 모듈 모두 포함 시 최고 성능  
- **Loss 분석**:  
  - Cross-entropy (L_cls) + CLIP alignment (L_clip) + Distillation loss (L_d) + Relation loss (L_lrela) 조합이 가장 효과적  

---

## ✅ 결론  

- WSMA는 **Multimodal 약지도 프레임워크**로 egocentric affordance grounding을 크게 향상  
- 주요 기여:  
  1. **HOI-Transfer Module**로 exocentric affordance 지식 전이  
  2. **Pixel-Text Fusion Module**로 텍스트–이미지 결합  
  3. ADE20K, HICO-IIF에서 SOTA 수준 성능 달성  
- → 로봇 인지, 인간-로봇 상호작용(HOI), AR/VR 등 **실세계 응용**에 중요한 기여 🎯  

---
