---
layout: post
title: "ğŸ§  CIRCO - Zero-Shot Composed Image Retrieval with Textual Inversion (ICCV 2023)"
author: [DrFirst]
date: 2025-07-29 09:00:00 +0900
categories: [AI, Research]
tags: [VLM, CIR, CIRCO, Textual Inversion, Zero-Shot, ICCV, ICCV 2023, Image Retrieval]
sitemap :
  changefreq : monthly
  priority : 0.8

---

### ğŸ§  (í•œêµ­ì–´) Textual Inversionì„ í™œìš©í•œ ì œë¡œìƒ· ì¡°í•© ì´ë¯¸ì§€ ê²€ìƒ‰!! **CIRCO**  
> ê¸°ì¡´ CIRì—°êµ¬ì—ì„œ ZS-CIR ëª¨ë¸ì„ ê³µê°œí•˜ê³ , CIRCO ë°ì´í„°ì…‹ë„ ê³µê°œ!!  

![Image](https://github.com/user-attachments/assets/1a4f2dcb-38b2-4a2d-a919-cadc21025f7d)

- **ì œëª©**: [Zero-Shot Composed Image Retrieval with Textual Inversion](https://arxiv.org/abs/2303.15247)  
- **í•™íšŒ**: ICCV 2023 (Zhang et al.)  
- **ì½”ë“œ**: [CIRCO (GitHub)](https://github.com/miccunifi/CIRCO)  
- **í•µì‹¬ í‚¤ì›Œë“œ**: `Composed Image Retrieval`, `CIRCO`, `Textual Inversion`, `Zero-Shot`, `ICCV 2023`, `ZS-CIR`, `SEARLE`
- ì¶”ê°€!! : ìš©ì–´ê°€ í–‡ê°ˆë¦¬ëŠ”ë° `ZS-CIR`, `SEARLE` ê°€ ëª¨ë‘ ì´ ë…¼ë¬¸ì—ì„œ ê³µê°œí•œ ëª¨ë¸ì„ ì§€ì¹­í•©ë‹ˆë‹¤!!  
- `ZS-CIR` ëŠ”  Zero shot Composed Image Retrievalì˜ ì•½ì–´, `SEARLE`ì€  zero-Shot composEd imAge Retrieval with textuaL invErsion ì…ë‹ˆë‹¤!!  


---

### ğŸ” ì—°êµ¬ ë°°ê²½

CIRR(ICCV 2021)ì€ **ì¡°í•© ì´ë¯¸ì§€ ê²€ìƒ‰(Composed Image Retrieval, CIR)**ì„ ì •ì˜í–ˆì§€ë§Œ, í•™ìŠµ ê¸°ë°˜ ë°©ì‹ê³¼ ë‹¨ì¼ ì •ë‹µ ë¼ë²¨ë§ì— ì˜ì¡´í–ˆìŠµë‹ˆë‹¤.  
í•˜ì§€ë§Œ í˜„ì‹¤ ì‘ìš©ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìš”êµ¬ê°€ ìˆìŠµë‹ˆë‹¤:

> â€œìƒˆë¡œìš´ ë„ë©”ì¸ì—ì„œë„ í•™ìŠµ ì—†ì´(Zero-Shot)  
> ì°¸ì¡° ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ìˆ˜ì •ìœ¼ë¡œ ì›í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ì°¾ê³ ,  
> ë™ì‹œì— ë³µìˆ˜ì˜ ì •ë‹µì„ í—ˆìš©í•´ì•¼ í•œë‹¤!â€

ì´ë¥¼ ìœ„í•´ ICCV 2023ì—ì„œ ë°œí‘œëœ ì´ë²ˆ ë…¼ë¬¸ì€  
ZS-CIR ëª¨ë¸ ê³µê°œ!! â†’ Textual Inversionì„ í™œìš©í•œ ì œë¡œìƒ· CIR í”„ë ˆì„ì›Œí¬ ê³µê°œ  
ë˜í•œ  **CIRCO**ë¼ëŠ” ë°ì´í„° ì…‹ìœ¼ë¡œ, **ë” í˜„ì‹¤ì ì´ê³  ì •êµí•œ ë°ì´í„°ì…‹**ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.   

---

### ğŸ§  ì£¼ìš” ê¸°ì—¬

1. #### ì œë¡œìƒ· CIR í”„ë ˆì„ì›Œí¬ ì œì•ˆ(ZS-CIR)  
   - Textual Inversionì„ í™œìš©í•´ **ì°¸ì¡° ì´ë¯¸ì§€ë¥¼ ìƒˆë¡œìš´ ê°œë… í† í°**ìœ¼ë¡œ ì„ë² ë”©  
   - ìˆ˜ì • í…ìŠ¤íŠ¸ì™€ ê²°í•©í•˜ì—¬ **ì¡°í•© ì¿¼ë¦¬** í˜•ì„±  
   - ë°ì´í„°ì…‹ë³„ í•™ìŠµ ì—†ì´ ë‹¤ì–‘í•œ ë„ë©”ì¸ ì ìš© ê°€ëŠ¥  

2. #### CIRCO ë°ì´í„°ì…‹ êµ¬ì¶•  
   - **COCO 2017** ê¸°ë°˜ì˜ í˜„ì‹¤ ì´ë¯¸ì§€ ì‚¬ìš©  
   - ê°ì²´ ì¤‘ì‹¬(object-centric) + ë‹¤ì¤‘ ê°ì²´ í¬í•¨ ì¿¼ë¦¬  
   - ì‹¤ì œ ì¥ë©´ì—ì„œ **ê°ì²´ ì†ì„± ë³€ê²½ + ê°ì²´ ê°„ ê´€ê³„ ìˆ˜ì •** ë°˜ì˜  


3. #### ë²¤ì¹˜ë§ˆí¬ ë° ì œë¡œìƒ· ì„±ëŠ¥  
   - CIRR, FashionIQ, CIRCOì—ì„œ ì œë¡œìƒ· ì„±ëŠ¥ ê²€ì¦  
   - í•™ìŠµ ì—†ì´ë„ ì˜ë¯¸ ìˆëŠ” ì„±ëŠ¥ í™•ë³´  

---

### ğŸ§  ì£¼ìš” ê¸°ì—¬ (ìì„¸íˆ!!)

#### 1. ì œë¡œìƒ· CIR í”„ë ˆì„ì›Œí¬ ì œì•ˆ (**ZS-CIR**)  

![Image](https://github.com/user-attachments/assets/4973487e-1cc9-4000-a59f-dc9084e55a3b)

- **ê¸°ì¡´ ë¬¸ì œ**  
  - CIRR, FashionIQ ê°™ì€ ê¸°ì¡´ ë°ì´í„°ì…‹ì—ì„œëŠ” ëŒ€ë¶€ë¶„ ëª¨ë¸ì´ **í›ˆë ¨ ë°ì´í„° ê¸°ë°˜ íŒŒì¸íŠœë‹**ì„ ê±°ì³ì•¼ í–ˆìŒ  
  - ë”°ë¼ì„œ ìƒˆë¡œìš´ ë„ë©”ì¸ì´ë‚˜ unseen ì¹´í…Œê³ ë¦¬ì—ì„œëŠ” ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ì €í•˜ë˜ëŠ” ë¬¸ì œ ì¡´ì¬  

- **í•µì‹¬ ì•„ì´ë””ì–´**  
  - Textual Inversion ê¸°ë²•ì„ CIRì— ì ‘ëª©  
  - ì°¸ì¡° ì´ë¯¸ì§€ë¥¼ **ìƒˆë¡œìš´ í† í°(embedding)**ìœ¼ë¡œ ë³€í™˜ â†’ ë§ˆì¹˜ â€œë‹¨ì–´â€ì²˜ëŸ¼ í™œìš©  
  - ìˆ˜ì • í…ìŠ¤íŠ¸ì™€ ê²°í•© â†’ ìµœì¢…ì ìœ¼ë¡œ â€œì´ë¯¸ì§€+ë¬¸ì¥ ì¡°í•© ì¿¼ë¦¬â€ í˜•ì„±  

- **ì¥ì **  
  - ì¶”ê°€ í•™ìŠµ ì—†ì´ë„ ê²€ìƒ‰ ê°€ëŠ¥ (Zero-Shot)  
  - íŠ¹ì • ë„ë©”ì¸(íŒ¨ì…˜, ì‹¤ìƒí™œ ì´ë¯¸ì§€ ë“±)ì— êµ­í•œë˜ì§€ ì•Šê³  **ë²”ìš©ì„±** í™•ë³´  
  - ì¶”ë¡  ê³¼ì •ì´ ë‹¨ìˆœí•´ íš¨ìœ¨ì„±ë„ ë³´ì¥  

---

#### 2. CIRCO ë°ì´í„°ì…‹ êµ¬ì¶•  

> CIRCOëŠ” CIR ì—°êµ¬ì—ì„œ ì²˜ìŒìœ¼ë¡œ **ì œë¡œìƒ· ì¡°í•© ê²€ìƒ‰**ì„ ê°€ëŠ¥í•˜ê²Œ í–ˆì„ ë¿ ì•„ë‹ˆë¼,  
> ë°ì´í„°ì…‹ ì¸¡ë©´ì—ì„œë„  
> - **ë³µìˆ˜ì˜ ì •ë‹µ**  
> - **í˜„ì‹¤ì  ì´ë¯¸ì§€**  
> - **ë³µì¡í•œ ì¿¼ë¦¬ êµ¬ì„±**  
> ì„ ë°˜ì˜í•´ **CIR í‰ê°€ì˜ ì§ˆì  ìˆ˜ì¤€ì„ ëŒì–´ì˜¬ë¦° ê¸°ë…ë¹„ì  ì—°êµ¬**ì…ë‹ˆë‹¤.  

![Image](https://github.com/user-attachments/assets/4285a3a1-18ba-4d4f-9dc5-e8e43488940e)  

- **í˜„ì‹¤ì„±**  
  - MS-COCO 2017 ì´ë¯¸ì§€ ê¸°ë°˜  
  - íŠ¹ì • ë„ë©”ì¸(ì˜ˆ: íŒ¨ì…˜) í¸í–¥ì„ ì¤„ì´ê³ , ë‹¤ì–‘í•œ ë¬¼ì²´Â·ë°°ê²½Â·ê´€ê³„ì„±ì„ í¬í•¨  

- **ê°ì²´ ì¤‘ì‹¬ (Object-Centric)**  
  - ë‹¨ìˆœíˆ â€œì „ì²´ ì¥ë©´â€ì´ ì•„ë‹ˆë¼, íŠ¹ì • ê°ì²´ì˜ ì†ì„± ë³€í™”ë¥¼ ë°˜ì˜í•˜ëŠ” ì¿¼ë¦¬ ì œê³µ  
  - ì˜ˆ: â€œì‚¬ì§„ ì† ìë™ì°¨ëŠ” ë¹¨ê°„ìƒ‰ìœ¼ë¡œ ë°”ê¾¸ê³ , ì˜†ì— ìˆë˜ ê°•ì•„ì§€ëŠ” ê³ ì–‘ì´ë¡œ ë°”ê¿”ì¤˜.â€  

- **ë³µìˆ˜ ì •ë‹µ (Multi-Ground Truths)**  
  - ì¿¼ë¦¬ë‹¹ í‰ê·  4.53ê°œì˜ íƒ€ê¹ƒ ì´ë¯¸ì§€ ì¡´ì¬  
  - ê¸°ì¡´ FashionIQ ê°™ì€ ë‹¨ì¼ ì •ë‹µ êµ¬ì¡°ì˜ í•œê³„ë¥¼ ê·¹ë³µ  
  - False Negative ë¬¸ì œ ì™„í™” â†’ ê²€ìƒ‰ ëª¨ë¸ í‰ê°€ê°€ í›¨ì”¬ ê³µì •í•´ì§  

- **ë³µì¡í•œ ì§ˆì˜ (Complex Queries)**  
  - ê°ì²´ ì†ì„± ìˆ˜ì •ë¿ ì•„ë‹ˆë¼ **ë‹¤ì¤‘ ê°ì²´ ë° ê°ì²´ ê°„ ê´€ê³„**ë¥¼ í¬í•¨  
  - ë‹¨ìˆœí•œ â€œìƒ‰ìƒ ë³€ê²½â€ì„ ë„˜ì–´  
    - â€œì‚¬ëŒì´ ì•‰ì•„ ìˆë˜ ìœ„ì¹˜ì— ë‹¤ë¥¸ ì¸ë¬¼ì´ ì„œ ìˆë‹¤â€  
    - â€œê°œê°€ ìˆë˜ ìë¦¬ì— ê³ ì–‘ì´ê°€ ìˆë‹¤â€ ê°™ì€ ë³µí•© ì¿¼ë¦¬ë„ í¬í•¨  

---

#### 3. ë²¤ì¹˜ë§ˆí¬ ë° ì œë¡œìƒ· ì„±ëŠ¥  

![Image](https://github.com/user-attachments/assets/51f8721c-609a-4d2b-aa81-c147cf035d21)

- **í‰ê°€ ë°ì´í„°ì…‹**: CIRR, FashionIQ, CIRCO ë“± ì£¼ìš” CIR ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì œë¡œìƒ· ì„±ëŠ¥ ê²€ì¦  

- **FashionIQ (Validation Set)**  
  - **SEARLE (B/32)**: í‰ê·  R@10 = 22.89, R@50 = 42.53  
  - **SEARLE-XL (L/14)**: í‰ê·  R@10 = 25.56, R@50 = 46.23  
  - ê¸°ì¡´ ë°©ë²• ëŒ€ë¹„ í™•ì—°íˆ í–¥ìƒ, Basesì¼ë•ŒëŠ” í”„ë¡¬í¬íŠ¸ë¥¼ í•™ìŠµì‹œí‚¨ OTIë³´ë‹¤ ê·¸ëƒ¥ SEARLEê°€ ë” ì¢‹ì„ë–„ê°€ ë§ì•˜ìŒ!!  

- **CIRR (Test Set)**  
  - **SEARLE (B/32)**: Recall@1 = 24.27, Recall@5 = 53.22, Recall@10 = 66.82  
  - **SEARLE-XL (L/14)**: Recall@1 = 24.22, Recall@5 = 52.48, Recall@10 = 66.29  
  - SEARLEì´ í”„ë¡¬í¬íŠ¸ í•™ìŠµí•œê²ƒ ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì•˜ìŒ!@  
  - Subset Recall (ë” ì •ë°€í•œ í‰ê°€)ì—ì„œë„ SEARLE-XLì´ **Recall@3 = 88.19**ë¡œ SOTA ìˆ˜ì¤€ ì„±ëŠ¥ í™•ë³´  

- **ì˜ì˜**  
  - ë‹¨ìˆœí•œ Zero-Shot ì ‘ê·¼ì„ì—ë„ ë¶ˆêµ¬í•˜ê³ , FashionIQì™€ CIRRì—ì„œ **ê¸°ì¡´ í•™ìŠµ ê¸°ë°˜ ê¸°ë²•ë³´ë‹¤ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥**ì„ ê¸°ë¡  
  - íŠ¹íˆ CIRRì—ì„œëŠ” Recall@1ì´ 24%ë¥¼ ë„˜ìœ¼ë©°, í•™ìŠµ ì—†ì´ë„ **ì˜ë¯¸ ìˆëŠ” ê²€ìƒ‰ í’ˆì§ˆ**ì„ ë³´ì¥  
  - ì´ëŠ” CIR ì—°êµ¬ì—ì„œ Zero-Shot ì ‘ê·¼ì˜ ê°€ëŠ¥ì„±ì„ ìµœì´ˆë¡œ ì‹¤ì¦í•œ ì„±ê³¼ì´ë©°, ì´í›„ **CIReVL (ICLR 2024)**, **OSrCIR (CVPR 2025)** ê°™ì€ Training-Free ê³„ì—´ ì—°êµ¬ë¡œ ì´ì–´ì§€ëŠ” ê¸°ë°˜ì„ ë§ˆë ¨í•¨


---

### ğŸ§© ê²°ë¡ 

> **CIRCO (ICCV 2023)**ëŠ” **Textual Inversion ê¸°ë°˜ ì œë¡œìƒ· CIR(ZS-CIR)**ì„ ì œì•ˆí•˜ê³ ,  
> í˜„ì‹¤ì„±ê³¼ ì •êµí•¨ì„ ê°•í™”í•œ ìƒˆë¡œìš´ ë°ì´í„°ì…‹(**CIRCO**)ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.  
> ì´ëŠ” CIR ì—°êµ¬ê°€ **â€œí•™ìŠµ ê¸°ë°˜ + ë‹¨ì¼ ì •ë‹µâ€**ì—ì„œ **â€œì œë¡œìƒ· + ë‹¤ì¤‘ ì •ë‹µ + ë³µì¡ ì§ˆì˜â€**ë¡œ  
> ì§„í™”í•˜ëŠ” ì¶œë°œì ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.


---

### ğŸ§  (English) Zero-Shot Composed Image Retrieval with Textual Inversion!! **CIRCO**  
> In this work, the authors released the ZS-CIR model and also introduced the CIRCO dataset!!  

![Image](https://github.com/user-attachments/assets/1a4f2dcb-38b2-4a2d-a919-cadc21025f7d)

- **Title**: [Zero-Shot Composed Image Retrieval with Textual Inversion](https://arxiv.org/abs/2303.15247)  
- **Conference**: ICCV 2023 (Zhang et al.)  
- **Code**: [CIRCO (GitHub)](https://github.com/miccunifi/CIRCO)  
- **Key Keywords**: `Composed Image Retrieval`, `CIRCO`, `Textual Inversion`, `Zero-Shot`, `ICCV 2023`, `ZS-CIR`, `SEARLE`  
- Note!!: The terms `ZS-CIR` and `SEARLE` both refer to the model released in this paper.  
- `ZS-CIR` is the abbreviation of *Zero-Shot Composed Image Retrieval*, while `SEARLE` stands for *zero-Shot composEd imAge Retrieval with textuaL invErsion*.  

---

### ğŸ” Background

CIRR (ICCV 2021) defined **Composed Image Retrieval (CIR)** but relied heavily on training-based methods and single ground-truth labels.  
However, in real-world scenarios, new demands have emerged:

> â€œWe need to retrieve images in unseen domains,  
> without additional training (Zero-Shot),  
> using reference images + textual modifications,  
> while allowing multiple correct answers!â€  

To meet these demands, the ICCV 2023 paper proposed:  
- **ZS-CIR model** â†’ a Textual Inversion-based zero-shot CIR framework  
- **CIRCO dataset** â†’ a more realistic and fine-grained benchmark  

---

### ğŸ§  Key Contributions

1. #### Zero-Shot CIR Framework (ZS-CIR)  
   - Applied Textual Inversion to embed **reference images as new concept tokens**  
   - Combined with modification text to form **composed queries**  
   - Applicable across domains without dataset-specific training  

2. #### CIRCO Dataset  
   - Based on **COCO 2017** real-world images  
   - Object-centric queries including multiple objects  
   - Captures **attribute changes + object relationships** in natural scenes  

3. #### Benchmark & Zero-Shot Performance  
   - Evaluated on CIRR, FashionIQ, and CIRCO  
   - Achieved meaningful zero-shot performance without additional training  

---

### ğŸ§  Key Contributions (Detailed)

#### 1. Zero-Shot CIR Framework (**ZS-CIR**)  

![Image](https://github.com/user-attachments/assets/4973487e-1cc9-4000-a59f-dc9084e55a3b)

- **Problem**  
  - Previous datasets like CIRR and FashionIQ required **fine-tuning on training data**  
  - Performance dropped drastically on unseen domains or categories  

- **Core Idea**  
  - Incorporate Textual Inversion into CIR  
  - Convert reference images into **pseudo-word tokens (embeddings)**, treated like â€œwordsâ€  
  - Combine with modification text â†’ final **image+text composed query**  

- **Advantages**  
  - Enables retrieval without additional training (Zero-Shot)  
  - Domain-agnostic: works across fashion, real-life, and beyond  
  - Simple inference pipeline with efficient retrieval  

---

#### 2. CIRCO Dataset  

> CIRCO is not only the first to enable **zero-shot composed retrieval**,  
> but also advances CIR evaluation by providing:  
> - **Multiple ground truths**  
> - **Real-world images**  
> - **Complex queries**  
> â†’ raising the evaluation quality of CIR benchmarks  

![Image](https://github.com/user-attachments/assets/4285a3a1-18ba-4d4f-9dc5-e8e43488940e)  

- **Realism**  
  - Built on MS-COCO 2017  
  - Avoids domain bias (e.g., fashion-only) and covers diverse scenes, objects, and contexts  

- **Object-Centric**  
  - Queries reflect changes in specific objects, not only the global scene  
  - Example: â€œChange the car in the image to red, and replace the dog with a cat.â€  

- **Multiple Ground Truths**  
  - On average, 4.53 target images per query  
  - Overcomes the single-ground-truth limitation of FashionIQ  
  - Mitigates False Negative issue â†’ fairer evaluation of retrieval systems  

- **Complex Queries**  
  - Includes not only attribute modifications but also **multi-object and relational changes**  
  - Beyond â€œcolor change,â€ includes cases like:  
    - â€œA person sitting becomes another person standingâ€  
    - â€œReplace the dog with a catâ€  

---

#### 3. Benchmark & Zero-Shot Performance  

![Image](https://github.com/user-attachments/assets/51f8721c-609a-4d2b-aa81-c147cf035d21)

- **Evaluation Datasets**: CIRR, FashionIQ, CIRCO  

- **FashionIQ (Validation Set)**  
  - **SEARLE (B/32)**: Avg R@10 = 22.89, R@50 = 42.53  
  - **SEARLE-XL (L/14)**: Avg R@10 = 25.56, R@50 = 46.23  
  - In some cases, plain SEARLE outperformed the optimized OTI version  

- **CIRR (Test Set)**  
  - **SEARLE (B/32)**: Recall@1 = 24.27, Recall@5 = 53.22, Recall@10 = 66.82  
  - **SEARLE-XL (L/14)**: Recall@1 = 24.22, Recall@5 = 52.48, Recall@10 = 66.29  
  - SEARLE achieved better results than OTI-trained prompts in some settings  
  - Subset Recall: SEARLE-XL reached **Recall@3 = 88.19**, achieving SOTA-level performance  

- **Significance**  
  - Even with a pure Zero-Shot setup, SEARLE achieved **competitive performance compared to training-based approaches**  
  - On CIRR, Recall@1 exceeded 24%, proving **high-quality retrieval without training**  
  - This milestone validated the feasibility of Zero-Shot CIR, laying the groundwork for follow-up works such as **CIReVL (ICLR 2024)** and **OSrCIR (CVPR 2025)**  

---

### ğŸ§© Conclusion

> **CIRCO (ICCV 2023)** introduced **Textual Inversion-based Zero-Shot CIR (ZS-CIR / SEARLE)**  
> and established a new, more realistic dataset (**CIRCO**).  
> This work marked the evolution of CIR from **â€œtraining-based + single ground-truthâ€**  
> to **â€œzero-shot + multiple ground-truths + complex queries.â€**
