---
layout: post
title: " The Relentless Rise of Chinese AI Models!! A Look at Qwen3!! - 끝없는 중국AI모델의 발전!!  Qwen3 살펴보기!! 🇨🇳🚀"
author: [DrFirst]
date: 2025-04-29 11:00:00 +0900
categories: [AI, Experiment]
tags:  [Qwen3, LLM, Open Source]
lastmod : 2025-04-29 11:00:00
sitemap :
  changefreq : weekly
  priority : 0.9
---

## (English ver) The Relentless Rise of Chinese AI Models!! A Look at Qwen3!! 🇨🇳🚀

## 🚀 Qwen 3 Released!

📖 **Open-source Name**: Qwen3  
🏢 **Developer**: Alibaba Cloud  
🎯 **Recommended for**: Researchers, engineers, developers interested in open-source LLMs  
🌟 **One-liner Summary**: Could it surpass DeepSeek and rise as China's leading open-source model!!!?

---

### 🧠 What is Qwen 3?

![Qwen3](https://github.com/user-attachments/assets/2a2f4479-eb5b-4d84-913d-53388071fb0e)
- Qwen 3 is a new **open-source large language model (LLM)** released by Alibaba Cloud on **April 29, 2025**.
- It follows the lineage of Qwen-1 (Sept 2023), Qwen-2 (June 2024), and Qwen-2.5 (Jan 2025)!

### Top 3 Features of the Model!!

- **Hybrid Thinking Modes**: Hybrid reasoning! The model switches between two thinking modes!
  - Users can determine the reasoning depth (mode) based on the question.
  - 🧩 **Thinking Mode** a.k.a. DeepSeek style: Performs **step-by-step reasoning** before final output.
  - ⚡ **Non-Thinking Mode**: Provides **fast and near-instant answers**, best for simple queries where speed matters.
  - Example code using `enable_thinking` might look like:  
  ```python
    text = tokenizer.apply_chat_template(
      messages,
      tokenize=False,
      add_generation_prompt=True,
      enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.
  )
  ```

- **Supports 119 Languages!**  
  - Officially supports 119 languages and dialects!
  - 🌍 Language Family Coverage Table:

  | Language Family     | Languages & Dialects |
  |---------------------|----------------------|
  | **Indo-European**   | English, French, Portuguese, German, Romanian, Swedish, Danish, Bulgarian, Russian, Czech, Greek, Ukrainian, Spanish, Dutch, Slovak, Croatian, Polish, Lithuanian, Norwegian Bokmål, Norwegian Nynorsk, Persian, Slovenian, Gujarati, Latvian, Italian, Occitan, Nepali, Marathi, Belarusian, Serbian, Luxembourgish, Venetian, Assamese, Welsh, Silesian, Asturian, Chhattisgarhi, Awadhi, Maithili, Bhojpuri, Sindhi, Irish, Faroese, Hindi, Punjabi, Bengali, Oriya, Tajik, Eastern Yiddish, Lombard, Ligurian, Sicilian, Friulian, Sardinian, Galician, Catalan, Icelandic, Tosk Albanian, Limburgish, Dari, Afrikaans, Macedonian, Sinhala, Urdu, Magahi, Bosnian, Armenian |
  | **Sino-Tibetan**    | Chinese (Simplified, Traditional, Cantonese), Burmese |
  | **Afro-Asiatic**    | Arabic (Standard, Najdi, Levantine, Egyptian, Moroccan, Mesopotamian, Ta’izzi-Adeni, Tunisian), Hebrew, Maltese |
  | **Austronesian**    | Indonesian, Malay, Tagalog, Cebuano, Javanese, Sundanese, Minangkabau, Balinese, Banjar, Pangasinan, Iloko, Waray (Philippines) |
  | **Dravidian**       | Tamil, Telugu, Kannada, Malayalam |
  | **Turkic**          | Turkish, North Azerbaijani, Northern Uzbek, Kazakh, Bashkir, Tatar |
  | **Tai-Kadai**       | Thai, Lao |
  | **Uralic**          | Finnish, Estonian, Hungarian |
  | **Austroasiatic**   | Vietnamese, Khmer |
  | **Other**           | Japanese, Korean, Georgian, Basque, Haitian, Papiamento, Kabuverdianu, Tok Pisin, Swahili |

- **Agentic Interaction Abilities!**
  - MCP and agentic reasoning are becoming essential in LLMs.
  - Qwen3 is optimized for **coding and agent-style reasoning**, and it has enhanced support for **MCP (Multi-Component Planning)**.
  - Check out the official [demo video](https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/mcp.mov) for a clear example!

---

### 🛠️ How the Model Was Trained

> Beyond vast data, Qwen3 places strong emphasis on **post-training refinement**.

#### A. **Pre-training**: A massive dataset of **36 trillion tokens**!
- Qwen2.5 used 18T tokens, while Qwen3 uses **double that: 36T tokens**.
- Sources included web data **and PDFs** (processed using Qwen2.5-VL, Math, Coder).

**Three-stage pretraining pipeline:**
1. **S1: Foundational Language Pretraining**  
   - 30T+ tokens, 4K context length
2. **S2: Knowledge-intensive Domain Focus**  
   - Added 5T tokens with STEM, coding, reasoning focus
3. **S3: Long Context Optimization**  
   - Extended to 32K context using high-quality long-form data

> Same performance, smaller size! ✨

| Qwen3 Model      | ≈ Qwen2.5 Model |
|------------------|------------------|
| Qwen3-1.7B-Base  | Qwen2.5-3B       |
| Qwen3-4B-Base    | Qwen2.5-7B       |
| Qwen3-8B-Base    | Qwen2.5-14B      |
| Qwen3-14B-Base   | Qwen2.5-32B      |
| Qwen3-32B-Base   | Qwen2.5-72B      |

---

#### B. **Post-training**: Four stages for hybrid mode mastery!

![post_train](https://github.com/user-attachments/assets/150ab9be-4668-486b-9c8f-cfab75141557)

> While the post-training framework is extensive, understanding how it works requires deeper exploration.

1. **Stage 1: Chain-of-Thought (CoT) Cold Start**
   - Fine-tuning with long-form CoT data across domains (math, logic, code, etc.)
2. **Stage 2: Reasoning-based Reinforcement Learning**
   - Applied RL using rule-based rewards to enhance exploration and exploitation
3. **Stage 3: Thinking Mode Fusion**
   - Blending CoT and instruction data for hybrid reasoning + fast response
4. **Stage 4: General Domain RL**
   - Fine-tuning over 20 tasks to increase general ability and reliability

---

### 📈 Model Performance?

> Each new model boasts improved performance, but **real-world use cases** and feedback matter most.

![metrics](https://github.com/user-attachments/assets/f131895b-9d64-42c0-b4f8-1d32fb97ffbe)

- Available model sizes: 0.6B, 1.7B, 4B, 8B, 14B, 32B, 30B-A3B, 235B-A22B
- Models like Qwen3-32B and 235B-A22B were first trained and distilled into smaller models

### 🔹 Qwen3 Model Spec Summary

| Model            | Layers | Heads (Q / KV) | Tie Embedding | Context Length |
|------------------|--------|----------------|----------------|----------------|
| Qwen3-0.6B       | 28     | 16 / 8         | Yes            | 32K            |
| Qwen3-1.7B       | 28     | 16 / 8         | Yes            | 32K            |
| Qwen3-4B         | 36     | 32 / 8         | Yes            | 32K            |
| Qwen3-8B         | 36     | 32 / 8         | No             | 128K           |
| Qwen3-14B        | 40     | 40 / 8         | No             | 128K           |
| Qwen3-32B*       | 64     | 64 / 8         | No             | 128K           |

### 🔹 Qwen3 MoE Models

> Models like A3B and A22B use a Mixture-of-Experts (MoE) structure. 
> A3B = "Activated 3 experts per block"

| Model              | Layers | Heads (Q / KV) | Experts (Total / Activated) | Context Length |
|--------------------|--------|----------------|------------------------------|----------------|
| Qwen3-30B-A3B      | 48     | 32 / 4         | 128 / 8                      | 128K           |
| Qwen3-235B-A22B*   | 94     | 64 / 4         | 128 / 8                      | 128K           |

---

### 🧩 Personal Thoughts

As a newly released model, I haven’t yet had the chance to run or benchmark Qwen3 against others. 

However, seeing how quickly new versions are released with cutting-edge features, I truly appreciate the effort and investment behind it.

It’s exciting that such a powerful model is open-source, and I look forward to exploring Qwen3 tuning and optimization use cases soon. 🚀

---

### References

- [Qwen3 Official Blog](https://qwenlm.github.io/blog/qwen3/)
- [Qwen3 GitHub](https://github.com/QwenLM/Qwen3)
- [Qwen3 on Hugging Face](https://huggingface.co/docs/transformers/en/model_doc/qwen3)
- [Qwen3 Chat Demo](https://chat.qwen.ai/)


--- 
## (한국어 ver) 끝없는 중국AI모델의 발전!!  Qwen3 살펴보기!! 🇨🇳🚀

## 🚀 Qwen 3 공개!

📖 **오픈소스명**: Qwen3  
🏢 **개발사**: Alibaba Cloud  
🎯 **추천 대상**: 오픈소스 LLM에 관심 있는 연구자, 엔지니어, 개발자  
🌟 **한줄 요약**: 딥식을 넘어서는 중국 대표의 오픈소스 모델로 등극!!!?  

---

### 🧠 Qwen 3란?

![Qwen3](https://github.com/user-attachments/assets/2a2f4479-eb5b-4d84-913d-53388071fb0e)
- Qwen 3는 Alibaba Cloud가 2025년 4월 29일!! 공개한 새로운 **오픈소스 대규모 언어 모델(LLM)**
- Qwen-1(23.9월), Qwen-2(24.6월), Qwen-2.5(25.1월)에 이은 새로운 모델!!  

### 모델의 주요 특징 3가지!!  

- **Hybrid Thinking Modes** : 하이브리드 씽킹!! 2가지 모드를 오가며 답변합니다!!!  
  - 사고모드와 비사고모드를 사용자가 질문에 따라 정해가며 진행할수 있어요!!
  - 🧩 사고 모드 (Thinking Mode) a.k.a deepseek 모드: 모델이 최종 답변을 제공하기 전에 **단계별로 차근차근 추론**을 수행
  - ⚡ 비사고 모드 (Non-Thinking Mode) : **빠르고 거의 즉각적인 응답**을 제공, 복잡한 사고보다는 **빠른 응답 속도**가 중요한 간단한 질문에 적합
  - 코드로 보면 아래와 같이 "enable_thinking" 의 항목을 어떻게 설정하냐에 따라 모드를 정할 수 있지요~!
  ```python
    text = tokenizer.apply_chat_template(
      messages,
      tokenize=False,
      add_generation_prompt=True,
      enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.
  )
  ```

- **119개 언어**로 사용가능!! 
  - 언어능력이 어마어마합니다~! 공식적으로 방언을 포함한 119개 언어를 사용할수 있다고해요~!
  - 🌍 언어 계통별 지원 언어 목록  

  | 언어 계통           | 언어 및 방언 |
  |----------------------|--------------|
  | **인도유럽어족**     | 영어, 프랑스어, 포르투갈어, 독일어, 루마니아어, 스웨덴어, 덴마크어, 불가리아어, 러시아어, 체코어, 그리스어, 우크라이나어, 스페인어, 네덜란드어, 슬로바키아어, 크로아티아어, 폴란드어, 리투아니아어, 노르웨이어(보크몰), 노르웨이어(뉘노르스크), 페르시아어, 슬로베니아어, 구자라트어, 라트비아어, 이탈리아어, 오크어, 네팔어, 마라티어, 벨라루스어, 세르비아어, 룩셈부르크어, 베네치아어, 아삼어, 웨일스어, 실레시아어, 아스투리아어, 찟띠스가르어, 아와디어, 마이틸리어, 보즈푸리어, 신디어, 아일랜드어, 페로어, 힌디어, 펀자브어, 벵골어, 오리야어, 타지크어, 동부 이디시어, 롬바르드어, 리구리아어, 시칠리아어, 프리울리어, 사르데냐어, 갈리시아어, 카탈루냐어, 아이슬란드어, 토스크 알바니아어, 림뷔르흐어, 다리어, 아프리칸스어, 마케도니아어, 싱할라어, 우르두어, 마가히어, 보스니아어, 아르메니아어 |
  | **중국티베트어족**  | 중국어(간체, 번체, 광둥어), 버마어 |
  | **아프리카아시아어족** | 아랍어(표준, 나지디, 레반트, 이집트, 모로코, 메소포타미아, 타이즈-아덴, 튀니지), 히브리어, 몰타어 |
  | **오스트로네시아어족** | 인도네시아어, 말레이어, 타갈로그어, 세부아노어, 자바어, 순다어, 미낭카바우어, 발리어, 반자르어, 팡가시난어, 일로코어, 와라이어(필리핀) |
  | **드라비다어족**     | 타밀어, 텔루구어, 칸나다어, 말라얄람어 |
  | **튀르크어족**       | 터키어, 북아제르바이잔어, 북우즈벡어, 카자흐어, 바시키르어, 타타르어 |
  | **타이카다이어족**   | 태국어, 라오어 |
  | **우랄어족**         | 핀란드어, 에스토니아어, 헝가리어 |
  | **오스트로아시아어족** | 베트남어, 크메르어 |
  | **기타**             | 일본어, 한국어, 그루지야어(조지아어), 바스크어, 아이티어, 파피아멘토어, 카보베르디아누어, 토크 피신어, 스와힐리어 |



- **에이전트와의 소통**능력!! 
  - MCP 등 AI agent와의 소통이 LLM의 필수 요소가 되어가고있습니다!!  
  - 이번 Qwen3도  **코딩 능력과 에이전트적(agentic) 사고 능력**을 중심으로 최적화되며,  
  - **MCP(Multi-Component Planning)**에 대한 지원도 강화되었다고합니다!  
  - 공식 사이트에서 [영상](https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/mcp.mov)으로 예시를 들고있어 꼭 확인해보세요~!!  

### 모델의 학습방식!!

> 모델의 학습에 있어 단순히 방대한 데이터를 썻을 뿐만 아니라 Post-training의 중요성도 강조했습니다!!

A. **Pre-training** : **수십조 개의 단어**로 학습된 방대한 데이터셋 
  - Qwen2.5도 18 trillion token으로 학습되었는데,  
  - 이번 Qwen3은 2배인 36 trillion, 즉 **36조**개의 토큰으로 학습!!  
  - 단순한 웹 데이터 뿐만 아니라 PDF 등으로부터도 학습되었으며, 2.5 Math와 Coder, VL등을 활용해서 다양한 데이터를 활용!!  
  - 3 단계로 이루어진 학습과정!!
    1. **S1: 기초 언어 능력 습득**
      - 30조+ 토큰  
      - 컨텍스트 길이: **4K**  
      - 일반 언어 지식 및 기초 학습  

    2. **S2: 지식 중심 데이터 비중 강화**
      - **STEM**, **코딩**, **추론** 중심 데이터 비중 증가  
      - 추가로 **5조 토큰** 학습  

    3. **S3: 롱 컨텍스트 최적화**
      - **32K 토큰** 입력을 처리할 수 있도록 설계  
      - 고품질 장문 데이터를 활용하여 긴 입력에 대한 처리 능력 강화  

  - 이를 통해서 2.5랑 비교시 더 작은 모델도 동일한 성능을 보인다고합니다~!  

    > 2/3정도의 사이즈 모델로 같은 성능을!!?  

    | Qwen3 모델       | ≈ Qwen2.5 모델 |
    |------------------|----------------|
    | Qwen3-1.7B-Base  | Qwen2.5-3B     |
    | Qwen3-4B-Base    | Qwen2.5-7B     |
    | Qwen3-8B-Base    | Qwen2.5-14B    |
    | Qwen3-14B-Base   | Qwen2.5-32B    |
    | Qwen3-32B-Base   | Qwen2.5-72B    |



B. Post-training : **Hybrid Thinking Modes**를 위한 4단계 학습방법!!!

  ![post_train](https://github.com/user-attachments/assets/150ab9be-4668-486b-9c8f-cfab75141557)

 > 내용이 장황하게 나와있는데,, 실질적으로 어떻게 작용하는지는 세부적인 공부를 해야겠습니다!!  

  - 1단계: Chain-of-Thought (CoT) Cold Start  
    - 다양한 과제와 도메인(수학, 코딩, 논리 추론, STEM 등)에 대한 **장문의 사고 체인 데이터**를 활용하여 파인튜닝
    - 목표: **기초적인 추론 능력**을 모델에 부여  

  - 2단계: 추론 기반 강화학습 (Reasoning-based RL)
    - **연산 자원**을 대폭 확장하여 강화학습(RL)을 적용
    - **규칙 기반 보상 함수(rule-based reward)**를 사용하여 모델의 **탐색(Explore)과 활용(Exploit)** 능력 향상

  - 3단계: 사고/비사고 모드 통합 (Thinking Mode Fusion)
    - 2단계의 향상된 사고 모델을 사용해 생성한 데이터를 기반으로 **장문 CoT 데이터 + 일반 명령어 튜닝(instruction tuning) 데이터**를 결합하여 재파인튜닝  
    - 효과: **깊은 사고 + 빠른 응답**이 공존하는 하이브리드 모델 탄생

  - 4단계: 범용 강화학습 (General RL)
    - **20개 이상의 일반 도메인 과제**에 대해 강화학습 적용
    - 모델의 **범용 능력 강화** 및 **비자연스러운 응답 제거 및 안정성 개선**



### 모델의 성능은!!?  
  > 모델의 성능은,, 새로나오는 모델마다 우수성을 강조해서,, 정량적 평가 보다는 직접 사용해보는 후기가 중요해지는것 같습니다!!  
  
  ![metrics](https://github.com/user-attachments/assets/f131895b-9d64-42c0-b4f8-1d32fb97ffbe)

  - 다양한 사이즈: 0.6B, 1.7B, 4B, 8B, 14B, 32B and 30B-A3B, 235B-A22B 등. Github를 통해 공개되었으며 곧 Huggingface에 모델이 올라오겠지요~!?  
  - 235B-A22B와 32B 두개의 모델을 먼저 학습시키고!! Strong to weak distillation을 통하여 경량모델들을 만들었네요!!  


### 🔹 Qwen3 모델 스펙 요약

| Models           | Layers | Heads (Q / KV) | Tie Embedding | Context Length |
|:-----------------|:-------|:--------------|:--------------|:---------------|
| Qwen3-0.6B        | 28     | 16 / 8         | Yes            | 32K            |
| Qwen3-1.7B        | 28     | 16 / 8         | Yes            | 32K            |
| Qwen3-4B          | 36     | 32 / 8         | Yes            | 32K            |
| Qwen3-8B          | 36     | 32 / 8         | No             | 128K           |
| Qwen3-14B         | 40     | 40 / 8         | No             | 128K           |
| Qwen3-32B*         | 64     | 64 / 8         | No             | 128K           |

### 🔹 Qwen3 MoE 모델!!   
  > 기존 모델명과 다르게 A3B A22B 가 붙어있지요!? 이는 MoE의 전문가수를 의미합니다!!  
  > A3B는 Activated 3 out of many experts in a Block의 줄임말 3개의 expert를 사용한 모델이라고 이해하면 됩니다~!  

| Models              | Layers | Heads (Q / KV) | # Experts (Total / Activated) | Context Length |
|:--------------------|:-------|:--------------|:------------------------------|:---------------|
| Qwen3-30B-A3B        | 48     | 32 / 4         | 128 / 8                        | 128K           |
| Qwen3-235B-A22B*      | 94     | 64 / 4         | 128 / 8                        | 128K           |


### 🧩 개인적인 생각

금일 공개된 모델로써!!  
아직 직접 모델을 코드로 돌려보지도 못했고 성능도 직접 다른모델과 많이 비교해보지 못했습니다!.  
그러나!! 이렇게 빠른 주기로 새로운 모델을 오픈하며,  
새로운 기술들을 적용하는 속도를 보면  
정말 많은 노력과 투자가 있었겠구나 느끼게 됩니다!!
이런 모델이 오픈소스임에 감사하며!!  
앞으로 이 Qwen 3를 활용한 튜닝 방법, 최적화 사례도 하나씩 다뤄보고 싶습니다. 🚀

### 참고  

- [Qwen3 공식 블로그](https://qwenlm.github.io/blog/qwen3/)
- [Qwen3 Github](https://github.com/QwenLM/Qwen3)
- [Qwen3 Huggingface](https://huggingface.co/docs/transformers/en/model_doc/qwen3)
- [Qwen3 Chat](https://chat.qwen.ai/)