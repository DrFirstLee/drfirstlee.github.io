---
layout: post
title: "Understanding Vision Transformers (ViT) - ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ ì´í•´í•˜ê¸°"
author: [DrFirst]
date: 2025-03-24 09:00:00 +0900
categories: [AI, Research]
tags: [ViT, Vision Transformer, AI]
lastmod : 2025-03-24 09:00:00
sitemap :
  changefreq : weekly
  priority : 0.9
---
# Hello, everyone! ğŸ‘‹

Today, letâ€™s dive into the world of **Vision Transformers (ViT)** â€” a groundbreaking approach in computer vision  
that applies the **Transformer architecture**, originally designed for natural language processing, to image understanding.

Unlike traditional CNNs that rely on convolutions to extract spatial features,  
**ViT splits an image into patches**, embeds them, and processes them through a Transformer encoder â€”  
treating the image more like a sequence of tokens than a grid of pixels.

This paradigm shift has opened new doors in computer vision research and has shown competitive or even superior results  
on large-scale image recognition tasks, especially when trained on massive datasets.

I'll be sharing more about ViTâ€™s core ideas, its architecture, and practical implications in future posts â€”  
including comparisons with CNNs and hybrid models.

Stay tuned for more deep dives into the intersection of **AI** and **vision**. ğŸ‘ï¸ğŸ¤–

â€” *DrFirst*

---

## ğŸ§ª Sample Code: Using ViT in Python

Hereâ€™s a simple example of how to use a pre-trained Vision Transformer model from Hugging Face to classify an image:

```python
from transformers import ViTFeatureExtractor, ViTForImageClassification
from PIL import Image
import requests
import torch

# Load image from URL
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png"
image = Image.open(requests.get(url, stream=True).raw)

# Load feature extractor and model
extractor = ViTFeatureExtractor.from_pretrained("google/vit-base-patch16-224")
model = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224")

# Preprocess the image
inputs = extractor(images=image, return_tensors="pt")

# Run inference
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits
    predicted_class_idx = logits.argmax(-1).item()

# Print result
print("Predicted class:", model.config.id2label[predicted_class_idx])
```

Make sure to install the required packages:

```bash
pip install transformers torch pillow
```

---

# ì•ˆë…•í•˜ì„¸ìš”, ì—¬ëŸ¬ë¶„! ğŸ‘‹

ì˜¤ëŠ˜ì€ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œ í˜ì‹ ì ì¸ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆëŠ”  
**ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸(Vision Transformer, ViT)**ì— ëŒ€í•´ ì†Œê°œë“œë¦¬ë ¤ í•©ë‹ˆë‹¤.

ViTëŠ” ì›ë˜ ìì—°ì–´ ì²˜ë¦¬(NLP)ë¥¼ ìœ„í•´ ê³ ì•ˆëœ **íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°**ë¥¼  
ì´ë¯¸ì§€ ì¸ì‹ì— ì ìš©í•œ ëª¨ë¸ë¡œ, ê¸°ì¡´ì˜ í•©ì„±ê³± ì‹ ê²½ë§(CNN) ë°©ì‹ê³¼ëŠ” ì „í˜€ ë‹¤ë¥¸ ê´€ì ì„ ì œì‹œí•©ë‹ˆë‹¤.

ì´ë¯¸ì§€ë¥¼ ì‘ì€ íŒ¨ì¹˜ë“¤ë¡œ ë‚˜ëˆ„ê³  ì´ë¥¼ ì¼ì¢…ì˜ í† í°ìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ì— ì…ë ¥í•¨ìœ¼ë¡œì¨,  
**ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ ë°ì´í„°ì²˜ëŸ¼ ì²˜ë¦¬**í•˜ëŠ” ê²ƒì´ ViTì˜ í•µì‹¬ ì•„ì´ë””ì–´ì…ë‹ˆë‹¤.

ì´ëŸ¬í•œ ë°©ì‹ì€ íŠ¹íˆ **ëŒ€ê·œëª¨ ë°ì´í„°ì…‹**ì—ì„œ í•™ìŠµí•  ê²½ìš°, ê¸°ì¡´ CNNì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì´ë©°  
ì»´í“¨í„° ë¹„ì „ ì—°êµ¬ì˜ ìƒˆë¡œìš´ ì§€í‰ì„ ì—´ê³  ìˆìŠµë‹ˆë‹¤.

---

## ğŸ§ª ViT íŒŒì´ì¬ ìƒ˜í”Œ ì½”ë“œ

ì•„ë˜ëŠ” Hugging Faceì—ì„œ ì œê³µí•˜ëŠ” ì‚¬ì „í•™ìŠµëœ ViT ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ê°„ë‹¨í•œ ì˜ˆì œì…ë‹ˆë‹¤:

```python
from transformers import ViTFeatureExtractor, ViTForImageClassification
from PIL import Image
import requests
import torch

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png"
image = Image.open(requests.get(url, stream=True).raw)

extractor = ViTFeatureExtractor.from_pretrained("google/vit-base-patch16-224")
model = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224")

inputs = extractor(images=image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits
    predicted_class_idx = logits.argmax(-1).item()

print("ì˜ˆì¸¡ëœ í´ë˜ìŠ¤:", model.config.id2label[predicted_class_idx])
```

í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ë¨¼ì € ì„¤ì¹˜í•˜ì„¸ìš”:

```bash
pip install transformers torch pillow
```

ì•ìœ¼ë¡œì˜ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ViTì˜ êµ¬ì¡°, í•µì‹¬ ê°œë…, ê·¸ë¦¬ê³  CNNì´ë‚˜ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ê³¼ì˜ ë¹„êµ ë“±  
ì‹¤ì§ˆì ì¸ ë¶„ì„ê³¼ í•¨ê»˜ ì¢€ ë” ê¹Šì´ ìˆê²Œ ë‹¤ë¤„ë³´ê² ìŠµë‹ˆë‹¤.

**AIì™€ ë¹„ì „ì˜ êµì°¨ì **ì— ëŒ€í•œ ë” ë§ì€ ì´ì•¼ê¸°, ê¸°ëŒ€í•´ì£¼ì„¸ìš”! ğŸ‘ï¸ğŸ¤–

![image](https://private-user-images.githubusercontent.com/43365171/425920688-75fe0edc-f996-486c-bb88-c9de733042d6.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDI3OTAyMTgsIm5iZiI6MTc0Mjc4OTkxOCwicGF0aCI6Ii80MzM2NTE3MS80MjU5MjA2ODgtNzVmZTBlZGMtZjk5Ni00ODZjLWJiODgtYzlkZTczMzA0MmQ2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAzMjQlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMzI0VDA0MTgzOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTVkMjRhOWExZWZhMzcyM2E4MjkzNjU3NWUwNjY4ZmMwZWQyYmY5OTUyNjQ1ZTE3NTZmNGQ3M2ZkYThmZmQ4YTYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.rpNnHhr5GCAUNs-0OSaEkMbnWd25FrAurbxRPXJFxSo){: width="20%" height="20%"}

â€” *ì¼ë“±ë°•ì‚¬*
