---
layout: post
title: "Understanding Vision Transformers (ViT) - 비전 트랜스포머 이해하기"
author: [DrFirst]
date: 2025-03-24 09:00:00 +0900
categories: [AI, Research]
tags: [ViT, Vision Transformer, AI]
lastmod : 2025-03-24 09:00:00
sitemap :
  changefreq : weekly
  priority : 0.9
---
# Hello, everyone! 👋

Today, let’s dive into the world of **Vision Transformers (ViT)** — a groundbreaking approach in computer vision  
that applies the **Transformer architecture**, originally designed for natural language processing, to image understanding.

Unlike traditional CNNs that rely on convolutions to extract spatial features,  
**ViT splits an image into patches**, embeds them, and processes them through a Transformer encoder —  
treating the image more like a sequence of tokens than a grid of pixels.

This paradigm shift has opened new doors in computer vision research and has shown competitive or even superior results  
on large-scale image recognition tasks, especially when trained on massive datasets.

I'll be sharing more about ViT’s core ideas, its architecture, and practical implications in future posts —  
including comparisons with CNNs and hybrid models.

Stay tuned for more deep dives into the intersection of **AI** and **vision**. 👁️🤖

— *DrFirst*

---

## 🧪 Sample Code: Using ViT in Python

Here’s a simple example of how to use a pre-trained Vision Transformer model from Hugging Face to classify an image:

```python
from transformers import ViTFeatureExtractor, ViTForImageClassification
from PIL import Image
import requests
import torch

# Load image from URL
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png"
image = Image.open(requests.get(url, stream=True).raw)

# Load feature extractor and model
extractor = ViTFeatureExtractor.from_pretrained("google/vit-base-patch16-224")
model = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224")

# Preprocess the image
inputs = extractor(images=image, return_tensors="pt")

# Run inference
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits
    predicted_class_idx = logits.argmax(-1).item()

# Print result
print("Predicted class:", model.config.id2label[predicted_class_idx])
```

Make sure to install the required packages:

```bash
pip install transformers torch pillow
```

---

# 안녕하세요, 여러분! 👋

오늘은 컴퓨터 비전 분야에서 혁신적인 접근 방식으로 주목받고 있는  
**비전 트랜스포머(Vision Transformer, ViT)**에 대해 소개드리려 합니다.

ViT는 원래 자연어 처리(NLP)를 위해 고안된 **트랜스포머 구조**를  
이미지 인식에 적용한 모델로, 기존의 합성곱 신경망(CNN) 방식과는 전혀 다른 관점을 제시합니다.

이미지를 작은 패치들로 나누고 이를 일종의 토큰으로 간주하여 트랜스포머 인코더에 입력함으로써,  
**이미지를 하나의 시퀀스 데이터처럼 처리**하는 것이 ViT의 핵심 아이디어입니다.

이러한 방식은 특히 **대규모 데이터셋**에서 학습할 경우, 기존 CNN을 능가하는 성능을 보이며  
컴퓨터 비전 연구의 새로운 지평을 열고 있습니다.

---

## 🧪 ViT 파이썬 샘플 코드

아래는 Hugging Face에서 제공하는 사전학습된 ViT 모델을 이용하여 이미지를 분류하는 간단한 예제입니다:

```python
from transformers import ViTFeatureExtractor, ViTForImageClassification
from PIL import Image
import requests
import torch

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png"
image = Image.open(requests.get(url, stream=True).raw)

extractor = ViTFeatureExtractor.from_pretrained("google/vit-base-patch16-224")
model = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224")

inputs = extractor(images=image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits
    predicted_class_idx = logits.argmax(-1).item()

print("예측된 클래스:", model.config.id2label[predicted_class_idx])
```

필요한 패키지를 먼저 설치하세요:

```bash
pip install transformers torch pillow
```

앞으로의 포스트에서는 ViT의 구조, 핵심 개념, 그리고 CNN이나 하이브리드 모델과의 비교 등  
실질적인 분석과 함께 좀 더 깊이 있게 다뤄보겠습니다.

**AI와 비전의 교차점**에 대한 더 많은 이야기, 기대해주세요! 👁️🤖

![image](https://private-user-images.githubusercontent.com/43365171/425920688-75fe0edc-f996-486c-bb88-c9de733042d6.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDI3OTAyMTgsIm5iZiI6MTc0Mjc4OTkxOCwicGF0aCI6Ii80MzM2NTE3MS80MjU5MjA2ODgtNzVmZTBlZGMtZjk5Ni00ODZjLWJiODgtYzlkZTczMzA0MmQ2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAzMjQlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMzI0VDA0MTgzOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTVkMjRhOWExZWZhMzcyM2E4MjkzNjU3NWUwNjY4ZmMwZWQyYmY5OTUyNjQ1ZTE3NTZmNGQ3M2ZkYThmZmQ4YTYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.rpNnHhr5GCAUNs-0OSaEkMbnWd25FrAurbxRPXJFxSo){: width="20%" height="20%"}

— *일등박사*
