---
layout: post
title: "ğŸ§  OSrCIR: Reason-before-Retrieve for Composed Image Retrieval"
author: [DrFirst]
date: 2025-07-31 09:00:00 +0900
categories: [AI, Research]
tags: [MLLM, CIR, Image Retrieval, CVPR 2025, Reasoning, One-Stage, CVPR, CVPR 2025 ]
sitemap :
  changefreq : monthly
  priority : 0.8

---

---
### ğŸ§  OSrCIR: Reason-before-Retrieve for Compositional Image Retrieval

![Image](https://github.com/user-attachments/assets/83515f70-2856-49ec-8f95-5db4d6f1777d)

- **Title**: [Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/pdf/2412.11077)  
- **Conference**: CVPR 2025 (Highlight paper, Yuanmin Tang et al.)  
- **Code**: [OSrCIR (GitHub)](https://github.com/Pter61/osrcir)  
- **Keywords**: `Composed Image Retrieval`, `Chain-of-Thought`, `One-Stage`, `MLLM`, `Zero-Shot`

---

### ğŸ“Œ 3-Sentence Summary

1. Previous **Composed Image Retrieval (CIR)** [studies](https://drfirstlee.github.io/posts/CIReVL/) adopted a **two-stage structure** (Image Captioning â†’ Text Reasoning).  
2. OSrCIR allows the MLLM to **directly reason over the reference image**, inferring the target imageâ€™s features without relying on intermediate text.  
3. As a result, it improves both accuracy and speed, operating with **zero-shot inference only, without any training**.  

---

### ğŸ” Limitations of Previous CIR Structures

| Approach | Structure | Limitation |
|----------|-----------|------------|
| Two-Stage CIR | (1) Image â†’ Caption (2) Text â†’ Reasoning â†’ Retrieval | Loss of image information, reasoning errors |
| Text-Only Reasoning | Reference image passed only via text | Hard to reflect visual attributes |
| MLLM-based QA | Reasoning through Q&A | Time-consuming, inconsistent |

â†’ In short, **using text as an intermediate step inherently causes information loss**.  

---

### ğŸŒ± Core Idea of OSrCIR  

> â€œ**Reason first. Then retrieve.**â€

- Traditional CIR: â€œRetrieve-and-Reasonâ€  
- **OSrCIR**: â€œReason-before-Retrieveâ€  
- Uses an **MLLM to directly infer target features from the reference image**  
- Retrieval is then performed using the generated reasoning result (text description)  

---

### ğŸ”§ OSrCIR Architecture  
> Itâ€™s one-stage, so everything happens at once! The pipeline is simple, though the internal prompting is complex.  

![Image](https://github.com/user-attachments/assets/0bd1ad55-fe54-4a7f-ac67-8cf699b46ca2)

- **Input**: (Reference Image, Text Query)  
- **Stage 1**: MLLM performs chain-of-thought style reasoning on the reference image  
- **Stage 2**: The reasoning is refined into a target description (text query)  
- **Stage 3**: Candidate images are retrieved using CLIP-based text-image matching (zero-shot)  

â†’ The entire process is **end-to-end in a single stage**, with more complex prompts rather than multiple models.  

![Image](https://github.com/user-attachments/assets/2cbc360b-04d4-4a68-8e8a-d397278c98fa)

- Both inputs (image + text query) go into one stage  
- Outputs include: image caption, thoughts, reflection, and the final description  
- The **final description** is then used for retrieval (same retrieval stage as CIReVL)  

![Image](https://github.com/user-attachments/assets/f151bcb5-cd7d-4bc5-9534-6d52fa24b6cd)

- The **thoughts and reflection** steps help **reduce hallucination**  

---

### ğŸ§ª Experimental Results  

- **Datasets & Metrics**  
  - Benchmarks: **CIRR, CIRCO, FashionIQ, GeneCIS**  
  - Metrics:  
    - CIRR, GeneCIS, FashionIQ â†’ **Recall@k (R@k)**  
    - CIRCO â†’ **mAP@k (multi-ground-truth)**  
    - CIRR Subset â†’ **RecallSubset@k**  

---

- **Baselines**  
  - **Textual Inversion (training-dependent)**: Pic2Word, SEARLE, Context-I2W, LinCIR  
  - **Training-Free**: CIReVL, CIReVL*  
    - CIReVL* (star): Same two-stage structure, but both image captioning + text composition handled by the same MLLM (e.g., GPT-4V, LLaVA).  
  - **Proposed Model**: OSrCIR  

---

### ğŸ“Š OSrCIR vs Baselines (Performance Summary)

| Dataset   | Metric        | OSrCIR (ViT-L/14) | CIReVL* | Context-I2W | LinCIR | Gain (vs CIReVL*) |
|-----------|---------------|-------------------|---------|-------------|--------|-------------------|
| **CIRCO** | mAP@5         | **23.87%**        | 18.92%  | 13.04%      | -      | +4.95% |
| **CIRR**  | Avg (R@k)     | **â†‘**             | -       | -           | -      | +3.23% |
| **GeneCIS** | Avg R@1     | **â†‘**             | -       | -           | -      | +1.8% (vs CIReVL*), +5.2% (vs Context-I2W) |
| **FashionIQ** | R@10 (L/14) | **â†‘**           | -       | -           | -      | +4.74% (vs CIReVL*), +5.47% (vs Context-I2W) |
| **FashionIQ** | R@10 (G/14) | **â†‘**           | -       | -           | Best   | +4.6% (vs CIReVL*), but < LinCIR |

---

- **Qualitative Results**  
  - OSrCIR better captures **fine-grained details and context**  
    - Examples: â€œposterâ€, â€œChihuahuaâ€, â€œLabradorâ€, â€œbeachâ€  
  - In fashion, more precise with **â€œone-shoulder dressâ€**, **complex t-shirt patterns**, etc.  
- Limitation: In specialized domains like fashion, CLIP misalignment still remains a bottleneck.  

---

## âœ… Conclusion & Significance  

- OSrCIR demonstrates how **MLLM reasoning** can be optimized for CIR.  
- Works purely with **training-free zero-shot inference** â†’ generalizable and lightweight.  
- **One of the first cases of applying Chain-of-Thought reasoning in a single-stage retrieval pipeline**.  
- Opens new possibilities for applying VLM/MLLM reasoning in **tutoring, search, and AGI planning** tasks.  


---


### ğŸ§  (í•œêµ­ì–´) OSrCIR: Reason-before-Retrieve. í•œë°©ì— ëë‚´ëŠ” CIR

![Image](https://github.com/user-attachments/assets/83515f70-2856-49ec-8f95-5db4d6f1777d)

- **ì œëª©**: [Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/pdf/2412.11077)  
- **í•™íšŒ**: CVPR 2025 (Highlight paper, Tang et al.)  
- **ì½”ë“œ**: [OSrCIR (GitHub)](https://github.com/Pter61/osrcir)  
- **í•µì‹¬ í‚¤ì›Œë“œ**: `Composed Image Retrieval`, `Chain-of-Thought`, `One-Stage`, `MLLM`, `Zero-Shot`

---

### ğŸ“Œ 3ì¤„ ìš”ì•½

1. ê¸°ì¡´ì˜ **ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ì¡°í•© ê²€ìƒ‰(CIR)** [ì—°êµ¬ëŠ”](https://drfirstlee.github.io/posts/CIReVL/) **2-Stage êµ¬ì¡°** (ì´ë¯¸ì§€ ìº¡ì…˜ â†’ í…ìŠ¤íŠ¸ ì¶”ë¡ ) ì‚¬ìš©  
2. OSrCIRì€ MLLMì´ Reference ì´ë¯¸ì§€ë¥¼ **ì§ì ‘ reasoningí•˜ì—¬**, í…ìŠ¤íŠ¸ ì—†ì´ **Target ì´ë¯¸ì§€ì˜ íŠ¹ì„± ìì²´ë¥¼ ì¶”ë¡ **  
3. ê²°ê³¼ì ìœ¼ë¡œ ì •í™•ë„/ì†ë„ í–¥ìƒ, **ì‚¬ì „ í•™ìŠµ ì—†ì´ zero-shot inferenceë§Œìœ¼ë¡œ** ì‘ë™ ê°€ëŠ¥

---

### ğŸ” ê¸°ì¡´ CIR êµ¬ì¡°ì˜ í•œê³„

| ë°©ì‹ | êµ¬ì¡° | ë¬¸ì œì  |
|------|------|--------|
| 2-Stage CIR | (1) ì´ë¯¸ì§€ â†’ ìº¡ì…˜ ìƒì„± (2) í…ìŠ¤íŠ¸ â†’ ì¶”ë¡  â†’ ê²€ìƒ‰ | ì´ë¯¸ì§€ ì •ë³´ ì†ì‹¤, reasoning ì˜¤ë¥˜ ë°œìƒ |
| Text-Only Reasoning | Reference ì´ë¯¸ì§€ ì •ë³´ë¥¼ ê°„ì ‘ì ìœ¼ë¡œ ì „ë‹¬ | ì‹œê°ì  ì†ì„± ë°˜ì˜ ì–´ë ¤ì›€ |
| MLLM í™œìš© ë°©ì‹ | ì§ˆë¬¸ ì‘ë‹µìœ¼ë¡œ ê°„ì ‘ reasoning | ì‹œê°„ ì†Œìš”, ì¼ê´€ì„± ë¶€ì¡± |

â†’ ì¦‰, **í…ìŠ¤íŠ¸ë¥¼ ì¤‘ê°„ ë§¤ê°œë¡œ ì‚¼ëŠ” ë°©ì‹ ìì²´ê°€ ë³¸ì§ˆì ì¸ ì •ë³´ ì†ì‹¤ì„ ìœ ë°œ**í•¨.

---

### ğŸŒ± OSrCIRì˜ í•µì‹¬ ì•„ì´ë””ì–´

> â€œ**Reason first. Then retrieve.**â€

- ê¸°ì¡´ CIRì€ â€œRetrieve-and-Reasonâ€ ë°©ì‹  
- **OSrCIRì€ ë°˜ëŒ€ë¡œ â€˜Reason-before-Retrieveâ€™**  
- **MLLMì„ ì‚¬ìš©í•´ ì´ë¯¸ì§€ì—ì„œ ì§ì ‘ target íŠ¹ì„± ì¶”ë¡ **  
- ì´ reasoning ê²°ê³¼(í…ìŠ¤íŠ¸)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Target ì´ë¯¸ì§€ ê²€ìƒ‰ ìˆ˜í–‰

---

### ğŸ”§ OSrCIR ì˜ êµ¬ì¡°!!  
> One-stageì—¬ì„œ í•œë²ˆì— ì²˜ë¦¬í•œë‹¤! ê·¸ë˜ì„œ êµ¬ì¡°ê°€ ë¹„êµì  ê°„ë‹¨í•˜ë‹¤, í•œ êµ¬ì¡° ë‚´ì˜ í”„ë¡¬í¬íŠ¸ê°€ ë³µì¡í• ë¿!  

![Image](https://github.com/user-attachments/assets/0bd1ad55-fe54-4a7f-ac67-8cf699b46ca2)

- **ì…ë ¥**: (Reference Image, Text Query)
- **Stage 1**: MLLMì„ í™œìš©í•´ Reference ì´ë¯¸ì§€ì— ëŒ€í•´ chain-of-thought ìŠ¤íƒ€ì¼ ì¶”ë¡  ìˆ˜í–‰  
- **Stage 2**: ì¶”ë¡  ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ì •ì œ
- **Stage 3**: ê²€ìƒ‰ í›„ë³´ ì´ë¯¸ì§€ë“¤ê³¼ CLIP ê¸°ë°˜ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ë§¤ì¹­ ìˆ˜í–‰ (zero-shot)

â†’ ì „ì²´ ê³¼ì •ì´ end-to-endë¡œ **ë‹¨ì¼ ë‹¨ê³„(one-stage)** ì—ì„œ ì²˜ë¦¬ë¨

![Image](https://github.com/user-attachments/assets/2cbc360b-04d4-4a68-8e8a-d397278c98fa)

- ìœ„ì™€ ê°™ì´ 1ë²ˆì˜ stageì—ì„œ Inputì´ 2ê°œ ë“¤ì–´ê°€ê³ ,  
- ì´ë¯¸ì§€ ì„¤ëª…, Thoughts, reflection, final descriptionì´ í•œë²ˆì— ë‚˜ì˜´!!  
- ì—¬ê¸°ì„œ final descriptionì„ ë°”íƒ•ìœ¼ë¡œ Image Retrieval ì§„í–‰!(CIReVLê³¼ ë™ì¼í•œ CIR)    

![Image](https://github.com/user-attachments/assets/f151bcb5-cd7d-4bc5-9534-6d52fa24b6cd)

- Thoughts, reflection ì„ í†µí•´ì„œ í• ë£¨ì‹œë‚´ì´ì…˜ì„ ì¤„ì—¬ì¤€ë‹¤!!

---

### ğŸ§ª ì‹¤í—˜ ê²°ê³¼ ìš”ì•½  

- **ë°ì´í„°ì…‹ ë° í‰ê°€ ì§€í‘œ**  
  - ì‚¬ìš©ëœ CIR ë²¤ì¹˜ë§ˆí¬: **CIRR, CIRCO, FashionIQ, GeneCIS**  
  - í‰ê°€ ì§€í‘œ:  
    - CIRR, GeneCIS, FashionIQ â†’ **Recall@k (R@k)**  
    - CIRCO â†’ **mAP@k (ë‹¤ì¤‘ ì •ë‹µ í—ˆìš©)**  
    - CIRR Subset â†’ **RecallSubset@k**  

---

- **ë¹„êµ ê¸°ì¤€**  
  - **Textual Inversion ê¸°ë°˜ (í•™ìŠµ í•„ìš”)**: Pic2Word, SEARLE, Context-I2W, LinCIR  
  - **Training-Free ê¸°ë°˜**: CIReVL, CIReVL*  
    - CIReVL* (CIReVL-star) :ë™ì¼í•œ 2ë‹¨ê³„ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ì§€ë§Œ, ì°¸ì¡° ì´ë¯¸ì§€ ìº¡ì…”ë‹ + í…ìŠ¤íŠ¸ ì¡°í•© ìƒì„± â†’ ê°™ì€ MLLM(ì˜ˆ: GPT-4V, LLaVA ë“±)ìœ¼ë¡œ ëª¨ë‘ ì²˜ë¦¬.  
  - ì œì•ˆ ëª¨ë¸: **OSrCIR**  

---

### ğŸ“Š OSrCIR vs Baselines ì„±ëŠ¥ ë¹„êµ ìš”ì•½

| Dataset   | Metric        | OSrCIR (ViT-L/14) | CIReVL* | Context-I2W | LinCIR | í–¥ìƒë„ (vs CIReVL*) |
|-----------|---------------|-------------------|---------|-------------|--------|----------------------|
| **CIRCO** | mAP@5         | **23.87%**        | 18.92%  | 13.04%      | -      | +4.95% |
| **CIRR**  | Avg (R@k)     | **â†‘**             | -       | -           | -      | +3.23% |
| **GeneCIS** | Avg R@1     | **â†‘**             | -       | -           | -      | +1.8% (vs CIReVL*), +5.2% (vs Context-I2W) |
| **FashionIQ** | R@10 (L/14) | **â†‘**           | -       | -           | -      | +4.74% (vs CIReVL*), +5.47% (vs Context-I2W) |
| **FashionIQ** | R@10 (G/14) | **â†‘**           | -       | -           | Best   | +4.6% (vs CIReVL*), but < LinCIR |


---

- **ì •ì„±ì  ê²°ê³¼ (Qualitative Results)**  
  - OSrCIRì€ **ì„¸ë¶€ ì†ì„±ê³¼ ë¬¸ë§¥ì„ ë” ì •í™•íˆ ë°˜ì˜**  
    - ì˜ˆ: â€œí¬ìŠ¤í„°â€, â€œì¹˜ì™€ì™€â€, â€œë˜ë¸Œë¼ë„â€, â€œí•´ë³€â€ ê°™ì€ ì„¸ë¶€ ìš”ì†Œ ë°˜ì˜  
  - íŒ¨ì…˜ ì†ì„±ì—ì„œë„ â€œì›ìˆ„ë” ë“œë ˆìŠ¤â€, â€œë³µì¡í•œ í‹°ì…”ì¸  íŒ¨í„´â€ ë“± ì„¸ë¶€ ë””í…Œì¼ì„ ë” ì˜ ìº¡ì²˜  
- ë‹¤ë§Œ, íŒ¨ì…˜ê³¼ ê°™ì€ íŠ¹ìˆ˜ ë„ë©”ì¸ì—ì„œëŠ” CLIP ì •ë ¬ ë¶€ì¡±ì˜ í•œê³„ê°€ ê´€ì°°ë¨.  


## âœ… ê²°ë¡  ë° ì˜ì˜

- OSrCIRì€ MLLMì˜ ê³ ì°¨ reasoning ëŠ¥ë ¥ì„ CIRì— **ìµœì í™”ëœ ë°©ì‹**ìœ¼ë¡œ ëŒì–´ë‚¸ ëŒ€í‘œì  ì‚¬ë¡€  
- ë³„ë„ í•™ìŠµ ì—†ì´ inferenceë§Œìœ¼ë¡œ ë™ì‘ â†’ **Training-free + Generalizable**
- **Chain-of-Thought reasoningì´ ë‹¨ì¼ ìŠ¤í…Œì´ì§€ retrievalì— ì§ì ‘ ì ìš©ëœ ìµœì´ˆ ì‚¬ë¡€ ì¤‘ í•˜ë‚˜**
- í–¥í›„ VLM ê¸°ë°˜ **íŠœí„°ë§, ê²€ìƒ‰, AGI planning ë“±**ì—ì„œì˜ ì‘ìš© ê°€ëŠ¥ì„± ë§¤ìš° í¼

